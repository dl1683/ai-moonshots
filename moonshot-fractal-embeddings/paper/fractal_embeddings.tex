\documentclass{article}

% NeurIPS 2026 style
\usepackage[final]{neurips_2026}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{wrapfig}

\newcommand{\steer}{\mathcal{S}}
\newcommand{\hlz}{H(L_0)}
\newcommand{\hlo}{H(L_1|L_0)}

\title{Fractal Embeddings: Hierarchy-Aligned Prefix Supervision\\for Steerable Semantic Granularity}

\author{
  Devansh Lodha \\
  Independent Researcher \\
  \texttt{devansh@example.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Modern embedding models support dimensional truncation, but truncation typically changes fidelity rather than semantic level, leaving no mechanism to \emph{steer} between coarse and fine meaning at inference time.
We introduce \textbf{Fractal Embeddings}, a hierarchy-aligned prefix supervision scheme that trains short prefixes (64d) on coarse labels ($L_0$) and full embeddings (256d) on fine labels ($L_1$), using a frozen backbone with learned heads only.
Against a matched Matryoshka Representation Learning (MRL) baseline trained on $L_1$ at all prefix lengths, our method preserves full-resolution performance while inducing robust \emph{steerability}: truncated prefixes encode coarse semantics, whereas full vectors recover fine semantics.
On CLINC-150, V5 achieves $\steer = +0.144 \pm 0.037$ while MRL produces $\steer = +0.015 \pm 0.011$ ($p < 0.01$, 3~seeds).
Causal ablations on two datasets identify alignment as the driver: inverting alignment reverses the steerability sign (CLINC: $-0.018$, TREC: $-0.025$), and removing prefix-specific supervision collapses it to near-zero; all comparisons significant ($p \leq 0.03$, Cohen's $d \geq 2.7$).
In a synthetic hierarchy experiment with fixed text and varied coarse partitions, steerability scales with coarse entropy $\hlz$ and exhibits a Goldilocks optimum at ${\sim}12$--$16$ coarse classes (quadratic $R^2 = 0.964$).
Across six real datasets spanning four domains, steerability is strongly rank-correlated with conditional entropy $\hlo$ (Spearman $\rho = 0.94$, $p = 0.005$).
MRL produces near-zero steerability in all conditions.
Cross-model replication on Qwen3-0.6B confirms architecture invariance.
These results establish hierarchy alignment as a principled control knob for semantic granularity.
\end{abstract}

% Hero figure
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig1_teaser.pdf}
    \caption{CLINC-150: V5 and MRL achieve comparable accuracy at full embedding length (256d), but V5's short prefixes (64d) specialize for coarse semantics while MRL's do not. This prefix specialization enables \emph{semantic steering} via dimensional truncation.}
    \label{fig:teaser}
\end{figure}

%=======================================================================
\section{Introduction}
\label{sec:intro}
%=======================================================================

Dense text embeddings~\citep{reimers2019sentence} are the backbone of modern retrieval systems, mapping sentences to fixed-dimensional vectors where similarity in vector space approximates semantic similarity.
Recent work on \emph{Matryoshka Representation Learning} (MRL)~\citep{kusupati2022matryoshka} has shown that embeddings can be trained to support dimensional truncation: a 256-dimensional embedding can be truncated to 64 dimensions with graceful accuracy degradation, enabling efficient storage and retrieval.

However, dimensional truncation in MRL changes \emph{fidelity}---the amount of information preserved---not \emph{semantic granularity}.
A 64-dimensional MRL prefix encodes the same kind of information as the full vector, just with lower resolution.
This is a missed opportunity: real-world semantics are inherently \emph{hierarchical}.
A question like ``What is the capital of France?'' simultaneously belongs to the coarse category \texttt{LOCATION} and the fine-grained category \texttt{CITY}.
An ideal embedding should allow a user to \emph{steer} between these levels by choosing how many dimensions to use.

We introduce \textbf{Fractal Embeddings}, a simple modification to MRL training that aligns prefix supervision with semantic hierarchy.
Instead of training all prefix lengths on the finest-grained labels (as in MRL), we train short prefixes on coarse labels ($L_0$) and full embeddings on fine labels ($L_1$).
This creates embeddings where dimensional truncation corresponds to semantic zoom: fewer dimensions $\rightarrow$ coarser meaning, more dimensions $\rightarrow$ finer meaning (Figure~\ref{fig:teaser}).

Our key contributions are:
\begin{enumerate}
    \item \textbf{A method} for training steerable embeddings via hierarchy-aligned prefix supervision (Section~\ref{sec:method}).
    \item \textbf{Causal identification} across two datasets that steerability is caused by alignment, not architecture: inverting the alignment reverses the steerability sign on both CLINC and TREC (Section~\ref{sec:causal}).
    \item \textbf{A scaling law} linking steerability magnitude to hierarchy structure, with a Goldilocks optimum identified via synthetic causal intervention (Section~\ref{sec:scaling}).
    \item \textbf{Cross-model replication} confirming the effect is architecture-invariant across BGE-small and Qwen3-0.6B (Section~\ref{sec:generality}).
\end{enumerate}

%=======================================================================
\section{Problem Setup and Definitions}
\label{sec:setup}
%=======================================================================

\paragraph{Hierarchical classification.}
We consider datasets with two-level label hierarchies: each sample $x$ has a coarse label $y^{(0)} \in \{1, \ldots, K_0\}$ and a fine label $y^{(1)} \in \{1, \ldots, K_1\}$, where each fine class maps to exactly one coarse class.
The hierarchy is characterized by the branching factor $K_1/K_0$ and the conditional entropy $\hlo$, which measures how much additional information $L_1$ carries beyond $L_0$.

\paragraph{Prefix-truncated embeddings.}
Given an embedding $\mathbf{e} \in \mathbb{R}^d$, the $j$-th prefix is $\mathbf{e}_{1:jd/J}$ for $j \in \{1, \ldots, J\}$.
We use $J = 4$ with $d = 256$, giving prefixes of 64, 128, 192, and 256 dimensions.
Classification accuracy is evaluated using a $k$-NN classifier ($k = 5$) on cosine distance.

\paragraph{Steerability.}
We define the steerability metric:
\begin{equation}
    \steer = \underbrace{(\text{L0@}j_1 - \text{L0@}j_4)}_{\text{coarse specialization}} + \underbrace{(\text{L1@}j_4 - \text{L1@}j_1)}_{\text{fine specialization}}
    \label{eq:steerability}
\end{equation}
where $\text{L}k\text{@}j$ denotes level-$k$ classification accuracy at prefix length $j$.
Positive $\steer$ means short prefixes specialize for coarse semantics while full embeddings specialize for fine semantics.
A perfectly steerable embedding has high $\steer$; MRL, which trains all lengths on $L_1$, should have $\steer \approx 0$.

\paragraph{Datasets.}
Table~\ref{tab:datasets} summarizes our six evaluation datasets, spanning conditional entropies from 1.23 (Yahoo~\citep{zhang2015yahoo}) to 3.90 bits (CLINC-150~\citep{larson2019clinc}), also including GoEmotions~\citep{demszky2020goemotions}, TREC~\citep{voorhees2000trec}, 20 Newsgroups, and arXiv~\citep{clement2019arxiv}.

\begin{table}[t]
\caption{Dataset statistics and hierarchy profiles. $K_0$, $K_1$: number of coarse and fine classes. Branch: $K_1/K_0$. $\hlz$, $\hlo$: coarse entropy and conditional entropy in bits.}
\label{tab:datasets}
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
Dataset & $K_0$ & $K_1$ & Branch & $\hlz$ & $\hlo$ & Train & Test \\
\midrule
Yahoo Answers & 4 & 10 & 2.5 & 1.91 & 1.23 & 10{,}000 & 2{,}000 \\
GoEmotions & 4 & 28 & 7.0 & 1.64 & 1.88 & 7{,}092 & 1{,}700 \\
20 Newsgroups & 6 & 20 & 3.3 & 2.43 & 1.88 & 10{,}000 & 2{,}000 \\
TREC & 6 & 50 & 8.3 & 2.38 & 2.21 & 5{,}452 & 500 \\
arXiv & 20 & 123 & 6.2 & 3.40 & 2.62 & 8{,}548 & 2{,}000 \\
CLINC-150 & 10 & 150 & 15.0 & 3.32 & 3.90 & 10{,}000 & 2{,}000 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Method: Progressive Prefix Supervision (V5)}
\label{sec:method}
%=======================================================================

Our method modifies MRL training in one key way: we align the \emph{supervision signal} with the \emph{prefix length}, so that shorter prefixes are trained to capture coarse structure while longer prefixes capture fine structure.

\paragraph{Architecture.}
We use a frozen pretrained embedding backbone (BGE-small-en-v1.5~\citep{xiao2023bge} or Qwen3-Embedding-0.6B) with a learned linear projection head $W \in \mathbb{R}^{h \times d}$ mapping from hidden dimension $h$ to output dimension $d = 256$.
Two classification heads operate on the output: $\text{head}_\text{top}$ (coarse, $K_0$ classes) and $\text{head}_\text{bot}$ (fine, $K_1$ classes).

\paragraph{Progressive prefix supervision.}
During training, we randomly sample a prefix length $j \in \{1, 2, 3, 4\}$ with probabilities $[0.4, 0.3, 0.2, 0.1]$ (favoring shorter prefixes).
The loss depends on $j$:
\begin{equation}
    \mathcal{L}_\text{prefix}(j) = \begin{cases}
        \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:64}), y^{(0)}) & j = 1 \\
        \alpha \cdot \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:jd/4}), y^{(0)}) + (1-\alpha) \cdot \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) & j = 2,3 \\
        \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:256}), y^{(1)}) & j = 4
    \end{cases}
    \label{eq:v5loss}
\end{equation}
where $\alpha$ decreases with $j$ (e.g., $\alpha = 0.7$ for $j=2$, $\alpha = 0.3$ for $j=3$).
The total loss combines the full-embedding loss with the sampled prefix loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)}) + w_\text{prefix} \cdot \mathcal{L}_\text{prefix}(j)
\end{equation}
with $w_\text{prefix} = 0.6$.

\paragraph{Block dropout.}
To prevent later dimensions from carrying redundant coarse information, we apply block dropout during training: dimension blocks are independently kept with probabilities $[0.95, 0.9, 0.8, 0.7]$ for blocks 1--4.
This forces the network to encode coarse information in early dimensions (high keep probability) and fine information in later dimensions (lower keep probability).

\paragraph{MRL baseline.}
Our matched baseline uses identical architecture and training procedure, but trains \emph{all} prefix lengths on $L_1$ (fine labels):
\begin{equation}
    \mathcal{L}_\text{MRL}(j) = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) \quad \forall j
\end{equation}
This isolates the effect of hierarchy alignment from all other training choices (backbone, optimizer, epochs, batch size).

\paragraph{Training details.}
Head-only training for 5 epochs, batch size 16, learning rate $10^{-4}$ with AdamW and cosine decay.
Mixed-precision (FP16) training with gradient clipping at 1.0.
Best model selected by validation score $\text{L0} + \text{L1}$.
Algorithm~\ref{alg:v5} summarizes the procedure.

\begin{algorithm}[t]
\caption{V5 Progressive Prefix Supervision}
\label{alg:v5}
\begin{algorithmic}[1]
\REQUIRE Backbone $f_\theta$ (frozen), head $W$, dataset $\mathcal{D}$ with $(x, y^{(0)}, y^{(1)})$
\REQUIRE Prefix probs $\mathbf{p} = [0.4, 0.3, 0.2, 0.1]$, block keep $\mathbf{k} = [0.95, 0.9, 0.8, 0.7]$
\FOR{each batch $\{(x_i, y_i^{(0)}, y_i^{(1)})\}$}
    \STATE $\mathbf{h} \leftarrow f_\theta(x_i)$ \hfill \COMMENT{frozen backbone}
    \STATE $\mathbf{e} \leftarrow W \cdot \mathbf{h}$ \hfill \COMMENT{learned projection to $\mathbb{R}^d$}
    \STATE Apply block dropout with keep probs $\mathbf{k}$
    \STATE Sample $j \sim \text{Categorical}(\mathbf{p})$
    \STATE $\mathcal{L}_\text{full} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \IF{$j = 1$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:d/4}), y^{(0)})$
    \ELSIF{$j = 4$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \ELSE
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \alpha_j \cdot \text{CE}(\text{head}_\text{top}, y^{(0)}) + (1-\alpha_j) \cdot \text{CE}(\text{head}_\text{bot}, y^{(1)})$
    \ENDIF
    \STATE $\mathcal{L} \leftarrow \mathcal{L}_\text{full} + 0.6 \cdot \mathcal{L}_\text{prefix}$
    \STATE Update $W$, $\text{head}_\text{top}$, $\text{head}_\text{bot}$ via $\nabla_W \mathcal{L}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%=======================================================================
\section{Main Results: Steerability Without Sacrificing Accuracy}
\label{sec:results}
%=======================================================================

\paragraph{Classification performance.}
At full embedding length ($j=4$, 256d), V5 and MRL achieve comparable classification accuracy on Yahoo, GoEmotions, Newsgroups, TREC, and arXiv (Table~\ref{tab:accuracy}).
On CLINC, V5 retains higher $L_0$ accuracy at $j=4$ while MRL retains higher $L_1$---a natural consequence of V5's coarse-to-fine information allocation, which persists even at full length.
Both methods substantially outperform the unfinetuned baseline across all datasets.

\begin{table}[t]
\caption{Classification accuracy at full embedding length ($j = 4$, 256d). V5 and MRL are comparable on Yahoo, Newsgroups, TREC, and arXiv. On CLINC, V5 retains higher $L_0$ while MRL retains higher $L_1$---reflecting V5's coarse-to-fine allocation. Both improve substantially over the unfinetuned baseline.}
\label{tab:accuracy}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{L0 Accuracy} & \multicolumn{3}{c}{L1 Accuracy} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Dataset & Baseline & V5 & MRL & Baseline & V5 & MRL \\
\midrule
Yahoo & 0.688 & 0.699 & 0.698 & 0.603 & 0.629 & 0.635 \\
GoEmotions & 0.502 & 0.600 & 0.578 & 0.343 & 0.429 & 0.411 \\
Newsgroups & 0.815 & 0.808 & 0.797 & 0.658 & 0.641 & 0.649 \\
TREC & 0.854 & 0.934 & 0.931 & 0.718 & 0.791 & 0.786 \\
arXiv & 0.721 & 0.729 & 0.721 & 0.465 & 0.448 & 0.446 \\
CLINC & 0.961 & 0.955 & 0.913 & 0.887 & 0.681 & 0.713 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig3_forest_plot.pdf}
    \caption{Cross-dataset steerability: V5 (blue) produces positive steerability that scales with hierarchy depth across six datasets, while MRL (orange) remains near zero. Error bars show 95\% CIs.}
    \label{fig:forest}
\end{figure}

\paragraph{Steerability.}
Despite classification parity, V5 produces dramatically higher steerability than MRL across all six datasets (Figure~\ref{fig:forest}, Table~\ref{tab:steerability}).
Steerability increases with hierarchy complexity ($\hlo$): from $\steer = +0.015$ on Yahoo ($\hlo = 1.23$) through GoEmotions ($+0.020$, $\hlo = 1.88$), TREC ($+0.045$, $\hlo = 2.21$), and arXiv ($+0.027$, $\hlo = 2.62$) to $\steer = +0.144$ on CLINC ($\hlo = 3.90$).
arXiv falls slightly below TREC despite higher $\hlo$, likely because cross-listed papers create fuzzy category boundaries that reduce effective hierarchy depth; we discuss this further in Section~\ref{sec:scaling}.
MRL steerability is consistently near zero ($\steer_\text{MRL} < 0.02$), confirming that steerability requires explicit hierarchy alignment.

\begin{table}[t]
\caption{Steerability across datasets ($\steer$ = coarse specialization + fine specialization, Eq.~\ref{eq:steerability}). V5 achieves positive steerability that scales with hierarchy complexity. MRL is consistently near zero. Reported as mean $\pm$ SD across seeds.}
\label{tab:steerability}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & V5 $\steer$ & MRL $\steer$ & Gap & Seeds \\
\midrule
Yahoo & 1.23 & $+0.015 \pm 0.019$ & $+0.005 \pm 0.011$ & $+0.010$ & 5 \\
GoEmotions & 1.88 & $+0.020 \pm 0.018$ & $+0.006 \pm 0.016$ & $+0.014$ & 5 \\
Newsgroups & 1.88 & $+0.022 \pm 0.029$ & $+0.009 \pm 0.012$ & $+0.013$ & 3 \\
TREC & 2.21 & $+0.045 \pm 0.023$ & $+0.003 \pm 0.019$ & $+0.041$ & 3 \\
arXiv & 2.62 & $+0.027 \pm 0.015$ & $-0.001 \pm 0.013$ & $+0.028$ & 5 \\
CLINC & 3.90 & $+0.144 \pm 0.037$ & $+0.015 \pm 0.011$ & $+0.129$ & 3 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Causal Identification}
\label{sec:causal}
%=======================================================================

The correlation between hierarchy alignment and steerability in Section~\ref{sec:results} could arise from confounds (e.g., dataset properties, label structure).
We perform three causal ablations on two datasets---CLINC-150 ($\hlo = 3.90$, 5~seeds) and TREC-50 ($\hlo = 2.21$, 3~seeds)---to isolate the effect of alignment and verify cross-dataset robustness.

\paragraph{Ablation conditions.}
All three conditions use identical architecture, optimizer, hyperparameters, and data split (held fixed across seeds).
Only the prefix-to-hierarchy mapping changes:
\begin{itemize}
    \item \textbf{Aligned (V5)}: $j=1 \rightarrow L_0$, $j=4 \rightarrow L_1$ (correct alignment).
    \item \textbf{Inverted}: $j=1 \rightarrow L_1$, $j=4 \rightarrow L_0$ (reversed alignment).
    \item \textbf{No-prefix}: All prefix lengths trained on $L_1$ with additional $L_0$ regularization (no alignment).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig4_ablation.pdf}
    \caption{Causal ablation on CLINC-150 (5 seeds) and TREC-50 (3~seeds). Aligned supervision (V5) produces positive steerability; inverting alignment reverses the sign; removing prefix-specific alignment collapses it. Individual seed values shown as dots.}
    \label{fig:ablation}
\end{figure}

\paragraph{Results.}
The ablation results (Table~\ref{tab:ablation}, Figure~\ref{fig:ablation}) provide definitive causal evidence:

\begin{table}[t]
\caption{Causal ablation on two datasets (BGE-small). All conditions share a fixed train/val split to eliminate data-split variance. Inverting alignment reverses the steerability sign on both datasets; removing prefix-specific alignment collapses it. The effect replicates across hierarchy depths.}
\label{tab:ablation}
\centering
\small
\begin{tabular}{llcccc}
\toprule
Dataset & Condition & $\steer$ (mean $\pm$ SD) & vs.\ V5 $t$ & vs.\ V5 $p$ & Cohen's $d$ \\
\midrule
\multirow{3}{*}{CLINC ($\hlo\!=\!3.90$, 5s)} & Aligned (V5) & $+0.053 \pm 0.004$ & --- & --- & --- \\
 & Inverted & $-0.018 \pm 0.005$ & 26.1 & $< 10^{-6}$ & 16.5 \\
 & No-prefix & $+0.009 \pm 0.005$ & 15.8 & $< 10^{-6}$ & 10.0 \\
\midrule
\multirow{3}{*}{TREC ($\hlo\!=\!2.21$, 3s)} & Aligned (V5) & $+0.045 \pm 0.023$ & --- & --- & --- \\
 & Inverted & $-0.025 \pm 0.008$ & 4.9 & $0.008$ & 4.0 \\
 & No-prefix & $-0.003 \pm 0.008$ & 3.3 & $0.030$ & 2.7 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Sign reversal}: Inverted alignment produces \emph{negative} steerability on both datasets (CLINC: $\steer = -0.018$; TREC: $\steer = -0.025$), meaning short prefixes now specialize for fine semantics and full embeddings for coarse---the opposite of V5.
    This rules out any explanation where steerability arises from architecture alone.
    \item \textbf{Signal collapse}: Without prefix-specific alignment, steerability drops to near-zero (CLINC: $+0.009$; TREC: $-0.003$), indistinguishable from MRL.
    \item \textbf{Cross-dataset robustness}: The sign reversal and signal collapse replicate across hierarchy depths ($\hlo = 2.21$ and $3.90$), confirming the effect is not dataset-specific.
    \item \textbf{Effect sizes}: Cohen's $d \geq 2.7$ across all comparisons on both datasets (up to $d = 16.5$ on CLINC), indicating the effect is both statistically and practically significant.
\end{enumerate}

These results establish the \textbf{Fractal Embedding Principle}: steerability is \emph{caused} by the alignment between prefix supervision and hierarchy structure, not by any other aspect of training. The effect replicates across datasets with different hierarchy depths, class counts, and domains.

\subsection{Information Localization}

The ablations above modify \emph{training}; we additionally measure whether V5 concentrates different semantic levels in different embedding blocks.
For each test sample, we independently classify $L_0$ and $L_1$ from (a)~the prefix only (first 64d) and (b)~the suffix only (dims~65--256), using $k$-NN against references at the same granularity.

On CLINC-150, V5 shows 1.8\% less $L_1$ information in the prefix (94.7\% vs.\ 96.5\% for MRL prefix-only $L_1$ accuracy), consistent with the training objective concentrating coarse information in early dimensions.
The effect is modest because 64 dimensions provide sufficient capacity for both $L_0$ (10~classes) and $L_1$ (150~classes) on this dataset.
The stronger evidence for semantic separation comes from the steerability metric itself (\S\ref{sec:main}), which measures the \emph{behavioral} consequence of prefix truncation rather than the raw information content.

%=======================================================================
\section{Steerability Scaling Law}
\label{sec:scaling}
%=======================================================================

Having established that alignment causes steerability, we investigate what determines its \emph{magnitude}.
We present two complementary analyses: an observational study across real datasets and a causal intervention using synthetic hierarchies.

\subsection{Observational: Steerability Scales with Hierarchy Complexity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig5_scaling_law.pdf}
    \caption{Steerability scales with hierarchy refinement entropy $\hlo$ across six real datasets (Spearman $\rho = 0.94$, $p = 0.005$). arXiv falls slightly below the linear trend, likely due to cross-listed papers creating fuzzy category boundaries.}
    \label{fig:scaling}
\end{figure}

Across six real datasets, steerability is strongly rank-correlated with conditional entropy $\hlo$ (Spearman $\rho = 0.94$, $p = 0.005$; Figure~\ref{fig:scaling}).
A linear fit yields $\steer \approx 0.049 \cdot \hlo - 0.067$ ($R^2 = 0.83$).
One mild outlier is arXiv ($\hlo = 2.62$, $\steer = +0.027$), which falls below TREC ($\hlo = 2.21$, $\steer = +0.045$) despite higher conditional entropy.
We attribute this to arXiv's pervasive cross-listing: many papers belong to multiple categories (e.g., cs.LG and stat.ML), creating fuzzy boundaries that reduce the \emph{effective} hierarchy depth below the nominal entropy.
The remaining five datasets follow a monotonic trend ($\rho = 1.0$), and the six-dataset correlation remains highly significant.

In natural datasets, $\hlz$ and $\hlo$ are positively correlated (more coarse classes $\Rightarrow$ more fine classes per coarse class).
This confound prevents us from determining whether the mechanism operates through $\hlz$ (prefix task demand) or $\hlo$ (refinement complexity).

\subsection{Causal: Synthetic Hierarchy Experiment}

To break this confound, we construct synthetic hierarchies with \emph{fixed text and fixed total entropy} $H(L_1) = \log_2 150$ but varying coarse partitions $K_0 \in \{2, 3, 5, 10, 15, 25, 50, 75\}$.
As $K_0$ increases, $\hlz = \log_2 K_0$ increases while $\hlo = \log_2(150/K_0)$ decreases.
We train V5 and MRL on each synthetic hierarchy using CLINC-150 text and evaluate steerability.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig6_synthetic.pdf}
    \caption{Synthetic hierarchy experiment: steerability shows a ``Goldilocks'' effect, peaking when coarse task entropy $\hlz$ matches prefix capacity ($K_0 \approx 12$--16). MRL remains near zero throughout. Quadratic fit $R^2 = 0.964$.}
    \label{fig:synthetic}
\end{figure}

\paragraph{Results.}
Figure~\ref{fig:synthetic} reveals the mechanism:

\begin{table}[t]
\caption{Synthetic hierarchy experiment. Fixed total entropy ($\log_2 150 = 7.23$ bits), varied coarse partition $K_0$. Steerability rises with $\hlz$, peaks at $K_0 \approx 15$, then declines---a \emph{Goldilocks} effect. MRL is near zero throughout.}
\label{tab:synthetic}
\centering
\small
\begin{tabular}{rcccccc}
\toprule
$K_0$ & $\hlz$ & $\hlo$ & Branch & V5 $\steer$ & MRL $\steer$ & Gap \\
\midrule
2 & 1.00 & 6.23 & 75.0 & $+0.134$ & $-0.010$ & $+0.144$ \\
3 & 1.58 & 5.64 & 50.0 & $+0.150$ & $+0.008$ & $+0.142$ \\
5 & 2.32 & 4.90 & 30.0 & $+0.216$ & $+0.002$ & $+0.214$ \\
10 & 3.32 & 3.90 & 15.0 & $+0.270$ & $-0.012$ & $+0.282$ \\
15 & 3.91 & 3.32 & 10.0 & $+0.278$ & $-0.004$ & $+0.282$ \\
25 & 4.64 & 2.58 & 6.0 & $+0.266$ & $-0.018$ & $+0.284$ \\
50 & 5.64 & 1.58 & 3.0 & $+0.252$ & $+0.010$ & $+0.242$ \\
75 & 6.23 & 1.00 & 2.0 & $+0.232$ & $-0.016$ & $+0.248$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Rising phase} ($K_0 = 2 \rightarrow 15$): Steerability increases with $\hlz$.
    More coarse classes create a richer ``routing codebook'' for the prefix, allowing finer-grained coarse discrimination.
    \item \textbf{Falling phase} ($K_0 = 15 \rightarrow 75$): Steerability declines as $\hlz$ exceeds the prefix's representational capacity.
    With only 64 dimensions, the prefix cannot reliably distinguish 50+ coarse classes.
    \item \textbf{Goldilocks optimum}: Peak at $K_0 \approx 12$--$16$ ($H^*(L_0) \approx 3.6$--$4.0$ bits), matching the effective capacity of a 64-dimensional prefix space.
    A quadratic fit captures the inverted-U shape with $R^2 = 0.964$.
    \item \textbf{MRL control}: MRL steerability remains near zero ($|\steer| < 0.02$) across all 8 conditions, confirming that the effect requires hierarchy-aligned supervision.
\end{enumerate}

\paragraph{Resolving the observational confound.}
The synthetic experiment reveals that $\steer \sim \hlz$ (prefix task demand), not $\steer \sim \hlo$.
In natural datasets, the observational correlation with $\hlo$ arises because $\hlz$ and $\hlo$ covary positively.
The synthetic experiment breaks this confound by holding total entropy fixed while varying $K_0$.
Figure~\ref{fig:entropy_alloc} in the appendix visualizes both relationships side by side.

%=======================================================================
\section{Generality and Limitations}
\label{sec:generality}
%=======================================================================

\paragraph{Cross-model replication.}
To verify architecture invariance, we replicate the CLINC and TREC experiments using Qwen3-Embedding-0.6B ($h = 1024$, 10$\times$ larger than BGE-small) with 3 seeds each.
Table~\ref{tab:crossmodel} shows that Qwen3 produces 2--3$\times$ higher steerability than BGE-small while preserving the rank ordering (CLINC $>$ TREC) and the V5 $\gg$ MRL gap.
On CLINC, Qwen3 V5 achieves $\steer = +0.153 \pm 0.013$ vs.\ MRL $+0.008 \pm 0.006$ ($p < 0.001$, Cohen's $d = 14.2$).
On TREC, V5 achieves $\steer = +0.081 \pm 0.012$ vs.\ MRL $+0.011 \pm 0.002$ ($p < 0.001$).
The larger backbone provides a richer feature space for hierarchy-aligned supervision to exploit.

\begin{table}[t]
\caption{Cross-model replication (3 seeds). Steerability is architecture-invariant: the V5 $\gg$ MRL pattern and rank ordering by $\hlo$ hold across both backbone sizes. Qwen3 shows 2--3$\times$ higher steerability, suggesting the effect scales with model capacity.}
\label{tab:crossmodel}
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{V5 $\steer$} & \multicolumn{2}{c}{MRL $\steer$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Dataset & BGE-small & Qwen3-0.6B & BGE-small & Qwen3-0.6B \\
\midrule
CLINC ($\hlo = 3.90$) & $+0.144 \pm 0.037$ & $+0.153 \pm 0.013$ & $+0.015 \pm 0.011$ & $+0.008 \pm 0.006$ \\
TREC ($\hlo = 2.21$) & $+0.045 \pm 0.019$ & $+0.081 \pm 0.012$ & $+0.003$ & $+0.011 \pm 0.002$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Downstream utility: Adaptive retrieval.}
Beyond classification, we test whether steerability enables practical \emph{adaptive search}.
We split test data into queries and documents, and compare four strategies: fixed~64d (fast), fixed~256d (full), adaptive progressive extension (start at 64d, extend if uncertain), and two-stage pipeline (64d first-pass, 256d reranker).
Across 3 seeds on CLINC-150:

\begin{itemize}
    \item \textbf{V5 at 64d}: $L_0 = 97.5\%$, $L_1 = 81.3\%$; excellent coarse retrieval with 75\% dimension savings.
    \item \textbf{MRL at 64d}: $L_0 = 97.3\%$, $L_1 = 91.2\%$; higher $L_1$ because the prefix was trained on fine labels.
    \item \textbf{V5 at 256d}: $L_1 = 92.5\%$ vs.\ MRL $L_1 = 91.6\%$; V5 slightly better at full resolution.
    \item \textbf{V5 steering range}: $L_1$ improves by 11.2\% from 64d to 256d; MRL improves by only 0.4\%.
\end{itemize}
This confirms steerability's practical signature: V5 provides two distinct operating points---a fast 64d prefix that matches MRL's coarse accuracy, and a full 256d embedding that recovers fine accuracy---enabling coarse-first, fine-on-demand retrieval pipelines that MRL cannot support.

\paragraph{Limitations.}
\begin{itemize}
    \item \textbf{Shallow hierarchies}: On Yahoo Answers ($K_0 = 4$, $\hlo = 1.23$), steerability is small and noisy ($\steer = 0.015 \pm 0.019$, 5~seeds). The method is most valuable for deep hierarchies.
    \item \textbf{Ceiling effects}: On datasets where the unfinetuned baseline already achieves high $L_0$ accuracy (e.g., DBPedia at $L_0 = 1.0$), there is no room for steerability improvement.
    \item \textbf{Two-level hierarchies}: Our current formulation handles two-level hierarchies ($L_0$, $L_1$). Extension to deeper hierarchies is straightforward in principle but untested.
\end{itemize}

%=======================================================================
\section{Theoretical Analysis: Successive Refinement}
\label{sec:theory}
%=======================================================================

We provide a formal information-theoretic analysis connecting fractal embeddings to the classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive}.

\paragraph{Setup.}
Let $(X, Y_0, Y_1)$ be a hierarchical source with $Y_0 = g(Y_1)$ (coarse is a deterministic function of fine).
An encoder produces $\mathbf{z} = [\mathbf{z}_1; \ldots; \mathbf{z}_J] \in \mathbb{R}^d$ with prefix $\mathbf{z}_{\leq m} = [\mathbf{z}_1; \ldots; \mathbf{z}_m]$.
Let $C(d')$ denote the effective capacity (in bits) of a $d'$-dimensional embedding under the encoder family.

\paragraph{Theorem 1 (Hierarchy-Successive-Refinement, informal).}
\emph{Assume $C(d/J) \geq H(L_0)$ and $C(d/J) < H(L_1)$.
Under V5 supervision ($\mathbf{z}_{\leq 1} \rightarrow L_0$, $\mathbf{z} \rightarrow L_1$):}
\begin{equation}
    I(\mathbf{z}_{\leq 1}; L_0) > I(\mathbf{z}_{\leq 1}; L_1 | L_0) \quad \text{(coarse-prioritized prefix)}
\end{equation}
\emph{Under MRL ($\mathbf{z}_{\leq 1} \rightarrow L_1$, $\mathbf{z} \rightarrow L_1$): no specialization, $I(\mathbf{z}_{\leq 1}; L_0) \approx I(\mathbf{z}; L_0)$.}

The proof follows from the capacity bottleneck: V5's prefix loss depends only on $L_0$, so the optimal prefix maximizes $I(\mathbf{z}_{\leq 1}; L_0)$. Since $C(d/J) < H(L_1)$ but $C(d/J) \geq H(L_0)$, the prefix allocates capacity preferentially to the coarse task.
MRL's prefix loss targets $L_1$, distributing capacity across both $L_0$ and $L_1|L_0$ components without specialization.

\paragraph{Connection to successive refinement.}
Hierarchical sources are naturally \emph{successively refinable}~\citep{rimoldi1994successive}: the optimal multi-resolution code first encodes $Y_0$ at rate $R_1 \geq H(Y_0)$, then encodes the residual $Y_1|Y_0$ at rate $R_2 \geq H(Y_1|Y_0)$.
V5 training approximates this: block 1 encodes $Y_0$, blocks 2--$J$ encode the refinement.
MRL instead performs single-resolution coding at each rate, losing the nested structure.
This explains why V5 achieves accuracy parity at full resolution while gaining steerability at short prefixes: it matches the rate-distortion bound for the multi-resolution problem.

\paragraph{Theorem 2 (Goldilocks capacity-demand matching, informal).}
\emph{With fixed $H(Y_1)$ and varying $K_0$, steerability $\steer(K_0)$ peaks at $H^*(L_0) \approx C(d/J)$:}
\begin{itemize}
    \item \emph{When $H(L_0) < C(d/J)$}: spare prefix capacity leaks $L_1|L_0$ information, reducing $\steer$.
    \item \emph{When $H(L_0) > C(d/J)$}: by Fano's inequality, prefix errors degrade coarse classification, reducing $\steer$.
    \item \emph{Taylor expansion around $H^*$}: $\steer \approx \steer^* - \alpha(H(L_0) - H^*)^2$, matching the empirical quadratic fit ($R^2 = 0.964$).
\end{itemize}

\paragraph{Testable prediction.}
Doubling the prefix dimension from 64 to 128 should shift the Goldilocks peak rightward (to higher $K_0$), as $C(d/J)$ increases. This is verifiable via a capacity sweep ablation.

%=======================================================================
\section{Related Work}
\label{sec:related}
%=======================================================================

\paragraph{Multi-resolution embeddings.}
Matryoshka Representation Learning (MRL)~\citep{kusupati2022matryoshka} trains embeddings that support prefix truncation, but all prefix lengths are supervised on the same task, producing no semantic specialization.
SMRL~\citep{li2025smrl} extends MRL with sequential training to reduce gradient variance; Matryoshka Multimodal Models~\citep{cai2024m3} apply the nesting principle to visual tokens in vision-language models.
None of these approaches introduce hierarchy-awareness: truncation changes \emph{fidelity}, not \emph{semantic level}.
Our work shows that aligning prefix supervision with hierarchy is the missing ingredient that converts fidelity truncation into semantic zoom.

\paragraph{Dimensional redundancy.}
\citet{dufter2025random} show that randomly removing 50\% of embedding dimensions causes $<$10\% performance degradation, revealing massive redundancy in standard embeddings.
We exploit this redundancy differently: rather than discarding dimensions for compression, we \emph{structure} them to carry semantically distinct information at each prefix length.
Random removal preserves flat performance but destroys hierarchical steerability---precisely the property we engineer.
\citet{luan2025limits} prove that the expressiveness of single-vector embeddings for top-$k$ retrieval is bounded by dimensionality, motivating multi-resolution access patterns that our method enables.

\paragraph{Hierarchical embeddings.}
Hyperbolic embeddings~\citep{nickel2017poincare} represent hierarchies through non-Euclidean geometry.
HEAL~\citep{zhang2025heal} (ICLR 2025) aligns LLM embeddings with domain hierarchies via hierarchical contrastive losses and matrix factorization.
Unlike HEAL, which requires external hierarchical labels and preprocessing, our method derives hierarchy from the embedding's own prefix structure, making it intrinsic and architecture-agnostic.
HEAL does not offer steerability via dimensional truncation.

\paragraph{Sparse and compressed embeddings.}
CSR~\citep{chen2025csr} (ICML 2025) and CSRv2~\citep{chen2026csrv2} (ICLR 2026) learn sparse codes as alternatives to MRL, achieving superior efficiency through selective activation.
These address adaptive dimensionality via sparsity; our approach addresses it via hierarchical prefix structure.
The two are orthogonal and potentially complementary: fractal embeddings could be sparsified for further efficiency.

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

We have shown that a simple modification to embedding training---aligning prefix supervision with semantic hierarchy---creates embeddings where dimensional truncation corresponds to semantic zoom.
Through causal ablations on two datasets, synthetic hierarchy experiments, and cross-model replication, we establish the \textbf{Fractal Embedding Principle}: steerability is caused by alignment, scales with prefix task demand, and is absent in standard MRL.
We connect this to the classical theory of successive refinement, showing that V5 training approximates the optimal multi-resolution code for hierarchical semantic sources.

The practical implications extend beyond classification.
In retrieval systems, fractal embeddings enable \emph{adaptive search}: start with a 64-dimensional prefix for fast coarse filtering, then extend to 256 dimensions only for queries that require fine-grained resolution---achieving the quality of full-resolution search at a fraction of the compute.
In any domain with hierarchical semantics---product taxonomies, medical coding (ICD-10), intent classification, scientific categorization---a single fractal embedding replaces separate coarse and fine models.

Our synthetic hierarchy experiment reveals that the prefix acts as a \emph{routing bottleneck}: steerability peaks when coarse task complexity matches the prefix's representational capacity.
This Goldilocks effect provides principled design guidance: measure $\hlz$ of your hierarchy and size prefix dimensions to match.
The connection to successive refinement theory suggests this is not merely an empirical regularity but a fundamental property of information allocation in hierarchical representations.

\bibliography{references}
\bibliographystyle{plainnat}

%=======================================================================
% APPENDIX
%=======================================================================
\appendix

\section{Entropy Allocation Analysis}
\label{app:entropy}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig7_entropy_allocation.pdf}
    \caption{Disentangling the scaling law. \textbf{Left:} Steerability vs.\ $\hlz$ (prefix task demand) --- the true driver, confirmed by the synthetic experiment. \textbf{Right:} Steerability vs.\ $\hlo$ --- a confounded proxy in observational data. Synthetic data (green diamonds) breaks the confound: $\hlo$ anti-correlates with steerability when $\hlz$ is varied independently.}
    \label{fig:entropy_alloc}
\end{figure}

\section{Full Synthetic Hierarchy Results}
\label{app:synthetic}

See Table~\ref{tab:synthetic} for complete results across all 8 coarse partition sizes.
The experiment uses CLINC-150 text with randomly reassigned hierarchies, holding total class count fixed at 150 while varying $K_0$ from 2 to 75.

\section{Training Convergence}
\label{app:convergence}

All models converge within 5 epochs of head-only training.
Stage 2 (backbone fine-tuning) was tested but provided no improvement over head-only training, consistent with the finding that the frozen backbone already provides sufficient representational capacity for hierarchy-aligned supervision.

\end{document}
