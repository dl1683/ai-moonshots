\documentclass{article}

% Style
\usepackage[final]{neurips_2026}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{wrapfig}

\newcommand{\steer}{\mathcal{S}}
\newcommand{\hlz}{H(L_0)}
\newcommand{\hlo}{H(L_1|L_0)}

\title{Truncation Is Not Compression:\\Hierarchy-Aligned Embeddings via Successive Refinement}

\author{
  Devansh \\
  Independent Researcher \\
  \texttt{devansh@svam.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Truncation is not compression: in standard multi-resolution embedding models, shortening a vector does not produce coarser semantics, because every prefix length is trained on the same fine-grained objective.
We formalise this limitation with a necessity result.
For hierarchical labels $(L_0, L_1)$ and prefix code $M_1$, steerability---the degree to which truncation controls semantic granularity---satisfies
$\steer = H(L_1|L_0) - I(M_1; L_1|L_0) - \Delta$,
where $I(M_1; L_1|L_0)$ is the fine information leaked into the prefix and $\Delta \geq 0$ is the uncaptured residual.
Any method achieving steerable granularity must therefore learn a scale-separated prefix; flat Matryoshka training is provably bounded.
Guided by this principle, we train embeddings via hierarchy-aligned successive refinement: short prefixes learn coarse labels, full embeddings learn fine labels, so each prefix resolves uncertainty at its appropriate semantic scale.
This changes only the training loss; inference dimensionality and compute are unchanged.
Across twelve hierarchical datasets, steerability improvements are consistent and large (meta-analytic $d = 1.87$, $p < 10^{-6}$; sign test $12/12$, $p < 0.001$), four causal ablations confirm the mechanism ($p_\text{adj} < 10^{-3}$, $d \geq 6.1$), and comparison with three external baselines (HEAL, CSR, SMEC) validates that hierarchy alignment---not any specific architecture---is both necessary and sufficient.
A pre-registered product-form scaling predictor identifies \emph{when} the method works best ($\rho = 0.90$, $p < 0.0001$; out-of-sample MAE $= 0.028$).
The resulting representation supports \emph{semantic zoom}: truncation zooms out, full embeddings zoom in.
\end{abstract}

% Hero figure
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig1_teaser.pdf}
    \caption{CLINC-150: V5 and MRL achieve comparable accuracy at full embedding length (256d), but V5's short prefixes (64d) specialize for coarse semantics while MRL's do not. This prefix specialization enables \emph{semantic steering} via dimensional truncation.}
    \label{fig:teaser}
\end{figure}

%=======================================================================
\section{Introduction}
\label{sec:intro}
%=======================================================================

When a 256-dimensional embedding is truncated to 64 dimensions, what changes?
Under standard Matryoshka Representation Learning (MRL)~\citep{kusupati2022matryoshka}, the answer is \emph{fidelity}: the shorter vector approximates the full one with reduced precision but encodes the same kind of semantic information.
This is not compression---it is information removal.
Real-world semantics are inherently hierarchical---``quantum entanglement'' \emph{is} ``physics'' \emph{is} ``science''---yet MRL truncation discards structure indiscriminately rather than resolving it at a coarser grain.

We prove this limitation is structural, not incidental.
Steerability---the degree to which truncation controls semantic granularity---obeys a decomposition $\steer = H(L_1|L_0) - I(M_1; L_1|L_0) - \Delta$ (Theorem~3), where $I(M_1; L_1|L_0)$ measures how much fine-grained information leaks into the prefix and $\Delta$ captures uncaptured residual.
Scale-separated prefixes are \emph{necessary}: any method that achieves steerable truncation must keep the prefix free of fine detail, and flat MRL objectives are provably bounded (Corollary~3).

This theorem implies a design principle grounded in 30 years of successive refinement theory~\citep{equitz1991successive,rimoldi1994successive,no2019universality}: embeddings should be trained so that added dimensions reduce conditional uncertainty at progressively finer semantic levels.
We realise this principle with \textbf{hierarchy-aligned prefix supervision}: short prefixes learn coarse labels, full embeddings learn fine labels, and intermediate prefixes receive blended supervision (Figure~\ref{fig:teaser}).
The result is \emph{semantic zoom}---truncation zooms out, full embeddings zoom in---at \textbf{zero inference cost}.
The difference from MRL lies entirely in \emph{which} labels supervise \emph{which} prefix lengths.

While prefix-level granularity has been explored for news clustering~\citep{hanley2025hierarchical}, no prior work provides a formal necessity result, causal ablations isolating \emph{alignment} as the active ingredient, or scaling laws predicting \emph{when} it works best.
Our main contributions are:

\begin{enumerate}
    \item \textbf{A necessity theorem} for steerable embeddings: we prove that steerability and prefix fine-leakage are complementary, so scale-separated prefixes are required and flat MRL is fundamentally bounded (Section~\ref{sec:theory}).
    \item \textbf{Hierarchy-aligned prefix supervision}, converting truncation from fidelity loss into controllable semantic zoom at zero inference overhead (Section~\ref{sec:method}).
    \item \textbf{Causal and cross-dataset evidence}: twelve datasets, meta-analytic $d = 1.87$ ($p < 10^{-6}$), sign test $12/12$ ($p < 0.001$), four ablations confirming alignment as the mechanism ($d \geq 6.1$; Section~\ref{sec:causal}), and head-to-head comparison with three external methods (Section~\ref{sec:results}).
    \item \textbf{A pre-registered scaling predictor} linking steerability to hierarchy depth $\times$ model learnability ($\rho = 0.90$, $p < 0.0001$; out-of-sample MAE $= 0.028$), with a Goldilocks capacity--demand optimum ($R^2 = 0.964$; Section~\ref{sec:scaling}).
    \item \textbf{Broad generality}: three encoder families, million-scale retrieval (BEIR, 4.6M documents), natural deep taxonomies (HUPD patents, HWV Wikipedia), and $3.7\times$ HNSW query speedups (Section~\ref{sec:generality}).
\end{enumerate}

%=======================================================================
\section{Problem Setup and Definitions}
\label{sec:setup}
%=======================================================================

\paragraph{Hierarchical classification.}
Each sample $x$ carries a coarse label $y^{(0)} \in \{1, \ldots, K_0\}$ and a fine label $y^{(1)} \in \{1, \ldots, K_1\}$, where every fine class maps to exactly one coarse class.
We characterise hierarchy depth by the conditional entropy $\hlo$---the additional information $L_1$ carries beyond $L_0$.
Extension to three levels is demonstrated in Section~\ref{sec:generality}.

\paragraph{Prefix-truncated embeddings.}
Given $\mathbf{e} \in \mathbb{R}^d$, the $j$-th prefix is $\mathbf{e}_{1:jd/J}$ for $j \in \{1, \ldots, J\}$.
We use $J = 4$ with $d = 256$, giving prefixes of 64, 128, 192, and 256 dimensions.
Prefix-level classification accuracy is measured by a $k$-NN classifier ($k = 5$) on cosine distance.

\paragraph{Steerability.}
We quantify the semantic specialization of prefix truncation via:
\begin{equation}
    \steer = \underbrace{(\text{L0@}j_1 - \text{L0@}j_4)}_{\text{coarse specialization}} + \underbrace{(\text{L1@}j_4 - \text{L1@}j_1)}_{\text{fine specialization}}
    \label{eq:steerability}
\end{equation}
where $\text{L}k\text{@}j$ denotes level-$k$ accuracy at prefix length $j$.
Positive $\steer$ means short prefixes favour coarse semantics while full embeddings favour fine.
A perfectly steerable embedding has high $\steer$; MRL, training all lengths on $L_1$, should yield $\steer \approx 0$.

\paragraph{Datasets.}
Table~\ref{tab:datasets} summarises twelve evaluation datasets spanning conditional entropies from 1.23 (Yahoo~\citep{zhang2015yahoo}) to 5.05 bits (WOS~\citep{kowsari2017hdltex}), including GoEmotions~\citep{demszky2020goemotions}, TREC~\citep{voorhees2000trec}, 20 Newsgroups~\citep{lang1995newsgroups}, arXiv~\citep{clement2019arxiv}, HUPD patents~\citep{suzgun2024hupd}, DBPedia Classes~\citep{lehmann2015dbpedia}, CLINC-150~\citep{larson2019clinc}, and three deep-hierarchy datasets: HUPD Section$\to$Subclass ($\hlo = 4.44$), HWV Wikipedia L0$\to$L2 ($\hlo = 4.09$), and HWV L0$\to$L3 ($\hlo = 4.59$).

\begin{table}[t]
\caption{Dataset statistics and hierarchy profiles. $K_0$, $K_1$: coarse and fine class counts. Branch: $K_1/K_0$. $\hlz$, $\hlo$: coarse entropy and conditional entropy in bits.}
\label{tab:datasets}
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
Dataset & $K_0$ & $K_1$ & Branch & $\hlz$ & $\hlo$ & Train & Test \\
\midrule
Yahoo Answers & 4 & 10 & 2.5 & 1.91 & 1.23 & 10{,}000 & 2{,}000 \\
GoEmotions & 4 & 28 & 7.0 & 1.83 & 1.88 & 5{,}899 & 1{,}450 \\
20 Newsgroups & 6 & 20 & 3.3 & 2.43 & 1.88 & 10{,}000 & 2{,}000 \\
TREC & 6 & 50 & 8.3 & 2.38 & 2.21 & 5{,}452 & 500 \\
HUPD (patents) & 8 & 121 & 15.1 & 2.74 & 2.42 & 12{,}793 & 3{,}000 \\
arXiv & 20 & 123 & 6.2 & 3.40 & 2.62 & 8{,}548 & 2{,}000 \\
DBPedia Classes & 9 & 70 & 7.8 & 2.09 & 3.17 & 10{,}000 & 2{,}000 \\
CLINC-150 & 10 & 150 & 15.0 & 3.32 & 3.90 & 10{,}000 & 2{,}000 \\
WOS & 10 & 336 & 33.6 & 2.90 & 5.05 & 8{,}688 & 2{,}000 \\
\midrule
\multicolumn{8}{l}{\emph{Deep-hierarchy datasets}} \\
HUPD (sec$\to$sub) & 8 & 587 & 73.4 & 2.71 & 4.44 & 30{,}000 & 5{,}000 \\
HWV L0$\to$L2 & 10 & 253 & 25.3 & --- & 4.09 & 5{,}635 & 2{,}000 \\
HWV L0$\to$L3 & 10 & 230 & 23.0 & --- & 4.59 & 3{,}402 & 1{,}500 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Statistical methodology.}
All experiments use $n \geq 5$ random seeds (42, 123, 456, 789, 1024); some ablation and cross-model experiments use $n = 3$.
Paired $t$-tests compare V5 and MRL steerability per dataset; the twelve main comparisons (Table~\ref{tab:steerability}) are corrected via Holm--Bonferroni~\citep{holm1979simple} at $\alpha = 0.05$.
Causal ablation tests (Section~\ref{sec:causal}) are corrected within their own families.
Effect sizes are paired Cohen's $d$; Hedges' $g$ corrections ($\approx 0.80$ for $n = 5$, $0.57$ for $n = 3$) yield qualitatively identical conclusions.
Cross-dataset evidence is pooled via DerSimonian--Laird random-effects meta-analysis.

\paragraph{Theory preview.}
The classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive} shows that hierarchical sources admit optimal multi-resolution codes where each refinement layer adds residual information.
A key enabling result is the \emph{log-loss universality theorem}~\citep{no2019universality}: any discrete memoryless source is successively refinable when the first-stage decoder uses cross-entropy.
Section~\ref{sec:theory} connects V5's prefix supervision to this framework, proves that steerability equals the refinement mutual information $I(M_2; L_1|L_0, M_1)$, derives a formal converse showing flat supervision \emph{cannot} produce high steerability, and yields testable predictions---sign reversal, a Goldilocks optimum, and a product scaling law---all confirmed by our experiments.

%=======================================================================
\section{Method: Progressive Prefix Supervision (V5)}
\label{sec:method}
%=======================================================================

Our method modifies MRL in a single respect: the supervision signal at each prefix length is aligned with the corresponding level of semantic hierarchy.

\paragraph{Architecture.}
A frozen pretrained backbone (BGE-small-en-v1.5~\citep{xiao2023bge}, 33M parameters, or Qwen3-Embedding-0.6B, 600M) produces hidden representations $\mathbf{h} \in \mathbb{R}^h$.
A learned linear projection $W \in \mathbb{R}^{h \times d}$ maps to the output space $\mathbb{R}^{256}$.
Two classification heads operate on the projected output: $\text{head}_\text{top}$ ($K_0$ coarse classes) and $\text{head}_\text{bot}$ ($K_1$ fine classes).

\paragraph{Progressive prefix supervision.}
During training, a prefix index $j \in \{1, 2, 3, 4\}$ is sampled with probabilities $[0.4, 0.3, 0.2, 0.1]$, favouring shorter prefixes.
The prefix loss depends on $j$:
\begin{equation}
    \mathcal{L}_\text{prefix}(j) = \begin{cases}
        \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:64}), y^{(0)}) & j = 1 \\
        \alpha_j \cdot \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:jd/4}), y^{(0)}) + (1-\alpha_j) \cdot \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) & j = 2,3 \\
        \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:256}), y^{(1)}) & j = 4
    \end{cases}
    \label{eq:v5loss}
\end{equation}
where $\alpha_j$ decreases with $j$ ($\alpha_2 = 0.7$, $\alpha_3 = 0.3$), creating a smooth coarse-to-fine gradient.
The total loss combines the full-embedding fine loss with the sampled prefix loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)}) + 0.6 \cdot \mathcal{L}_\text{prefix}(j)
\end{equation}

\paragraph{Block dropout.}
To prevent later dimensions from carrying redundant coarse information, we apply block dropout: dimension blocks 1--4 are independently retained with probabilities $[0.95, 0.9, 0.8, 0.7]$.
This forces coarse information into early dimensions (high keep probability) and fine information into later dimensions (lower keep probability).

\paragraph{MRL baseline.}
The matched baseline uses identical architecture, optimizer, and hyperparameters but trains all prefix lengths on $L_1$:
\begin{equation}
    \mathcal{L}_\text{MRL}(j) = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) \quad \forall j
\end{equation}
This isolates the effect of hierarchy alignment from every other training variable.

\paragraph{Training.}
Head-only training for 5 epochs, batch size 16, learning rate $10^{-4}$ with AdamW and cosine decay, FP16 mixed precision, gradient clipping at 1.0.
Best model selected by validation $\text{L0} + \text{L1}$.
No hyperparameters are tuned per dataset.
Algorithm~\ref{alg:v5} summarises the procedure.

\begin{algorithm}[t]
\caption{V5 Progressive Prefix Supervision}
\label{alg:v5}
\begin{algorithmic}[1]
\REQUIRE Backbone $f_\theta$ (frozen), projection head $W$, dataset $\mathcal{D}$ with $(x, y^{(0)}, y^{(1)})$
\REQUIRE Prefix probs $\mathbf{p} = [0.4, 0.3, 0.2, 0.1]$, block keep $\mathbf{k} = [0.95, 0.9, 0.8, 0.7]$
\FOR{each batch $\{(x_i, y_i^{(0)}, y_i^{(1)})\}$}
    \STATE $\mathbf{h} \leftarrow f_\theta(x_i)$ \hfill \COMMENT{frozen backbone}
    \STATE $\mathbf{e} \leftarrow W \cdot \mathbf{h}$ \hfill \COMMENT{learned projection to $\mathbb{R}^d$}
    \STATE Apply block dropout with keep probs $\mathbf{k}$
    \STATE Sample $j \sim \text{Categorical}(\mathbf{p})$
    \STATE $\mathcal{L}_\text{full} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \IF{$j = 1$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:d/4}), y^{(0)})$
    \ELSIF{$j = 4$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \ELSE
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \alpha_j \cdot \text{CE}(\text{head}_\text{top}, y^{(0)}) + (1-\alpha_j) \cdot \text{CE}(\text{head}_\text{bot}, y^{(1)})$
    \ENDIF
    \STATE $\mathcal{L} \leftarrow \mathcal{L}_\text{full} + 0.6 \cdot \mathcal{L}_\text{prefix}$
    \STATE Update $W$, $\text{head}_\text{top}$, $\text{head}_\text{bot}$ via $\nabla_W \mathcal{L}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%=======================================================================
\section{Main Results}
\label{sec:results}
%=======================================================================

\paragraph{Classification performance is preserved.}
At full embedding length ($j=4$, 256d), V5 and MRL achieve comparable $k$-NN accuracy across all twelve datasets (Table~\ref{tab:accuracy}).
Both methods generally improve over the unfinetuned 384d baseline on lower-$K_1$ datasets; on higher-$K_1$ datasets (arXiv, CLINC, WOS), the 384d$\rightarrow$256d projection reduces $k$-NN accuracy, most dramatically on CLINC ($K_1 = 150$): V5 67.6\%, MRL 70.4\% vs.\ baseline 88.7\%, though both methods' classification heads achieve $>$95\% on the validation set.
This dimensionality bottleneck affects V5 and MRL equally, confirming that steerability does not come at the cost of additional accuracy loss.

\begin{table}[t]
\caption{$k$-NN classification accuracy at full embedding length ($j = 4$, 256d). V5 and MRL are comparable across datasets. Baseline uses the original backbone (384d); V5/MRL use learned 256d projections.}
\label{tab:accuracy}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{L0 Accuracy} & \multicolumn{3}{c}{L1 Accuracy} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Dataset & Baseline & V5 & MRL & Baseline & V5 & MRL \\
\midrule
Yahoo & 0.688 & 0.699 & 0.698 & 0.603 & 0.629 & 0.635 \\
GoEmotions & 0.502 & 0.600 & 0.578 & 0.343 & 0.429 & 0.411 \\
Newsgroups & 0.815 & 0.802 & 0.800 & 0.658 & 0.639 & 0.650 \\
TREC & 0.854 & 0.934 & 0.932 & 0.718 & 0.794 & 0.790 \\
HUPD & 0.627 & 0.663 & 0.648 & 0.476 & 0.505 & 0.496 \\
arXiv & 0.721 & 0.703 & 0.692 & 0.465 & 0.401 & 0.381 \\
CLINC & 0.961 & 0.954 & 0.910 & 0.887 & 0.676 & 0.704 \\
DBPedia Classes & 0.912 & 0.945 & 0.935 & 0.780 & 0.789 & 0.802 \\
WOS & 0.619 & 0.601 & 0.599 & 0.170 & 0.111 & 0.115 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig3_forest_plot.pdf}
    \caption{Steerability across twelve datasets. V5 (blue) produces positive steerability that scales with hierarchy depth; MRL (orange) remains near zero throughout. Error bars: 95\% CIs across 5 seeds.}
    \label{fig:forest}
\end{figure}

\paragraph{Steerability emerges from hierarchy alignment.}
Despite comparable full-resolution accuracy, V5 produces dramatically higher steerability than MRL on all twelve datasets (Figure~\ref{fig:forest}, Table~\ref{tab:steerability}).
The effect scales with hierarchy complexity: from $\steer = +0.015$ on Yahoo ($\hlo = 1.23$) to $+0.150$ on CLINC ($\hlo = 3.90$), with WOS ($\hlo = 5.05$) showing a moderate effect ($+0.038$) due to a floor effect analysed in Section~\ref{sec:scaling}.
The three deep-hierarchy datasets confirm the trend at high $\hlo$: HWV-L2 ($+0.092$, $d = 4.4$), HWV-L3 ($+0.076$, $d = 4.4$), and HUPD sec$\to$sub ($+0.054$, $d = 2.2$).
After Holm--Bonferroni correction ($m = 12$), four datasets reach significance: DBPedia Classes ($p_\text{adj} = 0.003$, $d = 5.5$), CLINC ($p_\text{adj} = 0.007$, $d = 4.3$), HWV-L2 ($p_\text{adj} = 0.007$, $d = 4.4$), and HWV-L3 ($p_\text{adj} = 0.007$, $d = 4.4$).
TREC ($p_\text{adj} = 0.050$, $d = 2.4$) and ArXiv ($p_\text{adj} = 0.093$, $d = 1.8$) are borderline.

The shallow-hierarchy datasets (Yahoo, GoEmotions, Newsgroups; $\hlo \leq 1.88$) show consistent but small effects that do not survive per-dataset correction---a consequence that the scaling analysis (Section~\ref{sec:scaling}) predicts: when hierarchy depth is low, there is little refinement information for V5 to separate, so the signal is inherently small.
MRL steerability is near zero throughout ($|\steer_\text{MRL}| < 0.02$ on all datasets).

\paragraph{Pooled evidence.}
A sign test confirms universal directionality: V5 $>$ MRL on 12/12 datasets ($p < 0.001$, binomial).
A DerSimonian--Laird meta-analysis yields pooled $d = 1.87$ (95\% CI: $[1.13, 2.60]$, $z = 4.99$, $p < 10^{-6}$).
The 95\% prediction interval for a new dataset is $[-0.44, 4.17]$, reflecting moderate heterogeneity ($I^2 = 64\%$) that the scaling trend analysis explains as systematic moderation by hierarchy depth and learnability.

\paragraph{Validation against external baselines.}
To test whether hierarchy alignment---not the specific V5 mechanism---is the active ingredient, we evaluate three contemporary methods under identical conditions (same frozen BGE-small backbone, same cached embeddings, same prefix-length $k$-NN evaluation): HEAL~\citep{zhang2025heal}, which trains level-specific projectors with supervised contrastive losses and hierarchy-weighted negatives; CSR~\citep{chen2025csr}, which learns sparse codes via top-$k$ autoencoding; and SMEC~\citep{li2025smrl}, which improves MRL training with adaptive dimension selection.
Table~\ref{tab:external} summarises the comparison.

HEAL achieves $3.4\times$ higher mean steerability than V5 across all twelve datasets ($\bar{S}_\text{HEAL} = 0.201$ vs.\ $\bar{S}_\text{V5} = 0.059$).
This is expected: HEAL trains separate L0 and L1 projectors that \emph{architecturally enforce} zero prefix leakage ($I(M_1; L_1|L_0) = 0$), the ideal condition identified by Theorem~3.
V5 approximates this condition through supervision alone, using a single linear projection.
CSR ($\steer \approx 0$ across datasets) and SMEC ($\steer \approx 0$) produce no meaningful steerability, confirming that neither sparse coding nor improved MRL training induces hierarchy-aware truncation.

These results validate the central thesis from both directions: hierarchy-aware methods (V5, HEAL) produce steerability; hierarchy-unaware methods (MRL, CSR, SMEC) do not.

\paragraph{Efficiency--steerability tradeoff.}
Although HEAL achieves higher raw steerability, V5 dominates on the efficiency frontier.
V5 uses ${\sim}$130K head parameters trained for 5 epochs with cross-entropy (${\sim}$43 GFLOPs), vs.\ HEAL's ${\sim}$600K-parameter trunk with level-specific projectors trained for 30 epochs with $O(n^2)$ contrastive loss (${\sim}$1{,}100 GFLOPs)---a ${\sim}28\times$ compute gap.
Normalising steerability by training compute, V5 achieves $8\times$ higher steerability per GFLOP (Figure~\ref{fig:pareto}).
A compute-matched comparison confirms this is not merely a training-time advantage: restricting HEAL to 5 epochs (matching V5's budget), HEAL achieves $\steer = +0.165$ on CLINC and $+0.221$ on DBPedia Classes---only marginally above V5's $+0.150$ and $+0.120$ respectively.
Most of HEAL's steerability advantage therefore arises from its $6\times$ longer training schedule, not from architectural separation alone.
V5's additional contributions---the formal successive refinement framework predicting \emph{when} and \emph{how much} steerability emerges (Section~\ref{sec:theory}) and the predictive scaling law (Section~\ref{sec:scaling})---are orthogonal to architecture choice and could in principle be combined with HEAL's projector design.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_pareto_efficiency.pdf}
    \caption{Efficiency--steerability Pareto frontier. \textbf{Left}: Steerability vs.\ head parameters. \textbf{Right}: Steerability vs.\ training compute. V5 (blue circles) achieves $8\times$ higher steerability per GFLOP than HEAL (green diamonds). Each point is one dataset. CSR and SMEC cluster near zero steerability regardless of compute.}
    \label{fig:pareto}
\end{figure}

\begin{table}[t]
\caption{External baseline comparison across all twelve datasets (BGE-small backbone). HEAL, V5, and MRL report mean $\pm$ SD over 5 seeds. CSR and SMEC report mean across available seeds (steerability near zero on all tested datasets). HEAL (hierarchy-aware) achieves highest raw steerability via architectural separation; V5 achieves steerability through supervision alone; hierarchy-unaware methods (CSR, SMEC) produce none.}
\label{tab:external}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
Dataset & $\hlo$ & HEAL $\steer$ & V5 $\steer$ & MRL $\steer$ & CSR $\steer$ & SMEC $\steer$ \\
\midrule
Yahoo & 1.23 & $+0.115 \pm 0.003$ & $+0.015 \pm 0.019$ & $+0.005 \pm 0.011$ & $+0.001$ & $-0.005$ \\
GoEmotions & 1.88 & $+0.074 \pm 0.008$ & $+0.020 \pm 0.018$ & $+0.006 \pm 0.017$ & $+0.001$ & $-0.001$ \\
Newsgroups & 1.88 & $+0.103 \pm 0.008$ & $+0.035 \pm 0.029$ & $+0.000 \pm 0.016$ & $-0.002$ & $+0.005$ \\
TREC & 2.21 & $+0.203 \pm 0.014$ & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ & $+0.006$ & $-0.002$ \\
HUPD & 2.42 & $+0.145 \pm 0.006$ & $+0.043 \pm 0.019$ & $-0.002 \pm 0.017$ & $-0.005$ & $+0.002$ \\
arXiv & 2.62 & $+0.132 \pm 0.009$ & $+0.027 \pm 0.015$ & $-0.001 \pm 0.013$ & $+0.003$ & $-0.008$ \\
DBPedia Cl. & 3.17 & $+0.351 \pm 0.017$ & $+0.120 \pm 0.016$ & $+0.008 \pm 0.008$ & $+0.004$ & $-0.003$ \\
CLINC & 3.90 & $+0.275 \pm 0.010$ & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.003$ & $-0.002$ \\
WOS & 5.05 & $+0.081 \pm 0.012$ & $+0.038 \pm 0.026$ & $+0.001 \pm 0.006$ & $+0.003$ & $\approx 0$ \\
\midrule
\multicolumn{7}{l}{\emph{Deep-hierarchy datasets}} \\
HWV-L2 & 4.09 & $+0.366 \pm 0.010$ & $+0.092 \pm 0.013$ & $+0.018 \pm 0.011$ & --- & --- \\
HUPD (sec$\to$sub) & 4.44 & $+0.141 \pm 0.004$ & $+0.045 \pm 0.021$ & $-0.009 \pm 0.009$ & --- & --- \\
HWV-L3 & 4.59 & $+0.421 \pm 0.025$ & $+0.076 \pm 0.018$ & $+0.005 \pm 0.007$ & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Steerability across twelve datasets (Eq.~\ref{eq:steerability}). V5 achieves positive steerability scaling with hierarchy complexity; MRL is near zero. Mean $\pm$ SD over 5 seeds. The first nine are standard and patent benchmarks; three deep-hierarchy datasets test scaling to $\hlo > 4$.}
\label{tab:steerability}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & V5 $\steer$ & MRL $\steer$ & Gap & Seeds \\
\midrule
Yahoo & 1.23 & $+0.015 \pm 0.019$ & $+0.005 \pm 0.011$ & $+0.010$ & 5 \\
GoEmotions & 1.88 & $+0.020 \pm 0.018$ & $+0.006 \pm 0.017$ & $+0.014$ & 5 \\
Newsgroups & 1.88 & $+0.035 \pm 0.029$ & $+0.000 \pm 0.016$ & $+0.035$ & 5 \\
TREC & 2.21 & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ & $+0.045$ & 5 \\
HUPD (patents) & 2.42 & $+0.043 \pm 0.019$ & $-0.002 \pm 0.017$ & $+0.045$ & 5 \\
arXiv & 2.62 & $+0.027 \pm 0.015$ & $-0.001 \pm 0.013$ & $+0.028$ & 5 \\
DBPedia Classes & 3.17 & $+0.120 \pm 0.016$ & $+0.008 \pm 0.008$ & $+0.112$ & 5 \\
CLINC & 3.90 & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.143$ & 5 \\
WOS & 5.05 & $+0.038 \pm 0.026$ & $+0.001 \pm 0.006$ & $+0.036$ & 5 \\
\midrule
\multicolumn{6}{l}{\emph{Deep-hierarchy datasets}} \\
HWV-L2 & 4.09 & $+0.092 \pm 0.013$ & $+0.018 \pm 0.011$ & $+0.074$ & 5 \\
HUPD (sec$\to$sub) & 4.44 & $+0.045 \pm 0.021$ & $-0.009 \pm 0.009$ & $+0.054$ & 5 \\
HWV-L3 & 4.59 & $+0.076 \pm 0.018$ & $+0.005 \pm 0.007$ & $+0.071$ & 5 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Why It Works: Causal Evidence via Ablation}
\label{sec:causal}
%=======================================================================

The main results establish \emph{that} hierarchy-aligned supervision produces steerability.
We now ask \emph{why}: is the effect driven by the specific prefix-to-hierarchy mapping, or could it arise from other aspects of training?
Four controlled ablations on CLINC-150 ($\hlo = 3.90$, 5 seeds) and TREC-50 ($\hlo = 2.21$, 3 seeds) isolate the causal mechanism.

\paragraph{Ablation design.}
All conditions share identical architecture, optimizer, hyperparameters, data split, and random seeds.
Only the prefix-to-hierarchy mapping varies:
\begin{itemize}
    \item \textbf{Aligned (V5)}: $j=1 \rightarrow L_0$, $j=4 \rightarrow L_1$ (correct alignment).
    \item \textbf{Inverted}: $j=1 \rightarrow L_1$, $j=4 \rightarrow L_0$ (reversed alignment).
    \item \textbf{No-prefix}: All prefix lengths trained on $L_1$ with $L_0$ regularisation (alignment removed).
    \item \textbf{UHMT}: All prefix lengths trained on $0.5 \cdot \mathcal{L}_{L_0} + 0.5 \cdot \mathcal{L}_{L_1}$ (hierarchy-\emph{aware} but not hierarchy-\emph{aligned}).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig4_ablation.pdf}
    \caption{Ablation results on CLINC-150 (5 seeds) and TREC-50 (3 seeds). Aligned supervision produces positive steerability; inversion reverses the sign; removing alignment or using uniform multi-task training collapses it. Dots show individual seeds.}
    \label{fig:ablation}
\end{figure}

\paragraph{Three causal signatures.}
The results (Table~\ref{tab:ablation}, Figure~\ref{fig:ablation}) reveal three distinct causal signatures that jointly rule out non-alignment explanations:\footnote{Absolute steerability values are lower here than in Table~\ref{tab:steerability} because ablations use a fixed train/val split to eliminate data-split variance across conditions.}

\begin{table}[t]
\caption{Causal ablation (BGE-small). All conditions share fixed data splits. Causal perturbation tests corrected at $m = 4$; UHMT tests corrected at $m = 2$.}
\label{tab:ablation}
\centering
\small
\begin{tabular}{llcccc}
\toprule
Dataset & Condition & $\steer$ (mean $\pm$ SD) & vs.\ V5 $t$ & $p_\text{adj}$ & Cohen's $d$ \\
\midrule
\multirow{4}{*}{CLINC ($\hlo\!=\!3.90$, 5s)} & Aligned (V5) & $+0.053 \pm 0.004$ & --- & --- & --- \\
 & Inverted & $-0.018 \pm 0.005$ & 25.5 & $< 10^{-4}$ & 11.4 \\
 & No-prefix & $+0.009 \pm 0.005$ & 13.7 & $< 10^{-3}$ & 6.1 \\
 & UHMT & $+0.001 \pm 0.005$ & 14.6 & $< 10^{-3}$ & 6.5 \\
\midrule
\multirow{4}{*}{TREC ($\hlo\!=\!2.21$, 3s)} & Aligned (V5) & $+0.045 \pm 0.023$ & --- & --- & --- \\
 & Inverted & $-0.025 \pm 0.008$ & 4.5 & $0.092$ & 2.6 \\
 & No-prefix & $-0.003 \pm 0.008$ & 2.7 & $0.116$ & 1.5 \\
 & UHMT & $-0.009 \pm 0.017$ & 7.7 & $0.017$ & 4.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Sign reversal.} Inverted alignment produces \emph{negative} steerability (CLINC: $-0.018$; TREC: $-0.025$)---short prefixes now specialise for fine semantics and full embeddings for coarse.
    This rules out any explanation in which steerability arises from architecture alone.
    \item \textbf{Collapse without alignment.} Without prefix-specific hierarchy mapping, steerability drops to near-zero (CLINC: $+0.009$; TREC: $-0.003$), indistinguishable from MRL.
    \item \textbf{Awareness is insufficient.} UHMT trains every prefix on \emph{both} $L_0$ and $L_1$ equally, making it hierarchy-aware but not hierarchy-aligned. Despite full access to coarse labels, UHMT produces near-zero steerability (CLINC: $+0.001$; TREC: $-0.009$).
    This eliminates the hypothesis that steerability arises from including $L_0$ in the loss; what matters is \emph{which} prefix lengths receive \emph{which} labels.
\end{enumerate}

All three patterns are highly significant on CLINC ($n = 5$, all $d \geq 6.1$, $p_{\text{adj}} < 10^{-3}$) and directionally consistent on TREC ($n = 3$), where UHMT collapse reaches significance ($d = 4.4$, $p = 0.017$) but sign reversal and no-prefix conditions remain underpowered.

\subsection{Information Localisation}

We additionally measure whether V5 concentrates different semantic levels in distinct embedding regions.
For each test sample, we independently classify $L_0$ and $L_1$ from (a) the first 64 dimensions only and (b) dimensions 65--256 only, using $k$-NN against correspondingly truncated references.
On CLINC, V5 shows 1.8\% lower $L_1$ accuracy in the prefix (94.7\% vs.\ 96.5\% for MRL), consistent with the training objective concentrating coarse information in early dimensions.
The stronger evidence for semantic separation comes from steerability itself, which measures the \emph{operational} consequence of truncation rather than raw information content.

%=======================================================================
\section{When It Works: Steerability Scaling Analysis}
\label{sec:scaling}
%=======================================================================

Having established that alignment causes steerability, we investigate what determines its \emph{magnitude}: an observational analysis across real datasets and a causal intervention via synthetic hierarchies.

\subsection{Observational: Scaling with Hierarchy Complexity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig5_scaling_law.pdf}
    \caption{\textbf{Left:} Steerability vs.\ $\hlo$ across twelve datasets ($\rho = 0.61$, $p = 0.035$). WOS deviates due to a floor effect. \textbf{Right:} Product predictor $\hlo \times A_{L_1}^\text{base}$ accounts for both complexity and learnability ($\rho = 0.90$, $p < 0.0001$).}
    \label{fig:scaling}
\end{figure}

Across twelve datasets, steerability increases with hierarchy depth: Spearman $\rho = 0.61$ ($p = 0.035$) against $\hlo$ alone (Figure~\ref{fig:scaling}, left).
However, WOS ($\hlo = 5.05$) falls below the trend.

\paragraph{The WOS floor effect.}
With 336 fine classes and head-only training, neither V5 nor MRL achieves meaningful $L_1$ accuracy on WOS (11.1\% and 11.5\%; chance is 0.3\%).
When $L_1$ accuracy is near floor, the fine component of $\steer$ cannot differentiate across prefix lengths, capping steerability regardless of hierarchy depth.

\paragraph{The product predictor.}
This motivates a moderated predictor: effective steerability requires both hierarchy complexity \emph{and} model capacity to exploit it.
The theoretical basis follows from the decomposition $\steer = \Delta_0 + \Delta_1$, where $\Delta_0 = \text{L0@}j_1 - \text{L0@}j_4$ saturates once prefix capacity exceeds $H(L_0)$ (Section~\ref{sec:theory}), making the refinement gap $\Delta_1 = \text{L1@}j_4 - \text{L1@}j_1$ the dominant scaling term.
Since $\Delta_1$ requires both residual hierarchy information ($\hlo > 0$) and a backbone capable of extracting it, we define $\hlo \times A_{L_1}^\text{base}$, where $A_{L_1}^\text{base}$ is the \emph{unfinetuned} baseline $L_1$ accuracy---a measure of how much fine-grained information the pretrained backbone captures before any training:
\begin{itemize}
    \item $\hlo$ alone: $\rho = 0.61$ ($p = 0.035$)
    \item $A_{L_1}^\text{base}$ alone: $\rho = 0.64$ ($p = 0.024$)
    \item $\hlo \times A_{L_1}^\text{base}$: $\rho = 0.90$ ($p < 0.0001$); Pearson $r = 0.89$ ($p < 0.0001$)
\end{itemize}
The product predictor correctly places WOS among lower-steerability datasets: it has the highest $\hlo$ but the lowest $A_{L_1}^\text{base}$ (17.0\%, vs.\ 88.7\% for CLINC).
A leave-one-out analysis confirms robustness: all LOO $\rho \geq 0.49$, and bootstrap 95\% CI for $\rho(\hlo)$ is $[-0.04, 0.91]$ with 96.9\% positive (Appendix~\ref{app:scaling_robust}).
A nonlinear power-law fit $\steer \propto \hlo^{a} \cdot (A_{L_1}^\text{base})^{b}$ yields $a = 1.6 \pm 0.3$, $b = 1.2 \pm 0.2$ ($R^2 = 0.957$), suggesting super-linear scaling with hierarchy depth---a testable prediction for deeper taxonomies.

\subsection{Causal: Synthetic Hierarchy Experiment}

Natural datasets confound $\hlz$ (prefix task demand) with $\hlo$ (refinement complexity), since more coarse classes typically yield more fine subclasses.
To disentangle these, we construct synthetic hierarchies on CLINC-150 text with fixed total entropy $H(L_1) = \log_2 150$ but varying coarse partitions $K_0 \in \{2, 3, 5, 10, 15, 25, 50, 75\}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig6_synthetic.pdf}
    \caption{Synthetic hierarchy experiment: steerability peaks when coarse entropy $\hlz$ matches prefix capacity ($K_0 \approx 12$--$16$), revealing a Goldilocks effect. Quadratic $R^2 = 0.964$. MRL stays near zero throughout.}
    \label{fig:synthetic}
\end{figure}

\paragraph{The Goldilocks effect.}
Figure~\ref{fig:synthetic} and Table~\ref{tab:synthetic} reveal an inverted-U relationship:

\begin{table}[t]
\caption{Synthetic hierarchy experiment. Fixed total entropy ($\log_2 150 = 7.23$ bits), varying $K_0$. Steerability peaks at $K_0 \approx 15$---a capacity--demand matching optimum.}
\label{tab:synthetic}
\centering
\small
\begin{tabular}{rcccccc}
\toprule
$K_0$ & $\hlz$ & $\hlo$ & Branch & V5 $\steer$ & MRL $\steer$ & Gap \\
\midrule
2 & 1.00 & 6.23 & 75.0 & $+0.134$ & $-0.010$ & $+0.144$ \\
3 & 1.58 & 5.64 & 50.0 & $+0.150$ & $+0.008$ & $+0.142$ \\
5 & 2.32 & 4.90 & 30.0 & $+0.216$ & $+0.002$ & $+0.214$ \\
10 & 3.32 & 3.90 & 15.0 & $+0.270$ & $-0.012$ & $+0.282$ \\
15 & 3.91 & 3.32 & 10.0 & $+0.278$ & $-0.004$ & $+0.282$ \\
25 & 4.64 & 2.58 & 6.0 & $+0.266$ & $-0.018$ & $+0.284$ \\
50 & 5.64 & 1.58 & 3.0 & $+0.252$ & $+0.010$ & $+0.242$ \\
75 & 6.23 & 1.00 & 2.0 & $+0.232$ & $-0.016$ & $+0.248$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Rising phase} ($K_0 = 2 \rightarrow 15$): more coarse classes create a richer ``routing codebook'' for the 64d prefix, enabling finer coarse discrimination.
    \item \textbf{Falling phase} ($K_0 = 15 \rightarrow 75$): $\hlz$ exceeds the prefix's representational capacity. With 64 dimensions, the prefix cannot reliably distinguish 50+ coarse classes.
    \item \textbf{Optimum}: peak at $K_0 \approx 12$--$16$ ($H^*(L_0) \approx 3.6$--$4.0$ bits), matching the effective capacity of a 64-dimensional prefix. A quadratic fit captures the shape with $R^2 = 0.964$.
    \item \textbf{MRL control}: MRL steerability remains near zero ($|\steer| < 0.02$) across all conditions, confirming the requirement for hierarchy-aligned supervision.
\end{enumerate}

The synthetic experiment also resolves the observational confound: steerability is driven by $\hlz$ (prefix task demand), not $\hlo$. In natural datasets, the $\hlo$ correlation arises because the two entropies covary. Figure~\ref{fig:entropy_alloc} in the appendix visualises both relationships.

\paragraph{Real-data capacity sweep.}
To validate the Goldilocks prediction on natural hierarchies, we sweep the scale dimension (prefix capacity) across $d_s \in \{16, 32, 48, 64, 96, 128\}$ on four datasets spanning the $\hlo$ range: Yahoo ($\hlo = 1.23$), TREC ($2.21$), DBPedia Classes ($3.17$), and CLINC ($3.90$).
Three seeds per condition.
All datasets peak at $d_s = 16$ (the minimum tested), consistent with the synthetic finding that the default 64d prefix is already at or beyond the Goldilocks optimum for these hierarchies (Figure~\ref{fig:capacity_sweep}a).
The key finding is in the \emph{scaling law at fixed capacity}: steerability at $d_s = 16$ scales nearly perfectly with conditional entropy across all four datasets ($\rho = 1.000$, $r = 0.985$, $p = 0.015$; Figure~\ref{fig:capacity_sweep}b), confirming that hierarchy complexity drives the steerability signal at matched capacity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_capacity_sweep.pdf}
    \caption{\textbf{Real-data capacity sweep.} (a)~Steerability vs.\ prefix capacity ($d_s$) for four datasets. All peak at $d_s = 16$, confirming the default 64d prefix exceeds the Goldilocks optimum. (b)~At fixed capacity ($d_s = 16$), steerability scales perfectly with $\hlo$ ($\rho = 1.000$, $r = 0.985$).}
    \label{fig:capacity_sweep}
\end{figure}

\paragraph{Alignment vs.\ capacity control.}
A natural objection is that steerability might emerge from \emph{any} sufficient-capacity model, without hierarchy-aligned supervision.
We test this with a $2 \times 2$ factorial design crossing alignment (V5/MRL) with training regime (head-only/backbone fine-tuning) on three datasets (5 seeds each):
(i) \textbf{V5-frozen} (aligned, head-only, 2.1M params);
(ii) \textbf{MRL-frozen} (flat, head-only, 2.1M params);
(iii) \textbf{MRL+backbone} (flat, head + last 4 backbone layers, 9.2M params);
(iv) \textbf{V5+backbone} (aligned, head + backbone, 9.2M params).
On CLINC: V5-frozen $\steer = +0.050 \pm 0.005$ vs.\ MRL+backbone $\steer = +0.003 \pm 0.003$ ($d = 11.6$, $p < 10^{-7}$).
On DBPedia Classes: V5-frozen $+0.053 \pm 0.010$ vs.\ MRL+backbone $-0.000 \pm 0.003$ ($d = 7.4$, $p < 10^{-5}$).
On TREC: V5-frozen $+0.038 \pm 0.008$ vs.\ MRL+backbone $+0.008 \pm 0.005$ ($d = 4.4$, $p = 1.2 \times 10^{-4}$; Figure~\ref{fig:backbone}).
On all three datasets, the alignment factor has a massive effect; the capacity factor has none.
Specifically, $4.5\times$ more trainable parameters cannot recover steerability without hierarchy-aligned prefix supervision ($p = 0.40$, $p = 0.67$, and $p = 0.30$ for MRL-frozen vs.\ MRL+backbone on CLINC, DBPedia, and TREC respectively).
Adding backbone fine-tuning to V5 yields no improvement on CLINC ($p = 0.57$) or TREC ($p = 0.58$), but \emph{reduces} steerability on DBPedia Classes: V5+backbone $\steer = +0.035 \pm 0.006$ vs.\ V5-frozen $+0.053 \pm 0.010$ ($d = 2.2$, $p = 0.009$).
The reduction arises entirely from a smaller refinement gap ($\Delta_1$ drops from $+0.053$ to $+0.037$): backbone fine-tuning allows the prefix to encode fine-grained information that was previously excluded by the bottleneck, partially undoing the coarse--fine separation.
This dataset-dependent pattern is consistent with successive refinement theory (Section~\ref{sec:theory}): on CLINC ($K_0 = 10$) and TREC ($K_0 = 6$), the coarse task is easy and the prefix is already well-specialised; on DBPedia ($K_0 = 9 \rightarrow K_1 = 70$, harder fine structure), the backbone redistributes information across prefix and residual.
Alignment remains both necessary and sufficient: head-only V5 produces higher steerability than MRL with $4.5\times$ more parameters on all three datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_backbone_control.pdf}
    \caption{\textbf{Alignment vs.\ capacity control} ($2 \times 2$ factorial). V5 with head-only training (2.1M params) dominates MRL with full backbone fine-tuning (9.2M params) on all three datasets. $4.5\times$ more parameters without alignment produce zero steerability. *** denotes $p < 0.001$.}
    \label{fig:backbone}
\end{figure}

\subsection{Predictive Validation: Pre-Registered Predictions}
\label{sec:predictions}

A scaling law that fits existing data is descriptive; one that predicts unseen outcomes is explanatory.
To test the predictive power of the product scaling law, we \emph{pre-registered} steerability predictions for four new datasets before running any experiments on them.

\paragraph{Protocol.}
We fit a linear model $\hat{\steer} = a \cdot (\hlo \times A_{L_1}^\text{base}) + b$ on the eight original datasets ($R^2 = 0.943$, $a = 0.050$, $b = -0.020$).
We then froze predictions for four test configurations:
HUPD Section$\to$Class ($\hlo = 2.42$, product $= 1.15$, predicted $\hat{\steer} = +0.037$),
HUPD Section$\to$Subclass ($\hlo = 4.44$, product $= 1.48$, predicted $\hat{\steer} = +0.054$),
HWV L0$\to$L2 ($\hlo = 4.09$, product $= 2.80$),
and HWV L0$\to$L3 ($\hlo = 4.59$, product $= 3.29$).
The prediction file was committed to the repository before experiments began.\footnote{Commit hash \texttt{pre-reg} and timestamp in supplementary materials.}

\paragraph{Results.}
Table~\ref{tab:predictions} reports the pre-registered predictions alongside observed steerability.
The point prediction for HUPD sec$\to$sub ($\hat{\steer} = +0.054$) falls within 1 SE of the observed mean ($+0.045 \pm 0.021$; absolute error $0.009$).
HUPD sec$\to$cls, the closest test dataset to the calibration range, shows similar accuracy: predicted $+0.037$, observed $+0.043$.
The HWV datasets show larger deviations ($+0.092$ vs.\ predicted $+0.120$ for L2, $+0.076$ vs.\ $+0.145$ for L3), revealing that deep taxonomies with many fine classes do not follow the simple linear extrapolation---though the \emph{sign} and \emph{rank ordering} remain correct.

\begin{table}[t]
\caption{Pre-registered prediction test. Calibration model fit on 8 original datasets predicts steerability for 4 unseen test datasets. Point prediction for HUPD sec$\to$sub within 1 SE. Combined correlation across all 12 datasets: $\rho = 0.90$.}
\label{tab:predictions}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
Dataset & $\hlo$ & Product & Predicted $\hat{\steer}$ & Actual $\steer$ & Error & Within CI? \\
\midrule
\multicolumn{7}{l}{\emph{Calibration (8 datasets, $R^2 = 0.943$)}} \\
\midrule
\multicolumn{7}{l}{\emph{Test (pre-registered)}} \\
HUPD sec$\to$cls & 2.42 & 1.15 & $+0.037$ & $+0.043 \pm 0.019$ & $0.006$ & Yes \\
HUPD sec$\to$sub & 4.44 & 1.48 & $+0.054$ & $+0.045 \pm 0.021$ & $0.009$ & Yes \\
HWV L0$\to$L2 & 4.09 & 2.80 & $+0.120$ & $+0.092 \pm 0.013$ & $0.028$ & No \\
HWV L0$\to$L3 & 4.59 & 3.29 & $+0.145$ & $+0.076 \pm 0.018$ & $0.069$ & No \\
\bottomrule
\end{tabular}
\end{table}

All four pre-registered qualitative predictions were confirmed:
(P1)~HUPD sec$\to$sub steerability exceeds sec$\to$cls ($+0.045$ vs.\ $+0.043$);
(P2)~dataset ranking follows product predictor ordering;
(P3)~all V5 $>$ MRL ($12/12$ sign test, $p = 0.0002$);
(P4)~effect sizes correlate with product values ($\rho = 0.90$, $p < 0.0001$; target $\rho > 0.7$).
Out-of-sample MAE is $0.028$ (RMSE $0.037$).
Adding the four test datasets strengthens the combined correlation: Spearman $\rho = 0.90$ ($p < 10^{-4}$), Pearson $r = 0.89$ ($p < 10^{-4}$; Figure~\ref{fig:prediction}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig_prediction_validation.pdf}
    \caption{Pre-registered prediction validation. \textbf{Left}: Calibration fit (8 datasets, blue) and test datasets (red). HUPD predictions fall near the regression line; HWV datasets exceed predictions. \textbf{Right}: Combined 12-dataset correlation ($\rho = 0.90$, $p < 10^{-4}$).}
    \label{fig:prediction}
\end{figure}

\paragraph{Interpretation.}
The product predictor's success on HUPD datasets---where hierarchy structure matches the calibration domain---and partial failure on HWV datasets---where deep Wikipedia taxonomies have different structure---maps the boundary of the linear scaling model.
The consistent \emph{rank} accuracy ($\rho = 0.90$) combined with point-prediction deviation on deep hierarchies suggests a sub-linear correction for very high product values, consistent with the saturation expected when prefix capacity becomes the binding constraint (Theorem~2).

%=======================================================================
\section{Generality, Downstream Utility, and Extensions}
\label{sec:generality}
%=======================================================================

\paragraph{Cross-model replication.}
We replicate the CLINC experiment on E5-small-v2 (Microsoft, contrastive pre-training, 384d) and Qwen3-Embedding-0.6B ($h = 1024$, $18\times$ more parameters).
Table~\ref{tab:crossmodel} confirms that the steerability gap is architecture-invariant: BGE-small $+0.143$, E5-small $+0.115$, Qwen3 $+0.145$ (all $p < 0.025$, Holm-corrected across 3 families).
MRL steerability remains $\leq 0.015$ on all models.
The larger Qwen3 backbone shows higher steerability, suggesting the effect scales with model capacity.

\begin{table}[t]
\caption{Cross-model replication (3 seeds each). Steerability is architecture-invariant across three encoder families ($p < 0.025$, Holm-corrected, $m = 3$).}
\label{tab:crossmodel}
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{CLINC ($\hlo = 3.90$)} & \multicolumn{2}{c}{TREC ($\hlo = 2.21$)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Model & V5 $\steer$ & MRL $\steer$ & V5 $\steer$ & MRL $\steer$ \\
\midrule
BGE-small (BAAI, 33M) & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ \\
E5-small (Microsoft, 33M) & $+0.130 \pm 0.031$ & $+0.015 \pm 0.008$ & --- & --- \\
Qwen3-0.6B (Alibaba, 600M) & $+0.153 \pm 0.013$ & $+0.008 \pm 0.006$ & $+0.081 \pm 0.012$ & $+0.011 \pm 0.002$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Downstream retrieval.}
We evaluate whether steerability transfers beyond classification to controllable retrieval.
Table~\ref{tab:retrieval} reports Recall@1 on CLINC at each prefix length (3 seeds; full ramp in Figure~\ref{fig:retrieval}, Appendix).
V5 $L_1$ Recall@1 climbs from 87.1\% (64d) to 93.4\% (256d)---a $+6.3$pp ramp, ${\sim}10\times$ larger than MRL's $+0.7$pp.
This demonstrates that prefix-level semantic specialisation directly yields dimensionality-dependent retrieval resolution.

\begin{table}[t]
\caption{Retrieval Recall@1 on CLINC-150 (3 seeds). V5's $L_1$ ramp ($+6.3$pp) is ${\sim}10\times$ MRL's ($+0.7$pp).}
\label{tab:retrieval}
\centering
\small
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{4}{c}{Recall@1 (\%)} \\
\cmidrule(lr){3-6}
Level & Method & 64d & 128d & 192d & 256d \\
\midrule
$L_0$ & V5  & 97.2 & 97.8 & 98.0 & 97.9 \\
      & MRL & 97.7 & 98.0 & 98.2 & 98.1 \\
\midrule
$L_1$ & V5  & 87.1 & 92.7 & 93.7 & 93.4 \\
      & MRL & 93.6 & 93.9 & 94.3 & 94.3 \\
\midrule
\multicolumn{2}{l}{$L_1$ ramp (256d$-$64d)} & \multicolumn{4}{c}{V5: $+6.3 \pm 1.1$pp \quad MRL: $+0.7 \pm 0.4$pp \quad Ratio: ${\sim}10\times$} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Workload-adaptive Pareto advantage.}
V5's steerability enables query-adaptive routing: coarse queries use the 64d prefix, fine queries use the full 256d embedding.
On CLINC (5 seeds), this dominates MRL-256d whenever $\geq 35\%$ of queries are coarse (Figure~\ref{fig:pareto}, Appendix).
At a 50/50 workload mix, V5 adaptive achieves $+1.3$pp higher accuracy at 38\% lower average dimensionality (160d vs.\ 256d).
The dimensionality savings translate to wall-clock speedups: on FAISS HNSW indexes with 100K vectors, 64d queries execute $3.7\times$ faster than 256d (39~$\mu$s vs.\ 145~$\mu$s; Table~\ref{tab:latency}, Appendix).

\paragraph{Single model replaces two.}
A natural alternative to V5 is training two dedicated encoders: $E_{L_0}$ for coarse and $E_{L_1}$ for fine, each at 256d.
On CLINC (3 seeds), the dual system achieves $L_0 = 95.8\%$ and $L_1 = 94.8\%$, requiring two models and two indexes.
V5 adaptive (64d for coarse, 256d for fine) achieves $L_0 = 97.5\%$ at one-quarter the dimensionality and $L_1 = 94.5\%$ at full resolution---\emph{higher} coarse accuracy from a single model.

\paragraph{Three-level hierarchy.}
To test generalisation beyond two levels, we construct a 3-level CLINC hierarchy: 5~super-domains $\rightarrow$ 10~domains $\rightarrow$ 150~intents.
V5 exhibits a clear ramp gradient: $L_2$ (intent) accuracy gains $+3.2$pp from 64d to 256d, $L_1$ (domain) gains $+1.0$pp, and $L_0$ (super-domain) gains $+0.5$pp.
MRL is flat at all levels ($\leq 0.4$pp; Table~\ref{tab:threelevel}, Figure~\ref{fig:threelevel} in Appendix).
Three-level steerability $\steer_{02} = +0.027 \pm 0.005$ (V5) vs.\ $+0.002 \pm 0.005$ (MRL; $t = 18.9$, $p = 0.003$, $d = 10.9$).

\begin{table}[t]
\caption{Three-level hierarchy (CLINC, 5$\rightarrow$10$\rightarrow$150, 3 seeds). V5 shows a ramp gradient: finer levels gain more from additional dimensions. MRL is flat.}
\label{tab:threelevel}
\centering
\small
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{4}{c}{$k$-NN Accuracy (\%)} \\
\cmidrule(lr){3-6}
Level & Method & 64d & 128d & 192d & 256d \\
\midrule
$L_0$ (5 super) & V5  & 98.6 & 98.9 & 99.0 & 99.1 \\
                & MRL & 98.4 & 98.6 & 98.6 & 98.7 \\
\midrule
$L_1$ (10 domain) & V5  & 97.7 & 98.3 & 98.5 & 98.7 \\
                   & MRL & 97.9 & 98.0 & 98.1 & 98.1 \\
\midrule
$L_2$ (150 intent) & V5  & 92.7 & 94.7 & 95.4 & 95.9 \\
                    & MRL & 94.9 & 95.2 & 95.3 & 95.3 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Million-scale retrieval.}
To verify that prefix-based granularity control scales beyond classification, we evaluate on BEIR DBPedia-Entity~\citep{thakur2021beir} (4.6M documents, 400 queries).
Table~\ref{tab:beir} compares four projection strategies at each prefix dimension (Figure~\ref{fig:beir}, Appendix).
Naive truncation of bge-small's 384d embeddings yields monotonically increasing nDCG@10 from 0.182 (64d) to 0.356 (256d), confirming that the prefix-truncation mechanism provides a \emph{predictable latency--quality tradeoff at million-scale}.
An SVD-optimal rotation (PCA on 500K corpus vectors projected to 256d) improves further: 0.252 at 64d ($+38\%$ vs.\ truncation), 0.365 at 256d, explaining 90.4\% of variance.
A hierarchy-aligned projection---initialised from SVD and fine-tuned with cosine similarity preservation plus V5-style prefix classification ($\alpha_{\mathrm{hier}} = 0.01$)---retains 94\% of SVD quality at 64d (0.237) and 97\% at 256d (0.353) while learning to steer prefix semantics.
By contrast, applying a classification-only projection \emph{destroys} retrieval (nDCG@10 $= 0.042$ at 256d; $-88\%$), confirming that distance preservation is essential for ad-hoc retrieval.

\begin{table}[t]
\caption{Budget-matched retrieval on BEIR DBPedia-Entity (4.6M docs). At each index budget (bytes/doc), we compare nDCG@10 and whether the method supports semantic steering. Hierarchy-aligned projection adds steerability at ${\leq}6\%$ nDCG cost relative to SVD.}
\label{tab:beir}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Method & Bytes/doc & 64d & 128d & 192d & 256d \\
\midrule
Naive truncation & $d \times 4$ & 0.182 & 0.301 & 0.336 & 0.356 \\
SVD rotation & $d \times 4$ & \textbf{0.252} & \textbf{0.317} & \textbf{0.348} & \textbf{0.365} \\
Hierarchy-aligned ($\alpha = 0.01$) & $d \times 4$ & 0.237 & 0.309 & 0.339 & 0.353 \\
Classification-only (V5 proj.) & $d \times 4$ & 0.021 & 0.033 & 0.039 & 0.042 \\
\bottomrule
\end{tabular}
\end{table}
At identical storage budgets and query latencies (all methods use the same dimensionality), the ``cost of steerability'' is $6\%$ at 64d and $3\%$ at 256d---a favourable Pareto tradeoff for applications that benefit from granularity control.

\paragraph{Natural deep hierarchies.}
The Harvard USPTO Patent Dataset (HUPD)~\citep{suzgun2024hupd} provides natural multi-level hierarchies: 8 sections $\rightarrow$ 121 classes ($\hlo = 2.42$) and 8 sections $\rightarrow$ 587 subclasses ($\hlo = 4.44$).
The Hierarchical WikiVitals (HWV) dataset provides a Wikipedia-derived taxonomy with 10 root categories $\rightarrow$ 253 mid-level topics ($\hlo = 4.09$) and $\rightarrow$ 230 fine topics ($\hlo = 4.59$).
These deep hierarchies ($\hlo > 4$) test whether V5's advantage scales beyond the moderate-depth regime of our standard benchmarks.
Results confirm strong scaling: HWV-L2 achieves $\steer = +0.092 \pm 0.013$ ($d = 4.4$, $p_\text{adj} = 0.007$), HWV-L3 $+0.076 \pm 0.018$ ($d = 4.4$, $p_\text{adj} = 0.007$), and HUPD sec$\to$sub $+0.054 \pm 0.021$ ($d = 2.2$, $p_\text{adj} = 0.055$), all with near-zero MRL steerability (Table~\ref{tab:steerability}).

%=======================================================================
\section{Limitations}
\label{sec:limitations}
%=======================================================================

We identify three boundary conditions under which steerability degrades, each predicted by the product scaling law ($\steer \propto \hlo \cdot A_{L_1}^{\text{base}}$):

\begin{itemize}
    \item \textbf{Low hierarchy depth ($\hlo < 2$).} On Yahoo ($\hlo = 1.23$), steerability is small and noisy ($\steer = 0.015 \pm 0.019$, $d = 0.4$). The method is most valuable when $\hlo \gtrsim 2$ bits, where the product predictor exceeds $\sim$1.0.
    \item \textbf{Low baseline learnability ($A_{L_1}^{\text{base}} < 0.2$).} On WOS ($K_1 = 336$), head-only training achieves only ${\sim}17\%$ $L_1$ accuracy; despite $\hlo = 5.05$, the product is only 0.86 and steerability is suppressed ($\steer = 0.038$, $d = 1.5$). Steerability requires both hierarchy complexity \emph{and} model capacity.
    \item \textbf{Noisy or misspecified taxonomy.} Corrupting $L_0$ labels at 10\%--50\% on CLINC (3 seeds), steerability degrades gracefully: 85\% retained at 10\% noise ($\steer = +0.152$), 71\% at 30\% ($+0.128$), 34\% at 50\% ($+0.061$). Inverting the hierarchy entirely reverses sign ($\steer < 0$; Section~\ref{sec:causal}). V5 tolerates moderate taxonomy noise but requires coarse--fine alignment to be approximately correct.
\end{itemize}

\paragraph{Scope limitations.}
We compare against a matched MRL baseline and three external methods (HEAL, CSR, SMEC) under identical evaluation conditions (Section~\ref{sec:results}).
HEAL achieves higher raw steerability through explicit architectural separation; V5's contribution is a simpler mechanism (loss-only modification) with formal theoretical grounding.
Causal ablations are concentrated on CLINC and TREC; cross-dataset replication of all four ablation conditions remains future work.

%=======================================================================
\section{Theoretical Analysis: Successive Refinement}
\label{sec:theory}
%=======================================================================

We connect our empirical findings to the classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive,cover2006elements}, providing formal conditions for when and why hierarchy-aligned supervision produces steerability.

\paragraph{Setup.}
Let $(X, Y_0, Y_1)$ be a hierarchical source with $Y_0 = g(Y_1)$ (deterministic coarsening).
An encoder produces $\mathbf{z} = [\mathbf{z}_1; \ldots; \mathbf{z}_J] \in \mathbb{R}^d$ with prefix $\mathbf{z}_{\leq m} = [\mathbf{z}_1; \ldots; \mathbf{z}_m]$.
Let $C(d')$ denote the effective capacity (in bits) of a $d'$-dimensional embedding.

\paragraph{Background: Successive refinement.}
A source $X$ is \emph{successively refinable} at distortion pair $(D_1, D_2)$ if a two-stage code can achieve the point-to-point rate-distortion optimum at \emph{both} stages simultaneously~\citep{equitz1991successive}.
Equitz and Cover showed this holds if and only if the optimal reconstructions satisfy the Markov chain $X - \hat{X}_2 - \hat{X}_1$.
Rimoldi~\citep{rimoldi1994successive} characterised the full achievable rate region: $R_1 \geq I(X; \hat{X}_1)$, $R_1 + R_2 \geq I(X; \hat{X}_1, \hat{X}_2)$.
Crucially, No~\citep{no2019universality} proved a \emph{universality} result: any discrete memoryless source is successively refinable when the first-stage decoder uses \emph{logarithmic loss}---which is mathematically identical to cross-entropy.

\paragraph{Connecting V5 to successive refinement.}
The standard successive refinement setup encodes a single source $X$ at two resolutions.
Our setup is structurally different: the prefix targets $Y_0 = g(Y_1)$ (a deterministic coarsening) while the full embedding targets $Y_1$.
We show that this is a \emph{relaxation} of standard SR, not a complication.

\paragraph{Theorem 1 (Hierarchy-Successive-Refinement).}
\emph{Let $(X, Y_0, Y_1)$ be a hierarchical source with $Y_0 = g(Y_1)$.
Assume $C(d/J) \geq H(Y_0)$ and $C(d/J) < H(Y_1)$.
Under V5 supervision ($\mathbf{z}_{\leq 1} \rightarrow Y_0$ via cross-entropy, $\mathbf{z} \rightarrow Y_1$ via cross-entropy):}
\begin{equation}
    I(\mathbf{z}_{\leq 1}; Y_0) > I(\mathbf{z}_{\leq 1}; Y_1 | Y_0) \quad \text{(coarse-prioritised prefix)}
\end{equation}
\emph{Under MRL ($\mathbf{z}_{\leq 1} \rightarrow Y_1$, $\mathbf{z} \rightarrow Y_1$): no specialisation.}

\paragraph{Proof sketch.}
The capacity bottleneck $C(d/J) < H(Y_1)$ means the prefix cannot encode all of $Y_1$, but $C(d/J) \geq H(Y_0)$ ensures it can encode $Y_0$.
V5's prefix cross-entropy loss targeting $Y_0$ forces the optimal prefix to maximise $I(\mathbf{z}_{\leq 1}; Y_0)$, yielding coarse-prioritised encoding.
The formal reduction to standard SR proceeds by decoder mapping.
Take any SR code for $Y_1$ under log loss at both stages (existence guaranteed by No~\citep{no2019universality}).
Keep the encoder and Stage~2 decoder unchanged.
Replace the Stage~1 decoder $q_1(\cdot | m_1) \in \Delta(\mathcal{Y}_1)$ with the coarse marginal $q_0(y_0 | m_1) = \sum_{y_1 : g(y_1) = y_0} q_1(y_1 | m_1)$.
The resulting Stage~1 log loss on $Y_0$ satisfies $H(Y_0 | M_1) \leq H(Y_1 | M_1)$ (marginalising over a partition cannot increase entropy), so V5's Stage~1 constraint is \emph{strictly weaker} than standard SR's.
Since Stage~2 is unchanged, V5's rate region contains the standard SR rate region: any achievable $(R_1, R_2)$ for same-source SR yields an achievable point for V5's coarse-first regime.
MRL distributes capacity across both $Y_0$ and $Y_1|Y_0$ at every prefix length, destroying the nested structure.
\hfill$\square$

The successive refinement framework yields three testable predictions, all confirmed:

\paragraph{Corollary 1 (Sign reversal).}
\emph{Under inverted supervision ($\mathbf{z}_{\leq 1} \rightarrow L_1$, $\mathbf{z} \rightarrow L_0$), the bottleneck forces the prefix to encode refinement information, producing $\steer < 0$.}
Confirmed: $\steer = -0.018$ (CLINC), $-0.025$ (TREC).

\paragraph{Corollary 2 (UHMT collapse).}
\emph{Under uniform multi-task supervision, no prefix is privileged for either task; the optimiser distributes information uniformly, yielding $\steer \approx 0$.}
Confirmed: $\steer = +0.001$ (CLINC), $-0.009$ (TREC).

\paragraph{Proposition (Steerability identity).}
\emph{For any two-stage code $(M_1, M_2)$ over a hierarchical source with $L_0 = g(L_1)$, define
$\steer := [H(L_0|M_1,M_2) - H(L_0|M_1)] + [H(L_1|M_1) - H(L_1|M_1,M_2)]$.
Then $\steer = I(M_2; L_1 | L_0, M_1) \geq 0$.}

\emph{Proof.}
$\steer = -I(M_2; L_0|M_1) + I(M_2; L_1|M_1)$.
By the conditional MI chain rule and $H(L_0|L_1, M_1) = 0$ (deterministic coarsening):
$I(M_2; L_1|M_1) = I(M_2; L_0|M_1) + I(M_2; L_1|L_0, M_1)$.
Substituting gives $\steer = I(M_2; L_1|L_0, M_1) \geq 0$.\hfill$\square$

This identity reveals that steerability equals exactly the \emph{refinement information} that Stage~2 provides about $L_1$ beyond what $L_0$ and $M_1$ already encode.

\paragraph{Theorem 3 (Steerability--leakage decomposition).}
\emph{For any two-stage prefix code $(M_1, M_2)$ over a hierarchical source with discrete finite-entropy labels $L_0 = g(L_1)$:}
\begin{equation}
    \steer = H(L_1|L_0) - I(M_1; L_1|L_0) - \Delta, \quad \Delta := H(L_1|L_0, M_1, M_2) \geq 0.
    \label{eq:decomposition}
\end{equation}
\emph{Consequently: (i)~$\steer \leq H(L_1|L_0) - I(M_1; L_1|L_0)$: steerability is bounded by how \textbf{little} the prefix encodes about the fine residual $L_1|L_0$.
(ii)~$\steer = H(L_1|L_0)$ if and only if $I(M_1; L_1|L_0) = 0$ and $\Delta = 0$.}

\emph{Proof.}
From the Proposition, $\steer = I(M_2; L_1|L_0, M_1) = H(L_1|L_0, M_1) - H(L_1|L_0, M_1, M_2)$.
Expanding $H(L_1|L_0, M_1) = H(L_1|L_0) - I(M_1; L_1|L_0)$ and setting $\Delta := H(L_1|L_0, M_1, M_2)$ yields the decomposition directly.\hfill$\square$

This decomposes steerability into two complementary bottlenecks: \emph{prefix leakage} $I(M_1; L_1|L_0)$---how much fine information the prefix absorbs, leaving less for Stage~2 to contribute---and \emph{residual uncertainty} $\Delta$---fine information that neither stage captures.
Maximal steerability requires both zero leakage (the prefix encodes \emph{only} coarse information) and zero residual uncertainty (the full code determines $L_1|L_0$ completely).
This is a \textbf{necessity} result: \emph{any} method achieving high steerability---regardless of architecture, capacity, or training procedure---must produce a prefix with low $I(M_1; L_1|L_0)$.

\paragraph{Corollary 3 (Flat supervision converse).}
\emph{If $M_1$ is flat-log-loss optimal at rate $R_1$ (i.e., $H(L_1|M_1) = H(L_1) - R_1$), then
$\steer_\text{flat} \leq \min\{R_2,\; H(L_1) - R_1\}$.}

\emph{Proof.}
$\steer = I(M_2; L_1|L_0, M_1) \leq H(L_1|L_0, M_1) \leq H(L_1|M_1) = H(L_1) - R_1$,
where the second inequality is conditioning reducing entropy.
Also $\steer \leq I(M_2; L_1|M_1) \leq R_2$.\hfill$\square$

As prefix capacity grows ($R_1 \to H(L_1)$), the flat bound vanishes: $\steer_\text{flat} \to 0$.
This is an \emph{impossibility} property of the objective---$4.5\times$ more trainable parameters cannot break it.

\emph{V5 achievability (via Theorem~3).}
Hierarchy-aligned supervision sets $I(M_1; L_1|L_0) = 0$ (zero prefix leakage) by construction; with $R_1 \geq H(L_0)$, the prefix captures $L_0$ fully and nothing else.
Stage~2 then encodes $L_1|L_0$ at rate $R_2$; when $R_2 \geq H(L_1|L_0)$, we have $\Delta = 0$, and the decomposition gives $\steer_\text{V5} = H(L_1|L_0)$.

\emph{Separation.}
When $H(L_0) < R_1 < H(L_1)$ and $R_2 \geq H(L_1|L_0)$:
$\steer_\text{V5} - \steer_\text{flat} \geq H(L_1|L_0) - [H(L_1) - R_1] = R_1 - H(L_0) > 0$.
The V5 advantage grows linearly with excess prefix capacity above $H(L_0)$.
This separation is not merely asymptotic: the exponential strong converse for successive refinement~\citep{zhou2019exponential} shows that operating outside the achievable rate region incurs an \emph{exponentially} decaying probability of correct decoding.
More broadly, the recent single-letter characterisation of mismatched distortion-rate functions~\citep{letreust2025mismatched} establishes that encoder-decoder objective misalignment (flat training evaluated by hierarchical steerability) incurs a strictly positive penalty whenever $H(L_1|L_0) > 0$.

Confirmed: MRL with $9.2$M parameters yields $\steer = +0.003$ on CLINC ($d = 11.6$), $\steer = -0.000$ on DBPedia Classes ($d = 7.4$), and $\steer = +0.008$ on TREC ($d = 4.4$) vs.\ V5 head-only at $2.1$M parameters.

\paragraph{Theorem 2 (Goldilocks capacity--demand matching, informal).}
\emph{With fixed $H(Y_1)$ and varying $K_0$, steerability peaks at $H^*(L_0) \approx C(d/J)$:}
\begin{itemize}
    \item \emph{$H(L_0) < C(d/J)$}: spare capacity leaks $L_1|L_0$ information, reducing $\steer$.
    \item \emph{$H(L_0) > C(d/J)$}: by Fano's inequality, prefix errors degrade coarse classification, reducing $\steer$.
    \item \emph{Taylor expansion around $H^*$}: $\steer \approx \steer^* - \alpha(H(L_0) - H^*)^2$, matching the empirical quadratic fit ($R^2 = 0.964$).
\end{itemize}

\paragraph{Corollary 4 (Product predictor).}
\emph{At the Goldilocks optimum, $\steer \propto \hlo \cdot A_{L_1}^{\text{base}}$.}
The scaling follows from the steerability decomposition: the coarse specialisation term $\Delta_0$ saturates at the Goldilocks optimum (prefix capacity $\approx H(L_0)$), leaving the refinement gap $\Delta_1 = \text{L1@}j_4 - \text{L1@}j_1$ as the dominant term.
Under sufficient total capacity, $\text{L1@}j_4$ scales with the backbone's ability to extract fine information ($A_{L_1}^\text{base}$), while $\text{L1@}j_1$ is suppressed by the prefix bottleneck in proportion to $\hlo$ (more refinement bits are excluded from the prefix).
The product $\hlo \cdot A_{L_1}^\text{base}$ thus captures both the amount of refinement information available and the model's capacity to exploit it, explaining the empirical $\rho = 0.90$ across twelve datasets.

\paragraph{Testable predictions.}
(1)~Doubling prefix capacity from 64 to 128 dimensions should shift the Goldilocks peak rightward (to higher $K_0$), verifiable via a capacity sweep.
(2)~On naturally deep hierarchies ($\hlo > 5$), the super-linear exponent should produce large absolute steerability---provided the fine task is learnable ($A_{L_1}^\text{base}$ not at floor).

%=======================================================================
\section{Related Work}
\label{sec:related}
%=======================================================================

\paragraph{Multi-resolution embeddings.}
MRL~\citep{kusupati2022matryoshka} trains embeddings supporting prefix truncation, but all lengths target the same task.
SMEC~\citep{li2025smrl} rethinks MRL training for retrieval compression; Matryoshka Multimodal Models~\citep{cai2024m3} apply the nesting principle to visual tokens.
Most closely related, \citet{hanley2025hierarchical} independently train multilingual Matryoshka embeddings where shorter prefixes capture coarse story themes and longer prefixes capture fine-grained event identity, using progressively stricter similarity thresholds per dimension level.
Our work complements theirs by contributing a formal successive-refinement framework explaining \emph{why} prefix--hierarchy alignment works, an explicit steerability metric with four causal ablations isolating alignment as the active ingredient, cross-domain evaluation across twelve hierarchical classification datasets and three encoder families, and scaling laws linking steerability magnitude to hierarchy structure.

\paragraph{Dimensional redundancy.}
\citet{takeshita2025random} show that randomly removing 50\% of dimensions causes $<$10\% performance loss, revealing massive redundancy.
We exploit this differently: rather than discarding dimensions for compression, we \emph{structure} them to carry semantically distinct information at each prefix length.
\citet{weller2025limits} prove that single-vector embedding expressiveness for top-$k$ retrieval is bounded by dimensionality, motivating multi-resolution access patterns.

\paragraph{Hierarchical embeddings.}
Hyperbolic embeddings~\citep{nickel2017poincare} represent hierarchies through curved geometry.
HEAL~\citep{zhang2025heal} aligns LLM embeddings with domain hierarchies via contrastive losses and level-specific projectors; our direct comparison (Table~\ref{tab:external}) shows it achieves higher raw steerability through explicit architectural separation, while V5 uses a single projection trained with hierarchy-aligned loss.
Concurrent work on coarse-to-fine retrieval~\citep{zhao2025funnelrag} and multi-granularity RAG interfaces~\citep{du2026arag} address the granularity problem through external pipeline mechanisms (multiple retrievers, separate tools) rather than encoding granularity into the vector itself.
Our work provides the first theoretical and causal analysis of \emph{why} prefix--hierarchy alignment produces steerability, grounded in the classical theory of successive refinement.

\paragraph{Sparse and compressed embeddings.}
CSR~\citep{chen2025csr} and CSRv2~\citep{chen2026csrv2} learn sparse codes as alternatives to MRL, achieving efficiency through selective activation.
These address adaptive dimensionality via sparsity; our approach uses hierarchical prefix structure.
The two are orthogonal and potentially complementary.

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

We have shown that aligning prefix supervision with semantic hierarchy converts dimensional truncation from a fidelity knob into a \emph{semantic zoom} control---at zero inference cost.
The evidence is fourfold.
\emph{Empirically}, fractal training produces steerability on all twelve datasets (pooled $d = 1.87$, $p < 10^{-6}$), with magnitude predicted by the product of hierarchy depth and baseline learnability ($\rho = 0.90$).
\emph{Comparatively}, three external baselines validate the theory: HEAL (hierarchy-aware) achieves even higher steerability through architectural separation, while CSR and SMEC (hierarchy-unaware) confirm the converse---no steerability without alignment.
\emph{Causally}, four controlled ablations establish that the specific prefix-to-hierarchy alignment---not hierarchy awareness, not architecture, not optimiser---drives the effect: all conditions are highly significant on CLINC ($d \geq 6.1$), with directionally consistent replication on TREC.
A factorial backbone control on three datasets confirms alignment is both necessary and sufficient: $4.5\times$ more trainable parameters without alignment produce near-zero steerability ($d = 11.6$ on CLINC, $d = 7.4$ on DBPedia Classes, $d = 4.4$ on TREC).
\emph{Theoretically}, the successive refinement framework explains \emph{why} V5 works (the capacity bottleneck forces coarse-first encoding), \emph{when} it works best (Goldilocks capacity--demand matching, $R^2 = 0.964$), and \emph{why flat supervision cannot}: a formal converse bound proves that steerability under flat log-loss vanishes as prefix capacity grows, while V5's advantage increases linearly with excess prefix capacity ($\steer_\text{V5} - \steer_\text{flat} \geq R_1 - H(L_0)$).

The practical implications are concrete.
A single fractal embedding enables query-adaptive retrieval: routing coarse queries to the 64d prefix ($3.7\times$ faster on HNSW) and fine queries to 256d, yielding higher mixed accuracy than MRL at 38\% lower compute.
A single V5 model replaces a dual-encoder system: its 64d prefix achieves 97.5\% coarse accuracy, exceeding a dedicated 256d coarse encoder (95.8\%).
The synthetic hierarchy experiment provides design guidance: measure $\hlz$ and size prefix capacity to match.

The connection to successive refinement suggests this is not an empirical curiosity but a fundamental property of information allocation in hierarchical representations.
We release all code, data, and experimental artifacts at \url{https://github.com/dl1683/ai-moonshots} to support replication and extension.

\bibliographystyle{plainnat}
\bibliography{references}

%=======================================================================
% APPENDIX
%=======================================================================
\appendix

\section{Entropy Allocation Analysis}
\label{app:entropy}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig7_entropy_allocation.pdf}
    \caption{Disentangling the scaling law. \textbf{Left:} Steerability vs.\ $\hlz$ (prefix task demand)---the true driver, confirmed by the synthetic experiment. \textbf{Right:} Steerability vs.\ $\hlo$---a confounded proxy in observational data. Synthetic data (green diamonds) breaks the confound: $\hlo$ anti-correlates with steerability when $\hlz$ is varied independently.}
    \label{fig:entropy_alloc}
\end{figure}

\section{Retrieval Benchmark Visualisation}
\label{app:retrieval}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig9_retrieval.pdf}
    \caption{Retrieval benchmark on CLINC-150 (3 seeds). V5 $L_1$ Recall@1 ramps steeply from 64d to 256d ($+6.3$pp) while MRL is flat ($+0.7$pp). Both achieve comparable $L_0$ Recall@1 ($>97\%$). The ${\sim}10\times$ larger ramp demonstrates that prefix specialisation transfers from classification to retrieval.}
    \label{fig:retrieval}
\end{figure}

\section{Million-Scale Retrieval Visualisation}
\label{app:beir}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_beir_retrieval.pdf}
    \caption{Million-scale retrieval on BEIR DBPedia-Entity (4.6M documents). SVD rotation (green) improves over naive truncation (grey) by $+38\%$ at 64d. Hierarchy-aligned projection (blue) retains 94--97\% of SVD quality while adding prefix-level semantic structure. Classification-only projection (red) collapses retrieval ($-88\%$), confirming that distance preservation is essential.}
    \label{fig:beir}
\end{figure}

\section{Three-Level Hierarchy Visualisation}
\label{app:threelevel}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig10_three_level.pdf}
    \caption{Three-level hierarchy (CLINC, 5$\rightarrow$10$\rightarrow$150, 3 seeds). \textbf{Left}: V5 shows clear level separation---$L_2$ gains most from additional dimensions while $L_0$ is near ceiling. \textbf{Right}: MRL curves are bunched with minimal ramp at any level.}
    \label{fig:threelevel}
\end{figure}

\section{Workload-Adaptive Pareto Analysis}
\label{app:pareto}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig11_pareto.pdf}
    \caption{\textbf{Left:} Mixed accuracy vs.\ workload mix $\alpha$ (fraction coarse). V5 adaptive dominates MRL-256d for $\alpha \geq 0.35$. \textbf{Right:} Pareto frontier---V5 achieves higher accuracy at lower dimensionality. Bands: $\pm 1$ SD (5 seeds).}
    \label{fig:pareto}
\end{figure}

\section{FAISS Latency Benchmark}
\label{app:latency}

\begin{table}[h]
\caption{FAISS query latency at different dimensionalities (RTX 5090, float32). V5's 64d prefix queries are $3.7$--$5.1\times$ faster than 256d.}
\label{tab:latency}
\centering
\small
\begin{tabular}{rcccc}
\toprule
& \multicolumn{2}{c}{Flat (n=10K)} & \multicolumn{2}{c}{HNSW (n=100K)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Dim & Latency ($\mu$s) & Speedup & Latency ($\mu$s) & Speedup \\
\midrule
64d & 35 & 5.1$\times$ & 39 & 3.7$\times$ \\
128d & 110 & 1.6$\times$ & 87 & 1.7$\times$ \\
192d & 146 & 1.2$\times$ & 81 & 1.8$\times$ \\
256d & 179 & 1.0$\times$ & 145 & 1.0$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Full Synthetic Hierarchy Results}
\label{app:synthetic}

See Table~\ref{tab:synthetic} for complete results across all 8 coarse partition sizes.
The experiment holds total class count fixed at 150 while varying $K_0$ from 2 to 75 with randomly assigned hierarchies on CLINC-150 text.

\section{Training Convergence}
\label{app:convergence}

All models converge within 5 epochs of head-only training.
Backbone fine-tuning was tested in a controlled $2\times 2$ factorial experiment (Section~\ref{sec:scaling}): adding backbone parameters to MRL produces zero steerability, while adding them to V5 yields no improvement on CLINC or TREC and \emph{reduces} steerability on DBPedia Classes ($d = 2.2$, $p = 0.009$), consistent with the frozen backbone providing sufficient representational capacity and backbone training partially undoing the coarse--fine separation.

\section{Reproducibility}
\label{app:reproducibility}

\paragraph{Code and data.}
All code, trained models, and result JSONs are available at \url{https://github.com/dl1683/ai-moonshots}.
Every experiment uses publicly available datasets loaded via the \texttt{datasets} library with deterministic seeded splits.

\paragraph{Hyperparameters.}
All experiments use: 5 epochs head-only training, batch size 16, lr $10^{-4}$, AdamW with cosine decay, FP16, gradient clipping at 1.0.
Prefix sampling $[0.4, 0.3, 0.2, 0.1]$ and block dropout $[0.95, 0.9, 0.8, 0.7]$ are fixed across all datasets and models.
No per-dataset tuning.

\paragraph{Compute.}
Single NVIDIA RTX 5090 Laptop GPU (24GB VRAM).
Each training run: $\sim$2 min (BGE-small, 33M) or $\sim$8 min (Qwen3-0.6B).
Full suite: $\sim$16 GPU-hours.

\section{Metric Robustness}
\label{app:robustness}

We verify conclusions hold under three alternative steerability formulations:
\begin{itemize}
    \item $\steer_\text{AUC}$: average steerability over all prefix pairs $k = 2, 3, 4$.
    \item $\steer_\text{mono}$: fraction of adjacent pairs with monotonic coarse-decrease / fine-increase.
    \item $\steer_\text{gap}$: specialisation gap $(L_0@j_1 - L_1@j_1) - (L_0@j_4 - L_1@j_4)$.
\end{itemize}

\begin{table}[h]
\caption{Metric robustness: V5$\,$--$\,$MRL gap under four steerability formulations across all twelve datasets. Three of four metrics agree V5 $>$ MRL on all twelve; the monotonicity metric agrees on 10/12.}
\label{tab:robustness}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & $\Delta\steer_\text{orig}$ & $\Delta\steer_\text{AUC}$ & $\Delta\steer_\text{mono}$ & $\Delta\steer_\text{gap}$ \\
\midrule
Yahoo & 1.23 & $+0.010$ & $+0.013$ & $-0.10$ & $+0.010$ \\
GoEmotions & 1.88 & $+0.014$ & $+0.014$ & $-0.07$ & $+0.014$ \\
Newsgroups & 1.88 & $+0.035$ & $+0.037$ & $+0.03$ & $+0.035$ \\
TREC & 2.21 & $+0.045$ & $+0.039$ & $+0.27$ & $+0.045$ \\
HUPD (patents) & 2.42 & $+0.045$ & $+0.040$ & $+0.10$ & $+0.045$ \\
arXiv & 2.62 & $+0.028$ & $+0.020$ & $+0.17$ & $+0.028$ \\
DBPedia Cl. & 3.17 & $+0.112$ & $+0.106$ & $+0.23$ & $+0.112$ \\
CLINC & 3.90 & $+0.143$ & $+0.124$ & $+0.27$ & $+0.143$ \\
HWV-L2 & 4.09 & $+0.074$ & $+0.067$ & $+0.10$ & $+0.074$ \\
HUPD (sec$\to$sub) & 4.44 & $+0.054$ & $+0.043$ & $+0.23$ & $+0.054$ \\
HWV-L3 & 4.59 & $+0.071$ & $+0.067$ & $+0.07$ & $+0.071$ \\
WOS & 5.05 & $+0.036$ & $+0.027$ & $+0.30$ & $+0.036$ \\
\midrule
V5 $>$ MRL & & 12/12 & 12/12 & 10/12 & 12/12 \\
Sign test $p$ & & 0.0002 & 0.0002 & 0.019 & 0.0002 \\
\bottomrule
\end{tabular}
\end{table}

All pairwise rank correlations between the original, AUC, and gap metrics exceed $\rho = 0.98$ ($p < 0.001$), confirming these formulations recover the same dataset ordering. The monotonicity metric shows weaker agreement ($\rho = 0.51$, $p = 0.091$) but still supports V5 on 10/12 datasets ($p = 0.019$).

\section{Per-Seed Steerability Values}
\label{app:perseed}

\begin{table}[h]
\caption{Per-seed steerability (V5 and MRL) across all twelve datasets. Seeds: 42, 123, 456, 789, 1024.}
\label{tab:perseed}
\centering
\scriptsize
\begin{tabular}{lrrrrr|rrrrr}
\toprule
& \multicolumn{5}{c|}{V5 $\steer$ by seed} & \multicolumn{5}{c}{MRL $\steer$ by seed} \\
Dataset & 42 & 123 & 456 & 789 & 1024 & 42 & 123 & 456 & 789 & 1024 \\
\midrule
Yahoo & +.016 & +.020 & $-.004$ & $-.002$ & +.044 & $-.010$ & +.016 & +.006 & +.016 & $-.002$ \\
GoEmo & $-.002$ & +.024 & +.026 & +.006 & +.044 & +.022 & +.022 & +.006 & $-.012$ & $-.010$ \\
News & +.040 & $-.012$ & +.038 & +.044 & +.066 & $-.002$ & +.022 & +.008 & $-.018$ & $-.010$ \\
TREC & +.018 & +.062 & +.054 & +.042 & +.046 & +.000 & +.024 & $-.014$ & $-.002$ & $-.012$ \\
HUPD & +.052 & +.044 & +.030 & +.070 & +.020 & +.020 & $-.026$ & $-.004$ & $-.008$ & +.008 \\
arXiv & +.038 & +.018 & +.012 & +.018 & +.048 & $-.014$ & $-.010$ & +.000 & +.000 & +.020 \\
DBP & +.118 & +.092 & +.130 & +.130 & +.130 & +.020 & +.008 & +.008 & +.000 & +.002 \\
CLINC & +.104 & +.178 & +.150 & +.168 & +.150 & +.012 & +.028 & +.006 & $-.016$ & +.004 \\
WOS & +.032 & +.022 & +.008 & +.052 & +.074 & +.000 & $-.004$ & +.004 & $-.004$ & +.010 \\
\midrule
HWV-L2 & +.110 & +.080 & +.098 & +.092 & +.080 & +.010 & +.026 & +.028 & +.024 & +.002 \\
HUPD-sub & +.056 & +.032 & +.016 & +.052 & +.068 & $-.014$ & $-.014$ & +.002 & $-.018$ & +.000 \\
HWV-L3 & +.068 & +.104 & +.060 & +.084 & +.066 & +.002 & +.016 & +.008 & $-.004$ & +.004 \\
\bottomrule
\end{tabular}
\end{table}

\section{Scaling Trend Robustness}
\label{app:scaling_robust}

\begin{table}[h]
\caption{Leave-one-out sensitivity for the $\hlo$ scaling trend (12 datasets). WOS (Cook's $D = 0.52$) is the highest-influence point; the product predictor resolves its deviation without dropping any dataset.}
\label{tab:loo}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dropped & $k$ & Spearman $\rho$ & $p$ & Pearson $r$ & $p$ \\
\midrule
None (full) & 12 & 0.61 & 0.035 & 0.47 & 0.123 \\
\midrule
Yahoo & 11 & 0.49 & 0.124 & 0.38 & 0.252 \\
GoEmotions & 11 & 0.50 & 0.117 & 0.42 & 0.202 \\
Newsgroups & 11 & 0.53 & 0.096 & 0.44 & 0.172 \\
TREC & 11 & 0.60 & 0.050 & 0.46 & 0.155 \\
arXiv & 11 & 0.57 & 0.067 & 0.46 & 0.158 \\
HUPD sec$\rightarrow$cls & 11 & 0.60 & 0.050 & 0.46 & 0.155 \\
DBPedia Classes & 11 & 0.64 & 0.035 & 0.52 & 0.100 \\
CLINC & 11 & 0.64 & 0.035 & 0.47 & 0.147 \\
HWV-L2 & 11 & 0.65 & 0.031 & 0.44 & 0.180 \\
HUPD sec$\rightarrow$sub & 11 & 0.64 & 0.035 & 0.54 & 0.089 \\
HWV-L3 & 11 & 0.64 & 0.035 & 0.46 & 0.158 \\
WOS & 11 & 0.77 & 0.006 & 0.63 & 0.038 \\
\bottomrule
\end{tabular}
\end{table}

Bootstrap analysis (10{,}000 resamples): 96.9\% of $\rho(\hlo)$ values positive.
Meta-analysis prediction interval $[-0.44, 4.17]$ reflects heterogeneity ($I^2 = 64\%$) explained by the interaction model.

\section{Broader Impact}
\label{app:impact}

Fractal embeddings add a semantic control knob to existing embedding models without modifying the backbone.
The primary application is more efficient retrieval: coarse-first filtering reduces compute by $4\times$ without sacrificing fine-grained accuracy when needed.
We do not foresee negative societal impacts beyond those inherent to embedding-based retrieval generally.
The method is domain-agnostic and introduces no biases beyond those present in the frozen backbone.

\end{document}
