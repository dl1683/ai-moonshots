\documentclass{article}

% Style
\usepackage[final]{neurips_2026}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{wrapfig}

\newcommand{\steer}{\mathcal{S}}
\newcommand{\hlz}{H(L_0)}
\newcommand{\hlo}{H(L_1|L_0)}

\title{Fractal Embeddings: Hierarchy-Aligned Prefix Supervision\\for Steerable Semantic Granularity}

\author{
  Devansh Lodha \\
  Independent Researcher \\
  \texttt{devansh@example.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Matryoshka Representation Learning (MRL) trains embeddings that support dimensional truncation, but all prefix lengths encode the same semantic content at varying fidelity---there is no mechanism to \emph{steer} between coarse and fine meaning.
We introduce \textbf{Fractal Embeddings}, which align prefix supervision with label hierarchy: short prefixes (64d) learn coarse labels while full embeddings (256d) learn fine labels, converting truncation into \emph{semantic zoom} at zero additional inference cost.
Four controlled ablations establish that \emph{alignment}---not hierarchy awareness---causally drives steerability: inverting alignment reverses its sign; removing it or applying uniform multi-task training collapses it to zero (all $p_{\text{adj}} < 10^{-3}$, $d \geq 6.1$ on CLINC; directionally consistent on TREC with $n = 3$).
Across eight datasets spanning conditional entropies from 1.23 to 5.05 bits, a random-effects meta-analysis yields pooled $d = 1.49$ ($p = 0.0003$), with all eight favouring fractal training (sign test $p = 0.004$).
On CLINC-150, steerability reaches $\steer = +0.150 \pm 0.028$ versus MRL's $+0.007 \pm 0.016$ ($p_\text{adj} = 0.004$, $d = 4.3$).
A synthetic hierarchy experiment identifies a Goldilocks optimum where steerability peaks at capacity--demand matching (quadratic $R^2 = 0.964$), and steerability magnitude across real datasets is predicted by the product of hierarchy depth and baseline learnability ($\rho = 0.90$, $p = 0.002$).
A backbone fine-tuning control confirms that alignment is both necessary and sufficient: a flat MRL baseline with $4.4\times$ more trainable parameters (backbone fine-tuning) produces zero steerability ($d = 8.1$ vs.\ head-only V5).
Replication across three encoder families confirms architecture invariance.
We ground these findings in the classical theory of successive refinement, providing formal conditions for when and why hierarchy-aligned supervision produces steerability.
\end{abstract}

% Hero figure
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig1_teaser.pdf}
    \caption{CLINC-150: V5 and MRL achieve comparable accuracy at full embedding length (256d), but V5's short prefixes (64d) specialize for coarse semantics while MRL's do not. This prefix specialization enables \emph{semantic steering} via dimensional truncation.}
    \label{fig:teaser}
\end{figure}

%=======================================================================
\section{Introduction}
\label{sec:intro}
%=======================================================================

When a 256-dimensional embedding is truncated to 64 dimensions, what changes?
Under standard Matryoshka training~\citep{kusupati2022matryoshka}, the answer is \emph{fidelity}: the shorter vector approximates the full one with reduced precision but encodes the same kind of semantic information.
We argue this is a missed opportunity.
Real-world semantics are inherently hierarchical---a query like ``What is the capital of France?'' belongs simultaneously to the coarse category \textsc{location} and the fine category \textsc{city}---and an embedding that supports truncation \emph{already has the interface} for multi-resolution access.
What it lacks is the training signal to make truncation semantically meaningful.

We introduce \textbf{Fractal Embeddings}, a simple modification to MRL training that aligns prefix supervision with semantic hierarchy.
Short prefixes are trained on coarse labels; full embeddings are trained on fine labels; intermediate prefixes receive blended supervision.
The result is an embedding where dimensional truncation corresponds to \emph{semantic zoom}: fewer dimensions yield coarser meaning, more dimensions recover finer distinctions (Figure~\ref{fig:teaser}).
Critically, this adds \textbf{zero inference-time cost}---the deployed model has identical architecture and parameter count to MRL.
The difference lies entirely in \emph{how} the projection head is trained.

We call the resulting property \emph{steerability}: the degree to which prefix truncation controls semantic granularity rather than merely degrading fidelity.
While prefix-level granularity has been explored for news clustering~\citep{hanley2025hierarchical}, no prior work provides causal evidence for \emph{why} it works, formal theory grounding it in successive refinement~\citep{equitz1991successive}, or scaling laws predicting \emph{when} it works best.
Our evaluation spans eight hierarchical text datasets, three encoder families, four controlled ablations, a synthetic causal experiment, and connections to 30 years of information theory.
The main findings are:

\begin{enumerate}
    \item \textbf{A method} for inducing steerable embeddings via hierarchy-aligned prefix supervision, requiring only head-only training on a frozen backbone (Section~\ref{sec:method}).
    \item \textbf{Causal identification}: four ablations on two datasets establish that steerability is driven by the alignment between prefix length and hierarchy level---not by architecture, hierarchy awareness, or other training choices. Inverting alignment reverses the sign; removing it or using a uniform multi-task control collapses it (Section~\ref{sec:causal}).
    \item \textbf{A scaling analysis} linking steerability magnitude to the interaction of hierarchy depth and model learnability across eight datasets ($\rho = 0.90$, $p = 0.002$), with a Goldilocks optimum identified via synthetic intervention ($R^2 = 0.964$; Section~\ref{sec:scaling}).
    \item \textbf{Broad generality}: cross-model replication on BGE-small, E5-small, and Qwen3-0.6B; downstream retrieval benchmarks showing $10\times$ larger dimensionality--accuracy ramps; and $3.7\times$ HNSW query speedups from prefix routing (Section~\ref{sec:generality}).
\end{enumerate}

%=======================================================================
\section{Problem Setup and Definitions}
\label{sec:setup}
%=======================================================================

\paragraph{Hierarchical classification.}
Each sample $x$ carries a coarse label $y^{(0)} \in \{1, \ldots, K_0\}$ and a fine label $y^{(1)} \in \{1, \ldots, K_1\}$, where every fine class maps to exactly one coarse class.
We characterize hierarchy depth by the conditional entropy $\hlo$---the additional information $L_1$ carries beyond $L_0$.
Extension to three levels is demonstrated in Section~\ref{sec:generality}.

\paragraph{Prefix-truncated embeddings.}
Given $\mathbf{e} \in \mathbb{R}^d$, the $j$-th prefix is $\mathbf{e}_{1:jd/J}$ for $j \in \{1, \ldots, J\}$.
We use $J = 4$ with $d = 256$, giving prefixes of 64, 128, 192, and 256 dimensions.
Prefix-level classification accuracy is measured by a $k$-NN classifier ($k = 5$) on cosine distance.

\paragraph{Steerability.}
We quantify the semantic specialization of prefix truncation via:
\begin{equation}
    \steer = \underbrace{(\text{L0@}j_1 - \text{L0@}j_4)}_{\text{coarse specialization}} + \underbrace{(\text{L1@}j_4 - \text{L1@}j_1)}_{\text{fine specialization}}
    \label{eq:steerability}
\end{equation}
where $\text{L}k\text{@}j$ denotes level-$k$ accuracy at prefix length $j$.
Positive $\steer$ means short prefixes favour coarse semantics while full embeddings favour fine.
A perfectly steerable embedding has high $\steer$; MRL, training all lengths on $L_1$, should yield $\steer \approx 0$.

\paragraph{Datasets.}
Table~\ref{tab:datasets} summarizes eight evaluation datasets spanning conditional entropies from 1.23 (Yahoo~\citep{zhang2015yahoo}) to 5.05 bits (WOS~\citep{kowsari2017hdltex}), including GoEmotions~\citep{demszky2020goemotions}, TREC~\citep{voorhees2000trec}, 20 Newsgroups, arXiv~\citep{clement2019arxiv}, DBPedia Classes, and CLINC-150~\citep{larson2019clinc}.

\begin{table}[t]
\caption{Dataset statistics and hierarchy profiles. $K_0$, $K_1$: coarse and fine class counts. Branch: $K_1/K_0$. $\hlz$, $\hlo$: coarse entropy and conditional entropy in bits.}
\label{tab:datasets}
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
Dataset & $K_0$ & $K_1$ & Branch & $\hlz$ & $\hlo$ & Train & Test \\
\midrule
Yahoo Answers & 4 & 10 & 2.5 & 1.91 & 1.23 & 10{,}000 & 2{,}000 \\
GoEmotions & 4 & 28 & 7.0 & 1.83 & 1.88 & 5{,}899 & 1{,}450 \\
20 Newsgroups & 6 & 20 & 3.3 & 2.43 & 1.88 & 10{,}000 & 2{,}000 \\
TREC & 6 & 50 & 8.3 & 2.38 & 2.21 & 5{,}452 & 500 \\
arXiv & 20 & 123 & 6.2 & 3.40 & 2.62 & 8{,}548 & 2{,}000 \\
DBPedia Classes & 9 & 70 & 7.8 & 2.09 & 3.17 & 10{,}000 & 2{,}000 \\
CLINC-150 & 10 & 150 & 15.0 & 3.32 & 3.90 & 10{,}000 & 2{,}000 \\
WOS & 10 & 336 & 33.6 & 2.90 & 5.05 & 8{,}688 & 2{,}000 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Statistical methodology.}
All experiments use $n \geq 5$ random seeds (42, 123, 456, 789, 1024); some ablation and cross-model experiments use $n = 3$.
Paired $t$-tests compare V5 and MRL steerability per dataset; the eight main comparisons (Table~\ref{tab:steerability}) are corrected via Holm--Bonferroni~\citep{holm1979simple} at $\alpha = 0.05$.
Causal ablation tests (Section~\ref{sec:causal}) are corrected within their own families.
Effect sizes are paired Cohen's $d$; Hedges' $g$ corrections ($\approx 0.80$ for $n = 5$, $0.57$ for $n = 3$) yield qualitatively identical conclusions.
Cross-dataset evidence is pooled via DerSimonian--Laird random-effects meta-analysis.

\paragraph{Theory preview.}
The classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive} shows that hierarchical sources admit optimal multi-resolution codes where each refinement layer adds residual information.
V5 training approximates this structure: the prefix encodes coarse semantics, and additional dimensions encode the refinement.
Section~\ref{sec:theory} formalises this connection and derives two testable predictions---sign reversal under inversion and a Goldilocks capacity--demand optimum---both confirmed by our experiments.

%=======================================================================
\section{Method: Progressive Prefix Supervision (V5)}
\label{sec:method}
%=======================================================================

Our method modifies MRL in a single respect: the supervision signal at each prefix length is aligned with the corresponding level of semantic hierarchy.

\paragraph{Architecture.}
A frozen pretrained backbone (BGE-small-en-v1.5~\citep{xiao2023bge}, 33M parameters, or Qwen3-Embedding-0.6B, 600M) produces hidden representations $\mathbf{h} \in \mathbb{R}^h$.
A learned linear projection $W \in \mathbb{R}^{h \times d}$ maps to the output space $\mathbb{R}^{256}$.
Two classification heads operate on the projected output: $\text{head}_\text{top}$ ($K_0$ coarse classes) and $\text{head}_\text{bot}$ ($K_1$ fine classes).

\paragraph{Progressive prefix supervision.}
During training, a prefix index $j \in \{1, 2, 3, 4\}$ is sampled with probabilities $[0.4, 0.3, 0.2, 0.1]$, favouring shorter prefixes.
The prefix loss depends on $j$:
\begin{equation}
    \mathcal{L}_\text{prefix}(j) = \begin{cases}
        \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:64}), y^{(0)}) & j = 1 \\
        \alpha_j \cdot \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:jd/4}), y^{(0)}) + (1-\alpha_j) \cdot \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) & j = 2,3 \\
        \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:256}), y^{(1)}) & j = 4
    \end{cases}
    \label{eq:v5loss}
\end{equation}
where $\alpha_j$ decreases with $j$ ($\alpha_2 = 0.7$, $\alpha_3 = 0.3$), creating a smooth coarse-to-fine gradient.
The total loss combines the full-embedding fine loss with the sampled prefix loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)}) + 0.6 \cdot \mathcal{L}_\text{prefix}(j)
\end{equation}

\paragraph{Block dropout.}
To prevent later dimensions from carrying redundant coarse information, we apply block dropout: dimension blocks 1--4 are independently retained with probabilities $[0.95, 0.9, 0.8, 0.7]$.
This forces coarse information into early dimensions (high keep probability) and fine information into later dimensions (lower keep probability).

\paragraph{MRL baseline.}
The matched baseline uses identical architecture, optimizer, and hyperparameters but trains all prefix lengths on $L_1$:
\begin{equation}
    \mathcal{L}_\text{MRL}(j) = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) \quad \forall j
\end{equation}
This isolates the effect of hierarchy alignment from every other training variable.

\paragraph{Training.}
Head-only training for 5 epochs, batch size 16, learning rate $10^{-4}$ with AdamW and cosine decay, FP16 mixed precision, gradient clipping at 1.0.
Best model selected by validation $\text{L0} + \text{L1}$.
No hyperparameters are tuned per dataset.
Algorithm~\ref{alg:v5} summarises the procedure.

\begin{algorithm}[t]
\caption{V5 Progressive Prefix Supervision}
\label{alg:v5}
\begin{algorithmic}[1]
\REQUIRE Backbone $f_\theta$ (frozen), projection head $W$, dataset $\mathcal{D}$ with $(x, y^{(0)}, y^{(1)})$
\REQUIRE Prefix probs $\mathbf{p} = [0.4, 0.3, 0.2, 0.1]$, block keep $\mathbf{k} = [0.95, 0.9, 0.8, 0.7]$
\FOR{each batch $\{(x_i, y_i^{(0)}, y_i^{(1)})\}$}
    \STATE $\mathbf{h} \leftarrow f_\theta(x_i)$ \hfill \COMMENT{frozen backbone}
    \STATE $\mathbf{e} \leftarrow W \cdot \mathbf{h}$ \hfill \COMMENT{learned projection to $\mathbb{R}^d$}
    \STATE Apply block dropout with keep probs $\mathbf{k}$
    \STATE Sample $j \sim \text{Categorical}(\mathbf{p})$
    \STATE $\mathcal{L}_\text{full} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \IF{$j = 1$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:d/4}), y^{(0)})$
    \ELSIF{$j = 4$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \ELSE
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \alpha_j \cdot \text{CE}(\text{head}_\text{top}, y^{(0)}) + (1-\alpha_j) \cdot \text{CE}(\text{head}_\text{bot}, y^{(1)})$
    \ENDIF
    \STATE $\mathcal{L} \leftarrow \mathcal{L}_\text{full} + 0.6 \cdot \mathcal{L}_\text{prefix}$
    \STATE Update $W$, $\text{head}_\text{top}$, $\text{head}_\text{bot}$ via $\nabla_W \mathcal{L}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%=======================================================================
\section{Main Results}
\label{sec:results}
%=======================================================================

\paragraph{Classification performance is preserved.}
At full embedding length ($j=4$, 256d), V5 and MRL achieve comparable $k$-NN accuracy across all eight datasets (Table~\ref{tab:accuracy}).
Both methods generally improve over the unfinetuned 384d baseline on lower-$K_1$ datasets; on higher-$K_1$ datasets (arXiv, CLINC, WOS), the 384d$\rightarrow$256d projection reduces $k$-NN accuracy, most dramatically on CLINC ($K_1 = 150$): V5 67.6\%, MRL 70.4\% vs.\ baseline 88.7\%, though both methods' classification heads achieve $>$95\% on the validation set.
This dimensionality bottleneck affects V5 and MRL equally, confirming that steerability does not come at the cost of additional accuracy loss.

\begin{table}[t]
\caption{$k$-NN classification accuracy at full embedding length ($j = 4$, 256d). V5 and MRL are comparable across datasets. Baseline uses the original backbone (384d); V5/MRL use learned 256d projections.}
\label{tab:accuracy}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{L0 Accuracy} & \multicolumn{3}{c}{L1 Accuracy} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Dataset & Baseline & V5 & MRL & Baseline & V5 & MRL \\
\midrule
Yahoo & 0.688 & 0.699 & 0.698 & 0.603 & 0.629 & 0.635 \\
GoEmotions & 0.502 & 0.600 & 0.578 & 0.343 & 0.429 & 0.411 \\
Newsgroups & 0.815 & 0.802 & 0.800 & 0.658 & 0.639 & 0.650 \\
TREC & 0.854 & 0.934 & 0.932 & 0.718 & 0.794 & 0.790 \\
arXiv & 0.721 & 0.703 & 0.692 & 0.465 & 0.401 & 0.381 \\
CLINC & 0.961 & 0.954 & 0.910 & 0.887 & 0.676 & 0.704 \\
DBPedia Classes & 0.912 & 0.945 & 0.935 & 0.780 & 0.789 & 0.802 \\
WOS & 0.619 & 0.601 & 0.599 & 0.170 & 0.111 & 0.115 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig3_forest_plot.pdf}
    \caption{Steerability across eight datasets. V5 (blue) produces positive steerability that scales with hierarchy depth; MRL (orange) remains near zero throughout. Error bars: 95\% CIs across 5 seeds.}
    \label{fig:forest}
\end{figure}

\paragraph{Steerability emerges from hierarchy alignment.}
Despite comparable full-resolution accuracy, V5 produces dramatically higher steerability than MRL on all eight datasets (Figure~\ref{fig:forest}, Table~\ref{tab:steerability}).
The effect scales with hierarchy complexity: from $\steer = +0.015$ on Yahoo ($\hlo = 1.23$) to $+0.150$ on CLINC ($\hlo = 3.90$), with WOS ($\hlo = 5.05$) showing a moderate effect ($+0.038$) due to a floor effect analysed in Section~\ref{sec:scaling}.
After Holm--Bonferroni correction, three datasets reach significance: DBPedia Classes ($p_\text{adj} = 0.002$, $d = 5.5$), CLINC ($p_\text{adj} = 0.004$, $d = 4.3$), and TREC ($p_\text{adj} = 0.038$, $d = 2.4$).
ArXiv is borderline ($p_\text{adj} = 0.078$, $d = 1.8$).

The shallow-hierarchy datasets (Yahoo, GoEmotions, Newsgroups; $\hlo \leq 1.88$) show consistent but small effects that do not survive per-dataset correction---a consequence the scaling analysis (Section~\ref{sec:scaling}) predicts: when hierarchy depth is low, there is little refinement information for V5 to separate, so the signal is inherently small.
MRL steerability is near zero throughout ($|\steer_\text{MRL}| < 0.02$ on all datasets).

\paragraph{Pooled evidence.}
A sign test confirms universal directionality: V5 $>$ MRL on 8/8 datasets ($p = 0.004$, binomial).
A DerSimonian--Laird meta-analysis yields pooled $d = 1.49$ (95\% CI: $[0.69, 2.30]$, $z = 3.63$, $p = 0.0003$).
The 95\% prediction interval for a new dataset is $[-0.70, 3.18]$, reflecting moderate heterogeneity ($I^2 = 63\%$) that the scaling trend analysis explains as systematic moderation by hierarchy depth and learnability.

\begin{table}[t]
\caption{Steerability across eight datasets (Eq.~\ref{eq:steerability}). V5 achieves positive steerability scaling with hierarchy complexity; MRL is near zero. Mean $\pm$ SD over 5 seeds.}
\label{tab:steerability}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & V5 $\steer$ & MRL $\steer$ & Gap & Seeds \\
\midrule
Yahoo & 1.23 & $+0.015 \pm 0.019$ & $+0.005 \pm 0.011$ & $+0.010$ & 5 \\
GoEmotions & 1.88 & $+0.020 \pm 0.018$ & $+0.006 \pm 0.017$ & $+0.014$ & 5 \\
Newsgroups & 1.88 & $+0.035 \pm 0.029$ & $+0.000 \pm 0.016$ & $+0.035$ & 5 \\
TREC & 2.21 & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ & $+0.045$ & 5 \\
arXiv & 2.62 & $+0.027 \pm 0.015$ & $-0.001 \pm 0.013$ & $+0.028$ & 5 \\
DBPedia Classes & 3.17 & $+0.120 \pm 0.016$ & $+0.008 \pm 0.008$ & $+0.112$ & 5 \\
CLINC & 3.90 & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.143$ & 5 \\
WOS & 5.05 & $+0.038 \pm 0.026$ & $+0.001 \pm 0.005$ & $+0.036$ & 5 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Why It Works: Causal Evidence via Ablation}
\label{sec:causal}
%=======================================================================

The main results establish \emph{that} hierarchy-aligned supervision produces steerability.
We now ask \emph{why}: is the effect driven by the specific prefix-to-hierarchy mapping, or could it arise from other aspects of training?
Four controlled ablations on CLINC-150 ($\hlo = 3.90$, 5 seeds) and TREC-50 ($\hlo = 2.21$, 3 seeds) isolate the causal mechanism.

\paragraph{Ablation design.}
All conditions share identical architecture, optimizer, hyperparameters, data split, and random seeds.
Only the prefix-to-hierarchy mapping varies:
\begin{itemize}
    \item \textbf{Aligned (V5)}: $j=1 \rightarrow L_0$, $j=4 \rightarrow L_1$ (correct alignment).
    \item \textbf{Inverted}: $j=1 \rightarrow L_1$, $j=4 \rightarrow L_0$ (reversed alignment).
    \item \textbf{No-prefix}: All prefix lengths trained on $L_1$ with $L_0$ regularisation (alignment removed).
    \item \textbf{UHMT}: All prefix lengths trained on $0.5 \cdot \mathcal{L}_{L_0} + 0.5 \cdot \mathcal{L}_{L_1}$ (hierarchy-\emph{aware} but not hierarchy-\emph{aligned}).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig4_ablation.pdf}
    \caption{Ablation results on CLINC-150 (5 seeds) and TREC-50 (3 seeds). Aligned supervision produces positive steerability; inversion reverses the sign; removing alignment or using uniform multi-task training collapses it. Dots show individual seeds.}
    \label{fig:ablation}
\end{figure}

\paragraph{Three causal signatures.}
The results (Table~\ref{tab:ablation}, Figure~\ref{fig:ablation}) reveal three distinct causal signatures that jointly rule out non-alignment explanations:\footnote{Absolute steerability values are lower here than in Table~\ref{tab:steerability} because ablations use a fixed train/val split to eliminate data-split variance across conditions.}

\begin{table}[t]
\caption{Causal ablation (BGE-small). All conditions share fixed data splits. Causal perturbation tests corrected at $m = 4$; UHMT tests corrected at $m = 2$.}
\label{tab:ablation}
\centering
\small
\begin{tabular}{llcccc}
\toprule
Dataset & Condition & $\steer$ (mean $\pm$ SD) & vs.\ V5 $t$ & $p_\text{adj}$ & Cohen's $d$ \\
\midrule
\multirow{4}{*}{CLINC ($\hlo\!=\!3.90$, 5s)} & Aligned (V5) & $+0.053 \pm 0.004$ & --- & --- & --- \\
 & Inverted & $-0.018 \pm 0.005$ & 25.5 & $< 10^{-4}$ & 11.4 \\
 & No-prefix & $+0.009 \pm 0.005$ & 13.7 & $< 10^{-3}$ & 6.1 \\
 & UHMT & $+0.001 \pm 0.005$ & 14.6 & $< 10^{-3}$ & 6.5 \\
\midrule
\multirow{4}{*}{TREC ($\hlo\!=\!2.21$, 3s)} & Aligned (V5) & $+0.045 \pm 0.023$ & --- & --- & --- \\
 & Inverted & $-0.025 \pm 0.008$ & 4.5 & $0.092$ & 2.6 \\
 & No-prefix & $-0.003 \pm 0.008$ & 2.7 & $0.116$ & 1.5 \\
 & UHMT & $-0.009 \pm 0.017$ & 7.7 & $0.017$ & 4.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Sign reversal.} Inverted alignment produces \emph{negative} steerability (CLINC: $-0.018$; TREC: $-0.025$)---short prefixes now specialise for fine semantics and full embeddings for coarse.
    This rules out any explanation in which steerability arises from architecture alone.
    \item \textbf{Collapse without alignment.} Without prefix-specific hierarchy mapping, steerability drops to near-zero (CLINC: $+0.009$; TREC: $-0.003$), indistinguishable from MRL.
    \item \textbf{Awareness is insufficient.} UHMT trains every prefix on \emph{both} $L_0$ and $L_1$ equally, making it hierarchy-aware but not hierarchy-aligned. Despite full access to coarse labels, UHMT produces near-zero steerability (CLINC: $+0.001$; TREC: $-0.009$).
    This eliminates the hypothesis that steerability arises from including $L_0$ in the loss; what matters is \emph{which} prefix lengths receive \emph{which} labels.
\end{enumerate}

All three patterns are highly significant on CLINC ($n = 5$, all $d \geq 6.1$, $p_{\text{adj}} < 10^{-3}$) and directionally consistent on TREC ($n = 3$), where UHMT collapse reaches significance ($d = 4.4$, $p = 0.017$) but sign reversal and no-prefix conditions remain underpowered.

\subsection{Information Localisation}

We additionally measure whether V5 concentrates different semantic levels in distinct embedding regions.
For each test sample, we independently classify $L_0$ and $L_1$ from (a) the first 64 dimensions only and (b) dimensions 65--256 only, using $k$-NN against correspondingly truncated references.
On CLINC, V5 shows 1.8\% lower $L_1$ accuracy in the prefix (94.7\% vs.\ 96.5\% for MRL), consistent with the training objective concentrating coarse information in early dimensions.
The stronger evidence for semantic separation comes from steerability itself, which measures the \emph{operational} consequence of truncation rather than raw information content.

%=======================================================================
\section{When It Works: Steerability Scaling Analysis}
\label{sec:scaling}
%=======================================================================

Having established that alignment causes steerability, we investigate what determines its \emph{magnitude}: an observational analysis across real datasets and a causal intervention via synthetic hierarchies.

\subsection{Observational: Scaling with Hierarchy Complexity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig5_scaling_law.pdf}
    \caption{\textbf{Left:} Steerability vs.\ $\hlo$ across eight datasets ($\rho = 0.74$, $p = 0.035$). WOS deviates due to a floor effect. \textbf{Right:} Product predictor $\hlo \times A_{L_1}^\text{base}$ accounts for both complexity and learnability ($\rho = 0.90$, $p = 0.002$).}
    \label{fig:scaling}
\end{figure}

Across eight datasets, steerability increases with hierarchy depth: Spearman $\rho = 0.74$ ($p = 0.035$) against $\hlo$ alone (Figure~\ref{fig:scaling}, left).
However, WOS ($\hlo = 5.05$) falls below the trend.

\paragraph{The WOS floor effect.}
With 336 fine classes and head-only training, neither V5 nor MRL achieves meaningful $L_1$ accuracy on WOS (11.1\% and 11.5\%; chance is 0.3\%).
When $L_1$ accuracy is near floor, the fine component of $\steer$ cannot differentiate across prefix lengths, capping steerability regardless of hierarchy depth.

\paragraph{The product predictor.}
This motivates a moderated predictor: effective steerability requires both hierarchy complexity \emph{and} model capacity to exploit it.
We define $\hlo \times A_{L_1}^\text{base}$, where $A_{L_1}^\text{base}$ is the \emph{unfinetuned} baseline $L_1$ accuracy---a measure of how much fine-grained information the pretrained backbone captures before any training:
\begin{itemize}
    \item $\hlo$ alone: $\rho = 0.74$ ($p = 0.035$)
    \item $A_{L_1}^\text{base}$ alone: $\rho = 0.69$ ($p = 0.058$)
    \item $\hlo \times A_{L_1}^\text{base}$: $\rho = 0.90$ ($p = 0.002$); Pearson $r = 0.97$ ($p < 0.001$)
\end{itemize}
The product predictor correctly places WOS among lower-steerability datasets: it has the highest $\hlo$ but the lowest $A_{L_1}^\text{base}$ (17.0\%, vs.\ 88.7\% for CLINC).
A leave-one-out analysis confirms robustness: all LOO $\rho \geq 0.61$, and bootstrap 95\% CI for $\rho(\hlo)$ is $[0.05, 1.0]$ with 98.0\% positive (Appendix~\ref{app:scaling_robust}).

\subsection{Causal: Synthetic Hierarchy Experiment}

Natural datasets confound $\hlz$ (prefix task demand) with $\hlo$ (refinement complexity), since more coarse classes typically yield more fine subclasses.
To disentangle these, we construct synthetic hierarchies on CLINC-150 text with fixed total entropy $H(L_1) = \log_2 150$ but varying coarse partitions $K_0 \in \{2, 3, 5, 10, 15, 25, 50, 75\}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig6_synthetic.pdf}
    \caption{Synthetic hierarchy experiment: steerability peaks when coarse entropy $\hlz$ matches prefix capacity ($K_0 \approx 12$--$16$), revealing a Goldilocks effect. Quadratic $R^2 = 0.964$. MRL stays near zero throughout.}
    \label{fig:synthetic}
\end{figure}

\paragraph{The Goldilocks effect.}
Figure~\ref{fig:synthetic} and Table~\ref{tab:synthetic} reveal an inverted-U relationship:

\begin{table}[t]
\caption{Synthetic hierarchy experiment. Fixed total entropy ($\log_2 150 = 7.23$ bits), varying $K_0$. Steerability peaks at $K_0 \approx 15$---a capacity--demand matching optimum.}
\label{tab:synthetic}
\centering
\small
\begin{tabular}{rcccccc}
\toprule
$K_0$ & $\hlz$ & $\hlo$ & Branch & V5 $\steer$ & MRL $\steer$ & Gap \\
\midrule
2 & 1.00 & 6.23 & 75.0 & $+0.134$ & $-0.010$ & $+0.144$ \\
3 & 1.58 & 5.64 & 50.0 & $+0.150$ & $+0.008$ & $+0.142$ \\
5 & 2.32 & 4.90 & 30.0 & $+0.216$ & $+0.002$ & $+0.214$ \\
10 & 3.32 & 3.90 & 15.0 & $+0.270$ & $-0.012$ & $+0.282$ \\
15 & 3.91 & 3.32 & 10.0 & $+0.278$ & $-0.004$ & $+0.282$ \\
25 & 4.64 & 2.58 & 6.0 & $+0.266$ & $-0.018$ & $+0.284$ \\
50 & 5.64 & 1.58 & 3.0 & $+0.252$ & $+0.010$ & $+0.242$ \\
75 & 6.23 & 1.00 & 2.0 & $+0.232$ & $-0.016$ & $+0.248$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Rising phase} ($K_0 = 2 \rightarrow 15$): more coarse classes create a richer ``routing codebook'' for the 64d prefix, enabling finer coarse discrimination.
    \item \textbf{Falling phase} ($K_0 = 15 \rightarrow 75$): $\hlz$ exceeds the prefix's representational capacity. With 64 dimensions, the prefix cannot reliably distinguish 50+ coarse classes.
    \item \textbf{Optimum}: peak at $K_0 \approx 12$--$16$ ($H^*(L_0) \approx 3.6$--$4.0$ bits), matching the effective capacity of a 64-dimensional prefix. A quadratic fit captures the shape with $R^2 = 0.964$.
    \item \textbf{MRL control}: MRL steerability remains near zero ($|\steer| < 0.02$) across all conditions, confirming the requirement for hierarchy-aligned supervision.
\end{enumerate}

The synthetic experiment also resolves the observational confound: steerability is driven by $\hlz$ (prefix task demand), not $\hlo$. In natural datasets, the $\hlo$ correlation arises because the two entropies covary. Figure~\ref{fig:entropy_alloc} in the appendix visualises both relationships.

\paragraph{Real-data capacity sweep.}
To validate the Goldilocks prediction on natural hierarchies, we sweep the scale dimension (prefix capacity) across $d_s \in \{16, 32, 48, 64, 96, 128\}$ on four datasets spanning the $\hlo$ range: Yahoo ($\hlo = 1.23$), TREC ($2.21$), DBPedia Classes ($3.17$), and CLINC ($3.90$).
Three seeds per condition.
All datasets peak at $d_s = 16$ (the minimum tested), consistent with the synthetic finding that the default 64d prefix is already at or beyond the Goldilocks optimum for these hierarchies (Figure~\ref{fig:capacity_sweep}a).
The key finding is in the \emph{scaling law at fixed capacity}: steerability at $d_s = 16$ scales nearly perfectly with conditional entropy across all four datasets ($\rho = 1.000$, $r = 0.985$, $p = 0.015$; Figure~\ref{fig:capacity_sweep}b), confirming that hierarchy complexity drives the steerability signal at matched capacity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_capacity_sweep.pdf}
    \caption{\textbf{Real-data capacity sweep.} (a)~Steerability vs.\ prefix capacity ($d_s$) for four datasets. All peak at $d_s = 16$, confirming the default 64d prefix exceeds the Goldilocks optimum. (b)~At fixed capacity ($d_s = 16$), steerability scales perfectly with $\hlo$ ($\rho = 1.000$, $r = 0.985$).}
    \label{fig:capacity_sweep}
\end{figure}

\paragraph{Alignment vs.\ capacity control.}
A natural objection is that steerability might emerge from \emph{any} sufficient-capacity model, without hierarchy-aligned supervision.
We test this with a $2 \times 2$ factorial design crossing alignment (V5/MRL) with training regime (head-only/backbone fine-tuning) on CLINC and DBPedia Classes (5 seeds each):
(i) \textbf{V5-frozen} (aligned, head-only, 2.1M params);
(ii) \textbf{MRL-frozen} (flat, head-only, 2.1M params);
(iii) \textbf{MRL+backbone} (flat, head + last 4 backbone layers, 9.2M params);
(iv) \textbf{V5+backbone} (aligned, head + backbone, 9.2M params).
On CLINC: V5-frozen $\steer = +0.050 \pm 0.005$ vs.\ MRL+backbone $\steer = +0.003 \pm 0.003$ ($d = 8.1$, $p < 10^{-7}$; Figure~\ref{fig:backbone}).
The alignment factor has a massive effect; the capacity factor has none.
Specifically, $4.4\times$ more trainable parameters cannot recover steerability without hierarchy-aligned prefix supervision ($p = 0.40$ for MRL-frozen vs.\ MRL+backbone).
Adding backbone fine-tuning to V5 also yields no improvement ($p = 0.57$), confirming that alignment is both necessary and sufficient under these conditions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig_backbone_control.pdf}
    \caption{\textbf{Alignment vs.\ capacity control} ($2 \times 2$ factorial). V5 with head-only training (2.1M params) dominates MRL with full backbone fine-tuning (9.2M params) on both datasets. $4.4\times$ more parameters without alignment produce zero steerability. *** denotes $p < 0.001$.}
    \label{fig:backbone}
\end{figure}

%=======================================================================
\section{Generality, Downstream Utility, and Extensions}
\label{sec:generality}
%=======================================================================

\paragraph{Cross-model replication.}
We replicate the CLINC experiment on E5-small-v2 (Microsoft, contrastive pre-training, 384d) and Qwen3-Embedding-0.6B ($h = 1024$, $18\times$ more parameters).
Table~\ref{tab:crossmodel} confirms that the steerability gap is architecture-invariant: BGE-small $+0.143$, E5-small $+0.115$, Qwen3 $+0.145$ (all $p < 0.025$, Holm-corrected across 3 families).
MRL steerability remains $\leq 0.015$ on all models.
The larger Qwen3 backbone shows higher steerability, suggesting the effect scales with model capacity.

\begin{table}[t]
\caption{Cross-model replication (3 seeds each). Steerability is architecture-invariant across three encoder families ($p < 0.025$, Holm-corrected, $m = 3$).}
\label{tab:crossmodel}
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{CLINC ($\hlo = 3.90$)} & \multicolumn{2}{c}{TREC ($\hlo = 2.21$)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Model & V5 $\steer$ & MRL $\steer$ & V5 $\steer$ & MRL $\steer$ \\
\midrule
BGE-small (BAAI, 33M) & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ \\
E5-small (Microsoft, 33M) & $+0.130 \pm 0.031$ & $+0.015 \pm 0.008$ & --- & --- \\
Qwen3-0.6B (Alibaba, 600M) & $+0.153 \pm 0.013$ & $+0.008 \pm 0.006$ & $+0.081 \pm 0.012$ & $+0.011 \pm 0.002$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Downstream retrieval.}
We evaluate whether steerability transfers beyond classification to controllable retrieval.
Table~\ref{tab:retrieval} reports Recall@1 on CLINC at each prefix length (3 seeds; full ramp in Figure~\ref{fig:retrieval}, Appendix).
V5 $L_1$ Recall@1 climbs from 87.1\% (64d) to 93.4\% (256d)---a $+6.3$pp ramp, $10\times$ larger than MRL's $+0.6$pp.
This demonstrates that prefix-level semantic specialisation directly yields dimensionality-dependent retrieval resolution.

\begin{table}[t]
\caption{Retrieval Recall@1 on CLINC-150 (3 seeds). V5's $L_1$ ramp ($+6.3$pp) is $10\times$ MRL's ($+0.6$pp).}
\label{tab:retrieval}
\centering
\small
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{4}{c}{Recall@1 (\%)} \\
\cmidrule(lr){3-6}
Level & Method & 64d & 128d & 192d & 256d \\
\midrule
$L_0$ & V5  & 97.2 & 97.8 & 98.0 & 97.9 \\
      & MRL & 97.7 & 98.0 & 98.2 & 98.1 \\
\midrule
$L_1$ & V5  & 87.1 & 92.7 & 93.7 & 93.4 \\
      & MRL & 93.6 & 93.9 & 94.3 & 94.3 \\
\midrule
\multicolumn{2}{l}{$L_1$ ramp (256d$-$64d)} & \multicolumn{4}{c}{V5: $+6.3 \pm 1.1$pp \quad MRL: $+0.6 \pm 0.4$pp \quad Ratio: $10\times$} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Workload-adaptive Pareto advantage.}
V5's steerability enables query-adaptive routing: coarse queries use the 64d prefix, fine queries use the full 256d embedding.
On CLINC (5 seeds), this dominates MRL-256d whenever $\geq 35\%$ of queries are coarse.
At a 50/50 workload mix, V5 adaptive achieves $+1.3$pp higher accuracy at 38\% lower average dimensionality (160d vs.\ 256d).
The dimensionality savings translate to wall-clock speedups: on FAISS HNSW indexes with 100K vectors, 64d queries execute $3.7\times$ faster than 256d (39~$\mu$s vs.\ 145~$\mu$s; Table~\ref{tab:latency}, Appendix).

\paragraph{Single model replaces two.}
A natural alternative to V5 is training two dedicated encoders: $E_{L_0}$ for coarse and $E_{L_1}$ for fine, each at 256d.
On CLINC (3 seeds), the dual system achieves $L_0 = 95.8\%$ and $L_1 = 94.8\%$, requiring two models and two indexes.
V5 adaptive (64d for coarse, 256d for fine) achieves $L_0 = 97.5\%$ at one-quarter the dimensionality and $L_1 = 94.5\%$ at full resolution---\emph{higher} coarse accuracy from a single model.

\paragraph{Three-level hierarchy.}
To test generalisation beyond two levels, we construct a 3-level CLINC hierarchy: 5~super-domains $\rightarrow$ 10~domains $\rightarrow$ 150~intents.
V5 exhibits a clear ramp gradient: $L_2$ (intent) accuracy gains $+3.2$pp from 64d to 256d, $L_1$ (domain) gains $+1.0$pp, and $L_0$ (super-domain) gains $+0.5$pp.
MRL is flat at all levels ($\leq 0.4$pp; Table~\ref{tab:threelevel}).
Three-level steerability $\steer_{02} = +0.027 \pm 0.005$ (V5) vs.\ $+0.002 \pm 0.005$ (MRL; $t = 18.9$, $p = 0.003$, $d = 10.9$).

\begin{table}[t]
\caption{Three-level hierarchy (CLINC, 5$\rightarrow$10$\rightarrow$150, 3 seeds). V5 shows a ramp gradient: finer levels gain more from additional dimensions. MRL is flat.}
\label{tab:threelevel}
\centering
\small
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{4}{c}{$k$-NN Accuracy (\%)} \\
\cmidrule(lr){3-6}
Level & Method & 64d & 128d & 192d & 256d \\
\midrule
$L_0$ (5 super) & V5  & 98.6 & 98.9 & 99.0 & 99.1 \\
                & MRL & 98.4 & 98.6 & 98.6 & 98.7 \\
\midrule
$L_1$ (10 domain) & V5  & 97.7 & 98.3 & 98.5 & 98.7 \\
                   & MRL & 97.9 & 98.0 & 98.1 & 98.1 \\
\midrule
$L_2$ (150 intent) & V5  & 92.7 & 94.7 & 95.4 & 95.9 \\
                    & MRL & 94.9 & 95.2 & 95.3 & 95.3 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Limitations}
\label{sec:limitations}
%=======================================================================

\begin{itemize}
    \item \textbf{Shallow hierarchies.} On datasets with low $\hlo$ (e.g., Yahoo, $\hlo = 1.23$), steerability is small and noisy ($\steer = 0.015 \pm 0.019$). The method is most valuable for moderately deep hierarchies where the fine task is learnable.
    \item \textbf{Floor effects.} On WOS ($K_1 = 336$), head-only training achieves only ${\sim}15\%$ $L_1$ accuracy, capping steerability despite high $\hlo = 5.05$. Steerability requires both hierarchy complexity and model capacity.
    \item \textbf{Deeper hierarchies.} The 3-level extension confirms generalisation with a strong effect ($d = 10.9$), but only 5 super-domain classes leave $L_0$ near ceiling at all prefixes. Evaluation on naturally deep taxonomies (ICD-10, product hierarchies) remains future work.
    \item \textbf{Hierarchy noise robustness.} Corrupting $L_0$ labels at 10\%--50\% and retraining V5 on CLINC (3 seeds), steerability degrades gracefully: 85\% retained at 10\% noise ($\steer = +0.152$), 71\% at 30\% ($+0.128$), 34\% at 50\% ($+0.061$). V5 does not require a perfect hierarchy for meaningful steerability.
    \item \textbf{Baseline scope.} We compare against a matched MRL baseline controlling all variables except the prefix-to-label mapping. We do not benchmark HEAL~\citep{zhang2025heal}, CSR~\citep{chen2025csr}, or SMRL~\citep{li2025smrl} because these address orthogonal goals---none offers prefix-level steering, making fixed-dimensionality accuracy comparisons inapposite.
    \item \textbf{Ablation coverage.} Causal ablations are concentrated on CLINC and TREC (two datasets). Cross-dataset replication of all four ablation conditions remains future work.
\end{itemize}

%=======================================================================
\section{Theoretical Analysis: Successive Refinement}
\label{sec:theory}
%=======================================================================

We connect our empirical findings to the classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive}, providing formal conditions for when and why hierarchy-aligned supervision produces steerability.

\paragraph{Setup.}
Let $(X, Y_0, Y_1)$ be a hierarchical source with $Y_0 = g(Y_1)$.
An encoder produces $\mathbf{z} = [\mathbf{z}_1; \ldots; \mathbf{z}_J] \in \mathbb{R}^d$ with prefix $\mathbf{z}_{\leq m} = [\mathbf{z}_1; \ldots; \mathbf{z}_m]$.
Let $C(d')$ denote the effective capacity (in bits) of a $d'$-dimensional embedding.

\paragraph{Theorem 1 (Hierarchy-Successive-Refinement, informal).}
\emph{Assume $C(d/J) \geq H(L_0)$ and $C(d/J) < H(L_1)$.
Under V5 supervision ($\mathbf{z}_{\leq 1} \rightarrow L_0$, $\mathbf{z} \rightarrow L_1$):}
\begin{equation}
    I(\mathbf{z}_{\leq 1}; L_0) > I(\mathbf{z}_{\leq 1}; L_1 | L_0) \quad \text{(coarse-prioritised prefix)}
\end{equation}
\emph{Under MRL ($\mathbf{z}_{\leq 1} \rightarrow L_1$, $\mathbf{z} \rightarrow L_1$): no specialisation.}

The proof follows from the capacity bottleneck: V5's prefix loss targets only $L_0$, so the optimal prefix maximises $I(\mathbf{z}_{\leq 1}; L_0)$.
Since $C(d/J) < H(L_1)$ but $C(d/J) \geq H(L_0)$, the prefix allocates capacity preferentially to the coarse task.
MRL distributes capacity across both $L_0$ and $L_1|L_0$ without specialisation.

\paragraph{Connection to successive refinement.}
Hierarchical sources are naturally \emph{successively refinable}~\citep{rimoldi1994successive}: the optimal multi-resolution code first encodes $Y_0$ at rate $R_1 \geq H(Y_0)$, then encodes the residual $Y_1|Y_0$ at rate $R_2 \geq H(Y_1|Y_0)$.
V5 approximates this structure: block~1 encodes $Y_0$; blocks 2--$J$ encode the refinement.
MRL performs single-resolution coding at each rate, losing the nested structure.
This explains why V5 achieves accuracy parity at full resolution while gaining steerability at short prefixes.

The successive refinement framework yields three testable predictions, all confirmed:

\paragraph{Corollary 1 (Sign reversal).}
\emph{Under inverted supervision ($\mathbf{z}_{\leq 1} \rightarrow L_1$, $\mathbf{z} \rightarrow L_0$), the bottleneck forces the prefix to encode refinement information, producing $\steer < 0$.}
Confirmed: $\steer = -0.018$ (CLINC), $-0.025$ (TREC).

\paragraph{Corollary 2 (UHMT collapse).}
\emph{Under uniform multi-task supervision, no prefix is privileged for either task; the optimiser distributes information uniformly, yielding $\steer \approx 0$.}
Confirmed: $\steer = +0.001$ (CLINC), $-0.009$ (TREC).

\paragraph{Corollary 3 (Capacity irrelevance).}
\emph{Increasing model capacity (e.g., unfreezing backbone layers) under flat MRL supervision does not create a capacity bottleneck at any prefix length, and therefore cannot induce steerability.}
The proof is immediate: MRL's loss function treats all prefix lengths identically with respect to label information.
Without a prefix-specific bottleneck, additional capacity distributes evenly, maintaining $\steer \approx 0$ regardless of parameter count.
Confirmed: MRL with $4.4\times$ more parameters ($9.2$M vs.\ $2.1$M) yields $\steer = +0.003$ on CLINC ($d = 8.1$ vs.\ V5 head-only).

\paragraph{Theorem 2 (Goldilocks capacity--demand matching, informal).}
\emph{With fixed $H(Y_1)$ and varying $K_0$, steerability peaks at $H^*(L_0) \approx C(d/J)$:}
\begin{itemize}
    \item \emph{$H(L_0) < C(d/J)$}: spare capacity leaks $L_1|L_0$ information, reducing $\steer$.
    \item \emph{$H(L_0) > C(d/J)$}: by Fano's inequality, prefix errors degrade coarse classification, reducing $\steer$.
    \item \emph{Taylor expansion around $H^*$}: $\steer \approx \steer^* - \alpha(H(L_0) - H^*)^2$, matching the empirical quadratic fit ($R^2 = 0.964$).
\end{itemize}

\paragraph{Testable prediction.}
Doubling prefix capacity from 64 to 128 dimensions should shift the Goldilocks peak rightward (to higher $K_0$), verifiable via a capacity sweep.

%=======================================================================
\section{Related Work}
\label{sec:related}
%=======================================================================

\paragraph{Multi-resolution embeddings.}
MRL~\citep{kusupati2022matryoshka} trains embeddings supporting prefix truncation, but all lengths target the same task.
SMEC~\citep{li2025smrl} rethinks MRL training for retrieval compression; Matryoshka Multimodal Models~\citep{cai2024m3} apply the nesting principle to visual tokens.
Most closely related, \citet{hanley2025hierarchical} independently train multilingual Matryoshka embeddings where shorter prefixes capture coarse story themes and longer prefixes capture fine-grained event identity, using progressively stricter similarity thresholds per dimension level.
Our work complements theirs by contributing a formal successive-refinement framework explaining \emph{why} prefix--hierarchy alignment works, an explicit steerability metric with four causal ablations isolating alignment as the active ingredient, cross-domain evaluation across eight hierarchical classification datasets and three encoder families, and scaling laws linking steerability magnitude to hierarchy structure.

\paragraph{Dimensional redundancy.}
\citet{takeshita2025random} show that randomly removing 50\% of dimensions causes $<$10\% performance loss, revealing massive redundancy.
We exploit this differently: rather than discarding dimensions for compression, we \emph{structure} them to carry semantically distinct information at each prefix length.
\citet{weller2025limits} prove that single-vector embedding expressiveness for top-$k$ retrieval is bounded by dimensionality, motivating multi-resolution access patterns.

\paragraph{Hierarchical embeddings.}
Hyperbolic embeddings~\citep{nickel2017poincare} represent hierarchies through curved geometry.
HEAL~\citep{zhang2025heal} aligns LLM embeddings with domain hierarchies via contrastive losses and matrix factorisation, but does not offer steerability via dimensional truncation.
Concurrent work on coarse-to-fine retrieval~\citep{zhao2025funnelrag} and multi-granularity RAG interfaces~\citep{du2026arag} address the granularity problem through external pipeline mechanisms (multiple retrievers, separate tools) rather than encoding granularity into the vector itself.
Our work provides the first theoretical and causal analysis of \emph{why} prefix--hierarchy alignment produces steerability, grounded in the classical theory of successive refinement.

\paragraph{Sparse and compressed embeddings.}
CSR~\citep{chen2025csr} and CSRv2~\citep{chen2026csrv2} learn sparse codes as alternatives to MRL, achieving efficiency through selective activation.
These address adaptive dimensionality via sparsity; our approach uses hierarchical prefix structure.
The two are orthogonal and potentially complementary.

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

We have shown that aligning prefix supervision with semantic hierarchy converts dimensional truncation from a fidelity knob into a \emph{semantic zoom} control---at zero inference cost.
The evidence is threefold.
\emph{Empirically}, fractal training produces steerability on all eight datasets (pooled $d = 1.49$, $p = 0.0003$), with magnitude predicted by the product of hierarchy depth and baseline learnability ($\rho = 0.90$).
\emph{Causally}, four controlled ablations establish that the specific prefix-to-hierarchy alignment---not hierarchy awareness, not architecture, not optimiser---drives the effect: all conditions are highly significant on CLINC ($d \geq 6.1$), with directionally consistent replication on TREC.
A factorial backbone control confirms alignment is both necessary and sufficient: $4.4\times$ more trainable parameters without alignment produce zero steerability ($d = 8.1$).
\emph{Theoretically}, the successive refinement framework explains \emph{why} V5 works (capacity bottleneck forces coarse-first encoding) and \emph{when} it works best (Goldilocks capacity--demand matching, $R^2 = 0.964$).

The practical implications are concrete.
A single fractal embedding enables query-adaptive retrieval: routing coarse queries to the 64d prefix ($3.7\times$ faster on HNSW) and fine queries to 256d, yielding higher mixed accuracy than MRL at 38\% lower compute.
A single V5 model replaces a dual-encoder system: its 64d prefix achieves 97.5\% coarse accuracy, exceeding a dedicated 256d coarse encoder (95.8\%).
The synthetic hierarchy experiment provides design guidance: measure $\hlz$ and size prefix capacity to match.

The connection to successive refinement suggests this is not an empirical curiosity but a fundamental property of information allocation in hierarchical representations.
We release all code, data, and experimental artifacts to support replication and extension.

\bibliography{references}
\bibliographystyle{plainnat}

%=======================================================================
% APPENDIX
%=======================================================================
\appendix

\section{Entropy Allocation Analysis}
\label{app:entropy}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig7_entropy_allocation.pdf}
    \caption{Disentangling the scaling law. \textbf{Left:} Steerability vs.\ $\hlz$ (prefix task demand)---the true driver, confirmed by the synthetic experiment. \textbf{Right:} Steerability vs.\ $\hlo$---a confounded proxy in observational data. Synthetic data (green diamonds) breaks the confound: $\hlo$ anti-correlates with steerability when $\hlz$ is varied independently.}
    \label{fig:entropy_alloc}
\end{figure}

\section{Retrieval Benchmark Visualisation}
\label{app:retrieval}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig9_retrieval.pdf}
    \caption{Retrieval benchmark on CLINC-150 (3 seeds). V5 $L_1$ Recall@1 ramps steeply from 64d to 192d ($+6.3$pp) while MRL is flat ($+0.6$pp). Both achieve comparable $L_0$ Recall@1 ($>97\%$). The $10\times$ larger ramp demonstrates that prefix specialisation transfers from classification to retrieval.}
    \label{fig:retrieval}
\end{figure}

\section{Three-Level Hierarchy Visualisation}
\label{app:threelevel}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig10_three_level.pdf}
    \caption{Three-level hierarchy (CLINC, 5$\rightarrow$10$\rightarrow$150, 3 seeds). \textbf{Left}: V5 shows clear level separation---$L_2$ gains most from additional dimensions while $L_0$ is near ceiling. \textbf{Right}: MRL curves are bunched with minimal ramp at any level.}
    \label{fig:threelevel}
\end{figure}

\section{Workload-Adaptive Pareto Analysis}
\label{app:pareto}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig11_pareto.pdf}
    \caption{\textbf{Left:} Mixed accuracy vs.\ workload mix $\alpha$ (fraction coarse). V5 adaptive dominates MRL-256d for $\alpha \geq 0.35$. \textbf{Right:} Pareto frontier---V5 achieves higher accuracy at lower dimensionality. Bands: $\pm 1$ SD (5 seeds).}
    \label{fig:pareto}
\end{figure}

\section{FAISS Latency Benchmark}
\label{app:latency}

\begin{table}[h]
\caption{FAISS query latency at different dimensionalities (RTX 5090, float32). V5's 64d prefix queries are $3.7$--$5.1\times$ faster than 256d.}
\label{tab:latency}
\centering
\small
\begin{tabular}{rcccc}
\toprule
& \multicolumn{2}{c}{Flat (n=10K)} & \multicolumn{2}{c}{HNSW (n=100K)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Dim & Latency ($\mu$s) & Speedup & Latency ($\mu$s) & Speedup \\
\midrule
64d & 35 & 5.1$\times$ & 39 & 3.7$\times$ \\
128d & 110 & 1.6$\times$ & 87 & 1.7$\times$ \\
192d & 146 & 1.2$\times$ & 81 & 1.8$\times$ \\
256d & 179 & 1.0$\times$ & 145 & 1.0$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Full Synthetic Hierarchy Results}
\label{app:synthetic}

See Table~\ref{tab:synthetic} for complete results across all 8 coarse partition sizes.
The experiment holds total class count fixed at 150 while varying $K_0$ from 2 to 75 with randomly assigned hierarchies on CLINC-150 text.

\section{Training Convergence}
\label{app:convergence}

All models converge within 5 epochs of head-only training.
Stage 2 (backbone fine-tuning) was tested but provided no improvement, consistent with the frozen backbone providing sufficient representational capacity for hierarchy-aligned supervision.

\section{Reproducibility}
\label{app:reproducibility}

\paragraph{Code and data.}
All code, trained models, and result JSONs are available at \url{https://github.com/dl1683/ai-moonshots}.
Every experiment uses publicly available datasets loaded via the \texttt{datasets} library with deterministic seeded splits.

\paragraph{Hyperparameters.}
All experiments use: 5 epochs head-only training, batch size 16, lr $10^{-4}$, AdamW with cosine decay, FP16, gradient clipping at 1.0.
Prefix sampling $[0.4, 0.3, 0.2, 0.1]$ and block dropout $[0.95, 0.9, 0.8, 0.7]$ are fixed across all datasets and models.
No per-dataset tuning.

\paragraph{Compute.}
Single NVIDIA RTX 5090 Laptop GPU (24GB VRAM).
Each training run: $\sim$2 min (BGE-small, 33M) or $\sim$8 min (Qwen3-0.6B).
Full suite: $\sim$16 GPU-hours.

\section{Metric Robustness}
\label{app:robustness}

We verify conclusions hold under three alternative steerability formulations:
\begin{itemize}
    \item $\steer_\text{AUC}$: average steerability over all prefix pairs $k = 2, 3, 4$.
    \item $\steer_\text{mono}$: fraction of adjacent pairs with monotonic coarse-decrease / fine-increase.
    \item $\steer_\text{gap}$: specialisation gap $(L_0@j_1 - L_1@j_1) - (L_0@j_4 - L_1@j_4)$.
\end{itemize}

\begin{table}[h]
\caption{Metric robustness: V5$\,$--$\,$MRL gap under four steerability formulations. Three of four agree V5 $>$ MRL on all 8 datasets.}
\label{tab:robustness}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & $\Delta\steer_\text{orig}$ & $\Delta\steer_\text{AUC}$ & $\Delta\steer_\text{mono}$ & $\Delta\steer_\text{gap}$ \\
\midrule
Yahoo & 1.23 & $+0.010$ & $+0.013$ & $-0.10$ & $+0.010$ \\
GoEmotions & 1.88 & $+0.014$ & $+0.014$ & $-0.07$ & $+0.014$ \\
Newsgroups & 1.88 & $+0.035$ & $+0.037$ & $+0.03$ & $+0.035$ \\
TREC & 2.21 & $+0.045$ & $+0.039$ & $+0.27$ & $+0.045$ \\
arXiv & 2.62 & $+0.028$ & $+0.020$ & $+0.17$ & $+0.028$ \\
DBPedia Cl. & 3.17 & $+0.112$ & $+0.106$ & $+0.33$ & $+0.112$ \\
CLINC & 3.90 & $+0.143$ & $+0.124$ & $+0.27$ & $+0.143$ \\
WOS & 5.05 & $+0.036$ & $+0.027$ & $+0.30$ & $+0.036$ \\
\midrule
V5 $>$ MRL & & 8/8 & 8/8 & 6/8 & 8/8 \\
Sign test $p$ & & 0.004 & 0.004 & 0.14 & 0.004 \\
\bottomrule
\end{tabular}
\end{table}

All pairwise rank correlations exceed $\rho = 0.90$ ($p < 0.005$), confirming all four metrics recover the same dataset ordering.

\section{Per-Seed Steerability Values}
\label{app:perseed}

\begin{table}[h]
\caption{Per-seed steerability (V5 and MRL) across all eight datasets. Seeds: 42, 123, 456, 789, 1024.}
\label{tab:perseed}
\centering
\scriptsize
\begin{tabular}{lrrrrr|rrrrr}
\toprule
& \multicolumn{5}{c|}{V5 $\steer$ by seed} & \multicolumn{5}{c}{MRL $\steer$ by seed} \\
Dataset & 42 & 123 & 456 & 789 & 1024 & 42 & 123 & 456 & 789 & 1024 \\
\midrule
Yahoo & +.016 & +.020 & $-.004$ & $-.002$ & +.044 & $-.010$ & +.016 & +.006 & +.016 & $-.002$ \\
GoEmo & $-.002$ & +.024 & +.026 & +.006 & +.044 & +.022 & +.022 & +.006 & $-.012$ & $-.010$ \\
News & +.040 & $-.012$ & +.038 & +.044 & +.066 & $-.002$ & +.022 & +.008 & $-.018$ & $-.010$ \\
TREC & +.018 & +.062 & +.054 & +.042 & +.046 & +.000 & +.024 & $-.014$ & $-.002$ & $-.012$ \\
arXiv & +.038 & +.018 & +.012 & +.018 & +.048 & $-.014$ & $-.010$ & +.000 & +.000 & +.020 \\
DBP & +.118 & +.092 & +.130 & +.130 & +.130 & +.020 & +.008 & +.008 & +.000 & +.002 \\
CLINC & +.104 & +.178 & +.150 & +.168 & +.150 & +.012 & +.028 & +.006 & $-.016$ & +.004 \\
WOS & +.032 & +.022 & +.008 & +.052 & +.074 & +.000 & $-.004$ & +.004 & $-.004$ & +.010 \\
\bottomrule
\end{tabular}
\end{table}

\section{Scaling Trend Robustness}
\label{app:scaling_robust}

\begin{table}[h]
\caption{Leave-one-out sensitivity for the $\hlo$ scaling trend. WOS (Cook's $D = 3.68$) is the highest-influence point; the product predictor resolves its deviation without dropping any dataset.}
\label{tab:loo}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dropped & $k$ & Spearman $\rho$ & $p$ & Pearson $r$ & $p$ \\
\midrule
None (full) & 8 & 0.74 & 0.035 & 0.49 & 0.218 \\
\midrule
Yahoo & 7 & 0.61 & 0.144 & 0.40 & 0.376 \\
GoEmotions & 7 & 0.64 & 0.119 & 0.45 & 0.318 \\
Newsgroups & 7 & 0.71 & 0.071 & 0.47 & 0.289 \\
TREC & 7 & 0.83 & 0.021 & 0.48 & 0.272 \\
arXiv & 7 & 0.78 & 0.041 & 0.50 & 0.259 \\
DBPedia Classes & 7 & 0.72 & 0.068 & 0.49 & 0.261 \\
CLINC & 7 & 0.72 & 0.068 & 0.34 & 0.457 \\
WOS & 7 & 0.87 & 0.012 & 0.91 & 0.004 \\
\bottomrule
\end{tabular}
\end{table}

Bootstrap analysis (10{,}000 resamples): 98.0\% of $\rho(\hlo)$ values positive.
Meta-analysis prediction interval $[-0.70, 3.18]$ reflects heterogeneity ($I^2 = 63\%$) explained by the interaction model.

\section{Broader Impact}
\label{app:impact}

Fractal embeddings add a semantic control knob to existing embedding models without modifying the backbone.
The primary application is more efficient retrieval: coarse-first filtering reduces compute by $4\times$ without sacrificing fine-grained accuracy when needed.
We do not foresee negative societal impacts beyond those inherent to embedding-based retrieval generally.
The method is domain-agnostic and introduces no biases beyond those present in the frozen backbone.

\end{document}
