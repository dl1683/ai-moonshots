\documentclass{article}

% NeurIPS 2026 style
\usepackage[final]{neurips_2026}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{wrapfig}

\newcommand{\steer}{\mathcal{S}}
\newcommand{\hlz}{H(L_0)}
\newcommand{\hlo}{H(L_1|L_0)}

\title{Fractal Embeddings: Hierarchy-Aligned Prefix Supervision\\for Steerable Semantic Granularity}

\author{
  Devansh Lodha \\
  Independent Researcher \\
  \texttt{devansh@example.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Modern embedding models support dimensional truncation, but truncation typically changes fidelity rather than semantic level, leaving no mechanism to \emph{steer} between coarse and fine meaning at inference time.
We introduce \textbf{Fractal Embeddings}, a hierarchy-aligned prefix supervision scheme that trains short prefixes (64d) on coarse labels ($L_0$) and full embeddings (256d) on fine labels ($L_1$), using a frozen backbone with learned heads only.
Against a matched Matryoshka Representation Learning (MRL) baseline trained on $L_1$ at all prefix lengths, our method preserves full-resolution performance while inducing robust \emph{steerability}: truncated prefixes encode coarse semantics, whereas full vectors recover fine semantics.
On CLINC-150, V5 achieves $\steer = +0.150 \pm 0.028$ while MRL produces $\steer = +0.007 \pm 0.016$ ($p_\text{adj} = 0.004$, Cohen's $d = 4.3$, 5~seeds).
Causal ablations on two datasets identify alignment as the driver: inverting alignment reverses the steerability sign (CLINC: $-0.018$, TREC: $-0.025$), and removing prefix-specific supervision collapses it to near-zero; all comparisons significant ($p \leq 0.03$, Cohen's $d \geq 2.7$).
In a synthetic hierarchy experiment with fixed text and varied coarse partitions, steerability scales with coarse entropy $\hlz$ and exhibits a Goldilocks optimum at ${\sim}12$--$16$ coarse classes (quadratic $R^2 = 0.964$).
Across six real datasets spanning four domains, steerability is strongly correlated with conditional entropy $\hlo$ (Spearman $\rho = 0.83$, $p = 0.042$; Pearson $r = 0.90$, $R^2 = 0.82$).
MRL produces near-zero steerability in all conditions.
Cross-model replication on three model families (BGE-small, E5-small, Qwen3-0.6B) confirms architecture invariance.
These results support hierarchy alignment as a principled control knob for semantic granularity.
\end{abstract}

% Hero figure
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig1_teaser.pdf}
    \caption{CLINC-150: V5 and MRL achieve comparable accuracy at full embedding length (256d), but V5's short prefixes (64d) specialize for coarse semantics while MRL's do not. This prefix specialization enables \emph{semantic steering} via dimensional truncation.}
    \label{fig:teaser}
\end{figure}

%=======================================================================
\section{Introduction}
\label{sec:intro}
%=======================================================================

Dense text embeddings~\citep{reimers2019sentence} are the backbone of modern retrieval systems, mapping sentences to fixed-dimensional vectors where similarity in vector space approximates semantic similarity.
Recent work on \emph{Matryoshka Representation Learning} (MRL)~\citep{kusupati2022matryoshka} has shown that embeddings can be trained to support dimensional truncation: a 256-dimensional embedding can be truncated to 64 dimensions with graceful accuracy degradation, enabling efficient storage and retrieval.

However, dimensional truncation in MRL changes \emph{fidelity}---the amount of information preserved---not \emph{semantic granularity}.
A 64-dimensional MRL prefix encodes the same kind of information as the full vector, just with lower resolution.
This is a missed opportunity: real-world semantics are inherently \emph{hierarchical}.
A question like ``What is the capital of France?'' simultaneously belongs to the coarse category \texttt{LOCATION} and the fine-grained category \texttt{CITY}.
An ideal embedding should allow a user to \emph{steer} between these levels by choosing how many dimensions to use.

We introduce \textbf{Fractal Embeddings}, a simple modification to MRL training that aligns prefix supervision with semantic hierarchy.
Instead of training all prefix lengths on the finest-grained labels (as in MRL), we train short prefixes on coarse labels ($L_0$) and full embeddings on fine labels ($L_1$).
This creates embeddings where dimensional truncation corresponds to semantic zoom: fewer dimensions $\rightarrow$ coarser meaning, more dimensions $\rightarrow$ finer meaning (Figure~\ref{fig:teaser}).

Our key contributions are:
\begin{enumerate}
    \item \textbf{A method} for training steerable embeddings via hierarchy-aligned prefix supervision (Section~\ref{sec:method}).
    \item \textbf{Causal evidence} from controlled ablations on two datasets that steerability is caused by alignment, not architecture: inverting the alignment reverses the steerability sign on both CLINC and TREC (Section~\ref{sec:causal}).
    \item \textbf{A scaling trend} linking steerability magnitude to hierarchy structure across six real datasets, with a Goldilocks optimum identified via synthetic intervention (Section~\ref{sec:scaling}).
    \item \textbf{Cross-model replication} confirming the effect is architecture-invariant across three model families: BGE-small, E5-small-v2, and Qwen3-0.6B (Section~\ref{sec:generality}).
\end{enumerate}

%=======================================================================
\section{Problem Setup and Definitions}
\label{sec:setup}
%=======================================================================

\paragraph{Hierarchical classification.}
We consider datasets with two-level label hierarchies: each sample $x$ has a coarse label $y^{(0)} \in \{1, \ldots, K_0\}$ and a fine label $y^{(1)} \in \{1, \ldots, K_1\}$, where each fine class maps to exactly one coarse class.
The hierarchy is characterized by the branching factor $K_1/K_0$ and the conditional entropy $\hlo$, which measures how much additional information $L_1$ carries beyond $L_0$.

\paragraph{Prefix-truncated embeddings.}
Given an embedding $\mathbf{e} \in \mathbb{R}^d$, the $j$-th prefix is $\mathbf{e}_{1:jd/J}$ for $j \in \{1, \ldots, J\}$.
We use $J = 4$ with $d = 256$, giving prefixes of 64, 128, 192, and 256 dimensions.
Classification accuracy is evaluated using a $k$-NN classifier ($k = 5$) on cosine distance.

\paragraph{Steerability.}
We define the steerability metric:
\begin{equation}
    \steer = \underbrace{(\text{L0@}j_1 - \text{L0@}j_4)}_{\text{coarse specialization}} + \underbrace{(\text{L1@}j_4 - \text{L1@}j_1)}_{\text{fine specialization}}
    \label{eq:steerability}
\end{equation}
where $\text{L}k\text{@}j$ denotes level-$k$ classification accuracy at prefix length $j$.
Positive $\steer$ means short prefixes specialize for coarse semantics while full embeddings specialize for fine semantics.
A perfectly steerable embedding has high $\steer$; MRL, which trains all lengths on $L_1$, should have $\steer \approx 0$.

\paragraph{Datasets.}
Table~\ref{tab:datasets} summarizes our six evaluation datasets, spanning conditional entropies from 1.23 (Yahoo~\citep{zhang2015yahoo}) to 3.90 bits (CLINC-150~\citep{larson2019clinc}), also including GoEmotions~\citep{demszky2020goemotions}, TREC~\citep{voorhees2000trec}, 20 Newsgroups, and arXiv~\citep{clement2019arxiv}.

\begin{table}[t]
\caption{Dataset statistics and hierarchy profiles. $K_0$, $K_1$: number of coarse and fine classes. Branch: $K_1/K_0$. $\hlz$, $\hlo$: coarse entropy and conditional entropy in bits.}
\label{tab:datasets}
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
Dataset & $K_0$ & $K_1$ & Branch & $\hlz$ & $\hlo$ & Train & Test \\
\midrule
Yahoo Answers & 4 & 10 & 2.5 & 1.91 & 1.23 & 10{,}000 & 2{,}000 \\
GoEmotions & 4 & 28 & 7.0 & 1.64 & 1.88 & 7{,}092 & 1{,}700 \\
20 Newsgroups & 6 & 20 & 3.3 & 2.43 & 1.88 & 10{,}000 & 2{,}000 \\
TREC & 6 & 50 & 8.3 & 2.38 & 2.21 & 5{,}452 & 500 \\
arXiv & 20 & 123 & 6.2 & 3.40 & 2.62 & 8{,}548 & 2{,}000 \\
CLINC-150 & 10 & 150 & 15.0 & 3.32 & 3.90 & 10{,}000 & 2{,}000 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Statistical methodology.}
All experiments use $n \geq 5$ random seeds per condition (seeds 42, 123, 456, 789, 1024).
Steerability differences (V5 minus MRL, paired by seed) are tested with two-sided paired $t$-tests.
For the six per-dataset comparisons in Table~\ref{tab:steerability}, we apply Holm--Bonferroni correction~\citep{holm1979simple} to control the family-wise error rate at $\alpha = 0.05$.
Effect sizes are reported as Cohen's $d$ (paired).
Standard deviations are sample estimates (ddof $= 1$).
Causal ablation tests (Section~\ref{sec:causal}) are corrected within their own family of 4 comparisons (2 datasets $\times$ 2 conditions).
To pool evidence across datasets, we also report a DerSimonian--Laird random-effects meta-analysis on paired Cohen's $d$ values.

%=======================================================================
\section{Method: Progressive Prefix Supervision (V5)}
\label{sec:method}
%=======================================================================

Our method modifies MRL training in one key way: we align the \emph{supervision signal} with the \emph{prefix length}, so that shorter prefixes are trained to capture coarse structure while longer prefixes capture fine structure.

\paragraph{Architecture.}
We use a frozen pretrained embedding backbone (BGE-small-en-v1.5~\citep{xiao2023bge} or Qwen3-Embedding-0.6B) with a learned linear projection head $W \in \mathbb{R}^{h \times d}$ mapping from hidden dimension $h$ to output dimension $d = 256$.
Two classification heads operate on the output: $\text{head}_\text{top}$ (coarse, $K_0$ classes) and $\text{head}_\text{bot}$ (fine, $K_1$ classes).

\paragraph{Progressive prefix supervision.}
During training, we randomly sample a prefix length $j \in \{1, 2, 3, 4\}$ with probabilities $[0.4, 0.3, 0.2, 0.1]$ (favoring shorter prefixes).
The loss depends on $j$:
\begin{equation}
    \mathcal{L}_\text{prefix}(j) = \begin{cases}
        \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:64}), y^{(0)}) & j = 1 \\
        \alpha \cdot \mathcal{L}_\text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:jd/4}), y^{(0)}) + (1-\alpha) \cdot \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) & j = 2,3 \\
        \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:256}), y^{(1)}) & j = 4
    \end{cases}
    \label{eq:v5loss}
\end{equation}
where $\alpha$ decreases with $j$ (e.g., $\alpha = 0.7$ for $j=2$, $\alpha = 0.3$ for $j=3$).
The total loss combines the full-embedding loss with the sampled prefix loss:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)}) + w_\text{prefix} \cdot \mathcal{L}_\text{prefix}(j)
\end{equation}
with $w_\text{prefix} = 0.6$.

\paragraph{Block dropout.}
To prevent later dimensions from carrying redundant coarse information, we apply block dropout during training: dimension blocks are independently kept with probabilities $[0.95, 0.9, 0.8, 0.7]$ for blocks 1--4.
This forces the network to encode coarse information in early dimensions (high keep probability) and fine information in later dimensions (lower keep probability).

\paragraph{MRL baseline.}
Our matched baseline uses identical architecture and training procedure, but trains \emph{all} prefix lengths on $L_1$ (fine labels):
\begin{equation}
    \mathcal{L}_\text{MRL}(j) = \mathcal{L}_\text{CE}(\text{head}_\text{bot}(\mathbf{e}_{1:jd/4}), y^{(1)}) \quad \forall j
\end{equation}
This isolates the effect of hierarchy alignment from all other training choices (backbone, optimizer, epochs, batch size).

\paragraph{Training details.}
Head-only training for 5 epochs, batch size 16, learning rate $10^{-4}$ with AdamW and cosine decay.
Mixed-precision (FP16) training with gradient clipping at 1.0.
Best model selected by validation score $\text{L0} + \text{L1}$.
Crucially, V5 adds \textbf{zero inference-time overhead}: at deployment, the backbone and projection head are identical to MRL; steerability comes from how the head was trained, not from any architectural change.
Algorithm~\ref{alg:v5} summarizes the procedure.

\begin{algorithm}[t]
\caption{V5 Progressive Prefix Supervision}
\label{alg:v5}
\begin{algorithmic}[1]
\REQUIRE Backbone $f_\theta$ (frozen), head $W$, dataset $\mathcal{D}$ with $(x, y^{(0)}, y^{(1)})$
\REQUIRE Prefix probs $\mathbf{p} = [0.4, 0.3, 0.2, 0.1]$, block keep $\mathbf{k} = [0.95, 0.9, 0.8, 0.7]$
\FOR{each batch $\{(x_i, y_i^{(0)}, y_i^{(1)})\}$}
    \STATE $\mathbf{h} \leftarrow f_\theta(x_i)$ \hfill \COMMENT{frozen backbone}
    \STATE $\mathbf{e} \leftarrow W \cdot \mathbf{h}$ \hfill \COMMENT{learned projection to $\mathbb{R}^d$}
    \STATE Apply block dropout with keep probs $\mathbf{k}$
    \STATE Sample $j \sim \text{Categorical}(\mathbf{p})$
    \STATE $\mathcal{L}_\text{full} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \IF{$j = 1$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{top}(\mathbf{e}_{1:d/4}), y^{(0)})$
    \ELSIF{$j = 4$}
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \text{CE}(\text{head}_\text{bot}(\mathbf{e}), y^{(1)})$
    \ELSE
        \STATE $\mathcal{L}_\text{prefix} \leftarrow \alpha_j \cdot \text{CE}(\text{head}_\text{top}, y^{(0)}) + (1-\alpha_j) \cdot \text{CE}(\text{head}_\text{bot}, y^{(1)})$
    \ENDIF
    \STATE $\mathcal{L} \leftarrow \mathcal{L}_\text{full} + 0.6 \cdot \mathcal{L}_\text{prefix}$
    \STATE Update $W$, $\text{head}_\text{top}$, $\text{head}_\text{bot}$ via $\nabla_W \mathcal{L}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%=======================================================================
\section{Main Results: Steerability Without Sacrificing Accuracy}
\label{sec:results}
%=======================================================================

\paragraph{Classification performance.}
At full embedding length ($j=4$, 256d), V5 and MRL achieve comparable classification accuracy on Yahoo, GoEmotions, Newsgroups, TREC, and arXiv (Table~\ref{tab:accuracy}).
On CLINC, V5 retains higher $L_0$ accuracy at $j=4$ while MRL retains higher $L_1$---a natural consequence of V5's coarse-to-fine information allocation, which persists even at full length.
Both methods substantially outperform the unfinetuned baseline across all datasets.

\begin{table}[t]
\caption{Classification accuracy at full embedding length ($j = 4$, 256d). V5 and MRL are comparable on Yahoo, Newsgroups, TREC, and arXiv. On CLINC, V5 retains higher $L_0$ while MRL retains higher $L_1$---reflecting V5's coarse-to-fine allocation. Both improve substantially over the unfinetuned baseline.}
\label{tab:accuracy}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{L0 Accuracy} & \multicolumn{3}{c}{L1 Accuracy} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Dataset & Baseline & V5 & MRL & Baseline & V5 & MRL \\
\midrule
Yahoo & 0.688 & 0.699 & 0.698 & 0.603 & 0.629 & 0.635 \\
GoEmotions & 0.502 & 0.600 & 0.578 & 0.343 & 0.429 & 0.411 \\
Newsgroups & 0.815 & 0.802 & 0.800 & 0.658 & 0.639 & 0.650 \\
TREC & 0.854 & 0.934 & 0.932 & 0.718 & 0.794 & 0.790 \\
arXiv & 0.721 & 0.729 & 0.721 & 0.465 & 0.448 & 0.446 \\
CLINC & 0.961 & 0.954 & 0.910 & 0.887 & 0.676 & 0.704 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig3_forest_plot.pdf}
    \caption{Cross-dataset steerability: V5 (blue) produces positive steerability that scales with hierarchy depth across six datasets, while MRL (orange) remains near zero. Error bars show 95\% CIs.}
    \label{fig:forest}
\end{figure}

\paragraph{Steerability.}
Despite classification parity, V5 produces dramatically higher steerability than MRL across all six datasets (Figure~\ref{fig:forest}, Table~\ref{tab:steerability}).
Steerability increases with hierarchy complexity ($\hlo$): from $\steer = +0.015$ on Yahoo ($\hlo = 1.23$) through GoEmotions ($+0.020$, $\hlo = 1.88$), TREC ($+0.044$, $\hlo = 2.21$), and arXiv ($+0.027$, $\hlo = 2.62$) to $\steer = +0.150$ on CLINC ($\hlo = 3.90$).
After Holm--Bonferroni correction across six tests, CLINC and TREC remain significant ($p_\text{adj} = 0.004$ and $0.031$; Cohen's $d = 4.3$ and $2.4$); arXiv is borderline ($p_\text{adj} = 0.062$, $d = 1.8$).
The three shallow-hierarchy datasets (Yahoo, GoEmotions, Newsgroups; $\hlo \leq 1.88$) show consistent but small effects that do not reach per-dataset significance---a predicted consequence of the capacity-demand matching theory (Section~\ref{sec:scaling}): when $\hlo$ is low, there is little refinement information for V5 to separate from coarse information, so the steerability signal is inherently small.
arXiv falls slightly below TREC despite higher $\hlo$, likely because cross-listed papers create fuzzy category boundaries that reduce effective hierarchy depth; we discuss this further in Section~\ref{sec:scaling}.
MRL steerability is consistently near zero ($\steer_\text{MRL} < 0.02$), confirming that steerability requires explicit hierarchy alignment.
A sign test confirms the consistency: V5 $>$ MRL steerability on all 6 datasets ($p = 2^{-6} = 0.016$, one-sided binomial).
A random-effects meta-analysis (DerSimonian--Laird) across all six datasets yields a pooled Cohen's $d = 1.23$ (95\% CI: $[0.42, 2.04]$, $z = 2.98$, $p = 0.003$), confirming a robust overall V5 advantage.
The substantial heterogeneity ($I^2 = 59\%$, $Q = 12.1$, $p_Q = 0.033$) is expected: the scaling trend analysis (Section~\ref{sec:scaling}) predicts that effect size increases with hierarchy depth.

\begin{table}[t]
\caption{Steerability across datasets ($\steer$ = coarse specialization + fine specialization, Eq.~\ref{eq:steerability}). V5 achieves positive steerability that scales with hierarchy complexity. MRL is consistently near zero. Reported as mean $\pm$ SD across seeds.}
\label{tab:steerability}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & V5 $\steer$ & MRL $\steer$ & Gap & Seeds \\
\midrule
Yahoo & 1.23 & $+0.015 \pm 0.019$ & $+0.005 \pm 0.011$ & $+0.010$ & 5 \\
GoEmotions & 1.88 & $+0.020 \pm 0.018$ & $+0.006 \pm 0.017$ & $+0.014$ & 5 \\
Newsgroups & 1.88 & $+0.035 \pm 0.029$ & $+0.000 \pm 0.016$ & $+0.035$ & 5 \\
TREC & 2.21 & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ & $+0.045$ & 5 \\
arXiv & 2.62 & $+0.027 \pm 0.015$ & $-0.001 \pm 0.013$ & $+0.028$ & 5 \\
CLINC & 3.90 & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.143$ & 5 \\
\bottomrule
\end{tabular}
\end{table}

%=======================================================================
\section{Causal Evidence via Ablation}
\label{sec:causal}
%=======================================================================

The correlation between hierarchy alignment and steerability in Section~\ref{sec:results} could arise from confounds (e.g., dataset properties, label structure).
We perform three controlled ablations on two datasets---CLINC-150 ($\hlo = 3.90$, 5~seeds) and TREC-50 ($\hlo = 2.21$, 3~seeds)---to provide causal evidence that alignment drives steerability and verify cross-dataset robustness.

\paragraph{Ablation conditions.}
All three conditions use identical architecture, optimizer, hyperparameters, and data split (held fixed across seeds).
Only the prefix-to-hierarchy mapping changes:
\begin{itemize}
    \item \textbf{Aligned (V5)}: $j=1 \rightarrow L_0$, $j=4 \rightarrow L_1$ (correct alignment).
    \item \textbf{Inverted}: $j=1 \rightarrow L_1$, $j=4 \rightarrow L_0$ (reversed alignment).
    \item \textbf{No-prefix}: All prefix lengths trained on $L_1$ with additional $L_0$ regularization (no alignment).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig4_ablation.pdf}
    \caption{Causal ablation on CLINC-150 (5 seeds) and TREC-50 (3~seeds). Aligned supervision (V5) produces positive steerability; inverting alignment reverses the sign; removing prefix-specific alignment collapses it. Individual seed values shown as dots.}
    \label{fig:ablation}
\end{figure}

\paragraph{Results.}
The ablation results (Table~\ref{tab:ablation}, Figure~\ref{fig:ablation}) provide strong evidence that alignment drives steerability.\footnote{The absolute steerability in Table~\ref{tab:ablation} is lower than in Table~\ref{tab:steerability} because the ablation uses a fixed train/val split to eliminate data-split variance across conditions; the within-condition comparisons (V5 vs.\ inverted, V5 vs.\ no-prefix) are the relevant tests.}

\begin{table}[t]
\caption{Causal ablation on two datasets (BGE-small). All conditions share a fixed train/val split to eliminate data-split variance. Inverting alignment reverses the steerability sign on both datasets; removing prefix-specific alignment collapses it. All four $p$-values survive Holm--Bonferroni correction within this family ($m = 4$; largest adjusted $p = 0.030$).}
\label{tab:ablation}
\centering
\small
\begin{tabular}{llcccc}
\toprule
Dataset & Condition & $\steer$ (mean $\pm$ SD) & vs.\ V5 $t$ & $p_\text{adj}$ & Cohen's $d$ \\
\midrule
\multirow{3}{*}{CLINC ($\hlo\!=\!3.90$, 5s)} & Aligned (V5) & $+0.053 \pm 0.004$ & --- & --- & --- \\
 & Inverted & $-0.018 \pm 0.005$ & 26.1 & $< 10^{-5}$ & 16.5 \\
 & No-prefix & $+0.009 \pm 0.005$ & 15.8 & $< 10^{-5}$ & 10.0 \\
\midrule
\multirow{3}{*}{TREC ($\hlo\!=\!2.21$, 3s)} & Aligned (V5) & $+0.045 \pm 0.023$ & --- & --- & --- \\
 & Inverted & $-0.025 \pm 0.008$ & 4.9 & $0.016$ & 4.0 \\
 & No-prefix & $-0.003 \pm 0.008$ & 3.3 & $0.030$ & 2.7 \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Sign reversal}: Inverted alignment produces \emph{negative} steerability on both datasets (CLINC: $\steer = -0.018$; TREC: $\steer = -0.025$), meaning short prefixes now specialize for fine semantics and full embeddings for coarse---the opposite of V5.
    This rules out any explanation where steerability arises from architecture alone.
    \item \textbf{Signal collapse}: Without prefix-specific alignment, steerability drops to near-zero (CLINC: $+0.009$; TREC: $-0.003$), indistinguishable from MRL.
    \item \textbf{Cross-dataset robustness}: The sign reversal and signal collapse replicate across hierarchy depths ($\hlo = 2.21$ and $3.90$), confirming the effect is not dataset-specific.
    \item \textbf{Effect sizes}: Cohen's $d \geq 2.7$ across all comparisons on both datasets (up to $d = 16.5$ on CLINC), indicating the effect is both statistically and practically significant.
\end{enumerate}

These results support the \textbf{Fractal Embedding Principle}: steerability is driven by the alignment between prefix supervision and hierarchy structure, not by architecture or other training choices. The sign reversal and signal collapse replicate across datasets with different hierarchy depths, class counts, and domains.

\subsection{Information Localization}

The ablations above modify \emph{training}; we additionally measure whether V5 concentrates different semantic levels in different embedding blocks.
For each test sample, we independently classify $L_0$ and $L_1$ from (a)~the prefix only (first 64d) and (b)~the suffix only (dims~65--256), using $k$-NN against references at the same granularity.

On CLINC-150, V5 shows 1.8\% less $L_1$ information in the prefix (94.7\% vs.\ 96.5\% for MRL prefix-only $L_1$ accuracy), consistent with the training objective concentrating coarse information in early dimensions.
The effect is modest because 64 dimensions provide sufficient capacity for both $L_0$ (10~classes) and $L_1$ (150~classes) on this dataset.
The stronger evidence for semantic separation comes from the steerability metric itself (\S\ref{sec:results}), which measures the \emph{behavioral} consequence of prefix truncation rather than the raw information content.

%=======================================================================
\section{Steerability Scaling Trend}
\label{sec:scaling}
%=======================================================================

Having established that alignment causes steerability, we investigate what determines its \emph{magnitude}.
We present two complementary analyses: an observational study across real datasets and a causal intervention using synthetic hierarchies.

\subsection{Observational: Steerability Scales with Hierarchy Complexity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{../results/figures/paper/fig5_scaling_law.pdf}
    \caption{Steerability scales with hierarchy refinement entropy $\hlo$ across six real datasets (Spearman $\rho = 0.83$, $p = 0.042$; Pearson $r = 0.90$). arXiv falls slightly below the linear trend, likely due to cross-listed papers creating fuzzy category boundaries.}
    \label{fig:scaling}
\end{figure}

Across six real datasets, steerability is strongly correlated with conditional entropy $\hlo$ (Spearman $\rho = 0.83$, $p = 0.042$; Pearson $r = 0.90$, $R^2 = 0.82$; Figure~\ref{fig:scaling}).
A linear fit yields $\steer \approx 0.050 \cdot \hlo - 0.045$ ($R^2 = 0.82$, $p = 0.014$).
One mild outlier is arXiv ($\hlo = 2.62$, $\steer = +0.027$), which falls below both TREC ($\hlo = 2.21$, $\steer = +0.044$) and Newsgroups ($\hlo = 1.88$, $\steer = +0.035$) despite higher conditional entropy.
We attribute this to arXiv's pervasive cross-listing: many papers belong to multiple categories (e.g., cs.LG and stat.ML), creating fuzzy boundaries that reduce the \emph{effective} hierarchy depth below the nominal entropy.
Excluding arXiv, the remaining five datasets follow a monotonic trend ($\rho = 1.0$).
A leave-one-out sensitivity analysis (Appendix~\ref{app:scaling_robust}) confirms that the positive trend is robust to removing any single dataset, though CLINC provides high leverage as the deepest hierarchy: removing it reduces Spearman $\rho$ to 0.62 while Pearson $r$ drops to 0.53.

In natural datasets, $\hlz$ and $\hlo$ are positively correlated (more coarse classes $\Rightarrow$ more fine classes per coarse class).
This confound prevents us from determining whether the mechanism operates through $\hlz$ (prefix task demand) or $\hlo$ (refinement complexity).

\subsection{Causal: Synthetic Hierarchy Experiment}

To break this confound, we construct synthetic hierarchies with \emph{fixed text and fixed total entropy} $H(L_1) = \log_2 150$ but varying coarse partitions $K_0 \in \{2, 3, 5, 10, 15, 25, 50, 75\}$.
As $K_0$ increases, $\hlz = \log_2 K_0$ increases while $\hlo = \log_2(150/K_0)$ decreases.
We train V5 and MRL on each synthetic hierarchy using CLINC-150 text and evaluate steerability.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../results/figures/paper/fig6_synthetic.pdf}
    \caption{Synthetic hierarchy experiment: steerability shows a ``Goldilocks'' effect, peaking when coarse task entropy $\hlz$ matches prefix capacity ($K_0 \approx 12$--16). MRL remains near zero throughout. Quadratic fit $R^2 = 0.964$.}
    \label{fig:synthetic}
\end{figure}

\paragraph{Results.}
Figure~\ref{fig:synthetic} reveals the mechanism:

\begin{table}[t]
\caption{Synthetic hierarchy experiment. Fixed total entropy ($\log_2 150 = 7.23$ bits), varied coarse partition $K_0$. Steerability rises with $\hlz$, peaks at $K_0 \approx 15$, then declines---a \emph{Goldilocks} effect. MRL is near zero throughout.}
\label{tab:synthetic}
\centering
\small
\begin{tabular}{rcccccc}
\toprule
$K_0$ & $\hlz$ & $\hlo$ & Branch & V5 $\steer$ & MRL $\steer$ & Gap \\
\midrule
2 & 1.00 & 6.23 & 75.0 & $+0.134$ & $-0.010$ & $+0.144$ \\
3 & 1.58 & 5.64 & 50.0 & $+0.150$ & $+0.008$ & $+0.142$ \\
5 & 2.32 & 4.90 & 30.0 & $+0.216$ & $+0.002$ & $+0.214$ \\
10 & 3.32 & 3.90 & 15.0 & $+0.270$ & $-0.012$ & $+0.282$ \\
15 & 3.91 & 3.32 & 10.0 & $+0.278$ & $-0.004$ & $+0.282$ \\
25 & 4.64 & 2.58 & 6.0 & $+0.266$ & $-0.018$ & $+0.284$ \\
50 & 5.64 & 1.58 & 3.0 & $+0.252$ & $+0.010$ & $+0.242$ \\
75 & 6.23 & 1.00 & 2.0 & $+0.232$ & $-0.016$ & $+0.248$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
    \item \textbf{Rising phase} ($K_0 = 2 \rightarrow 15$): Steerability increases with $\hlz$.
    More coarse classes create a richer ``routing codebook'' for the prefix, allowing finer-grained coarse discrimination.
    \item \textbf{Falling phase} ($K_0 = 15 \rightarrow 75$): Steerability declines as $\hlz$ exceeds the prefix's representational capacity.
    With only 64 dimensions, the prefix cannot reliably distinguish 50+ coarse classes.
    \item \textbf{Goldilocks optimum}: Peak at $K_0 \approx 12$--$16$ ($H^*(L_0) \approx 3.6$--$4.0$ bits), matching the effective capacity of a 64-dimensional prefix space.
    A quadratic fit captures the inverted-U shape with $R^2 = 0.964$.
    \item \textbf{MRL control}: MRL steerability remains near zero ($|\steer| < 0.02$) across all 8 conditions, confirming that the effect requires hierarchy-aligned supervision.
\end{enumerate}

\paragraph{Resolving the observational confound.}
The synthetic experiment reveals that $\steer \sim \hlz$ (prefix task demand), not $\steer \sim \hlo$.
In natural datasets, the observational correlation with $\hlo$ arises because $\hlz$ and $\hlo$ covary positively.
The synthetic experiment breaks this confound by holding total entropy fixed while varying $K_0$.
Figure~\ref{fig:entropy_alloc} in the appendix visualizes both relationships side by side.

%=======================================================================
\section{Generality and Limitations}
\label{sec:generality}
%=======================================================================

\paragraph{Cross-model replication.}
To verify architecture invariance, we replicate the CLINC experiment on two additional model families: E5-small-v2~(Microsoft, contrastive pre-training, 384d) and Qwen3-Embedding-0.6B ($h = 1024$, 10$\times$ larger than BGE-small), plus TREC on Qwen3.
Table~\ref{tab:crossmodel} shows that all three model families produce the same V5 $\gg$ MRL steerability gap:
BGE-small $+0.144$, E5-small $+0.130$, Qwen3 $+0.153$ (all $p < 0.025$, Holm-corrected across 3 model families).
MRL steerability is near-zero across all models ($\leq 0.015$).
The larger Qwen3 backbone shows higher steerability, suggesting the effect scales with model capacity.

\begin{table}[t]
\caption{Cross-model replication (3 seeds each). Steerability is architecture-invariant across three model families with different pre-training objectives and sizes. The V5 $\gg$ MRL pattern holds universally ($p < 0.025$, Holm-corrected, $m = 3$).}
\label{tab:crossmodel}
\centering
\small
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{CLINC ($\hlo = 3.90$)} & \multicolumn{2}{c}{TREC ($\hlo = 2.21$)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Model & V5 $\steer$ & MRL $\steer$ & V5 $\steer$ & MRL $\steer$ \\
\midrule
BGE-small (BAAI, 33M) & $+0.150 \pm 0.028$ & $+0.007 \pm 0.016$ & $+0.044 \pm 0.017$ & $-0.001 \pm 0.015$ \\
E5-small (Microsoft, 33M) & $+0.130 \pm 0.031$ & $+0.015 \pm 0.008$ & --- & --- \\
Qwen3-0.6B (Alibaba, 600M) & $+0.153 \pm 0.013$ & $+0.008 \pm 0.006$ & $+0.081 \pm 0.012$ & $+0.011 \pm 0.002$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Downstream utility: Adaptive retrieval.}
Beyond classification, we test whether steerability enables practical \emph{adaptive search}.
We split test data into queries and documents, and compare fixed-resolution strategies at 64d and 256d, evaluated on CLINC-150 (deep, $\hlo = 3.90$) and TREC (moderate, $\hlo = 2.21$), each with 3 seeds.
Table~\ref{tab:adaptive} shows that V5 provides two distinct operating points: a fast 64d prefix optimized for coarse retrieval ($L_0 = 97.5\%$ on CLINC) and a full 256d embedding that recovers fine accuracy ($L_1 = 92.5\%$, $+11.2$pp).
MRL shows negligible change across dimensions ($\leq 0.4$pp $L_1$ improvement), confirming it cannot support coarse-first, fine-on-demand retrieval pipelines.
V5's prefix at 64d achieves $4\times$ dimensionality reduction while specializing for the coarse task, enabling two-stage retrieval: fast coarse filtering followed by fine reranking only where needed.

\begin{table}[t]
\caption{Adaptive retrieval: V5 provides distinct operating points at 64d and 256d. At 64d ($4\times$ cheaper), V5 specializes for $L_0$; extending to 256d recovers $L_1$. MRL shows no specialization: $L_1$ accuracy is flat across dimensions. Values are means over 3 seeds.}
\label{tab:adaptive}
\centering
\small
\begin{tabular}{llcccc}
\toprule
& & \multicolumn{2}{c}{CLINC ($\hlo = 3.90$)} & \multicolumn{2}{c}{TREC ($\hlo = 2.21$)} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
Method & Dims & $L_0$ & $L_1$ & $L_0$ & $L_1$ \\
\midrule
V5 & 64d ($4\times$ cheaper) & 97.5 & 81.3 & 94.0 & 71.3 \\
V5 & 256d (full) & 98.3 & 92.5 & 95.6 & 74.5 \\
$\Delta$ (256d$-$64d) & & $+0.8$ & $+11.2$ & $+1.6$ & $+3.2$ \\
\midrule
MRL & 64d & 92.1 & 91.2 & 92.3 & 74.1 \\
MRL & 256d & 91.6 & 91.6 & 93.2 & 73.9 \\
$\Delta$ (256d$-$64d) & & $-0.5$ & $+0.4$ & $+0.9$ & $-0.2$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Limitations.}
\begin{itemize}
    \item \textbf{Shallow hierarchies}: On Yahoo Answers ($K_0 = 4$, $\hlo = 1.23$), steerability is small and noisy ($\steer = 0.015 \pm 0.019$, 5~seeds). The method is most valuable for deep hierarchies.
    \item \textbf{Ceiling effects}: On datasets where the unfinetuned baseline already achieves high $L_0$ accuracy (e.g., DBPedia at $L_0 = 1.0$), there is no room for steerability improvement.
    \item \textbf{Two-level hierarchies}: Our current formulation handles two-level hierarchies ($L_0$, $L_1$). Extension to deeper hierarchies is straightforward in principle but untested.
\end{itemize}

%=======================================================================
\section{Theoretical Analysis: Successive Refinement}
\label{sec:theory}
%=======================================================================

We provide a formal information-theoretic analysis connecting fractal embeddings to the classical theory of \emph{successive refinement}~\citep{equitz1991successive,rimoldi1994successive}.

\paragraph{Setup.}
Let $(X, Y_0, Y_1)$ be a hierarchical source with $Y_0 = g(Y_1)$ (coarse is a deterministic function of fine).
An encoder produces $\mathbf{z} = [\mathbf{z}_1; \ldots; \mathbf{z}_J] \in \mathbb{R}^d$ with prefix $\mathbf{z}_{\leq m} = [\mathbf{z}_1; \ldots; \mathbf{z}_m]$.
Let $C(d')$ denote the effective capacity (in bits) of a $d'$-dimensional embedding under the encoder family.

\paragraph{Theorem 1 (Hierarchy-Successive-Refinement, informal).}
\emph{Assume $C(d/J) \geq H(L_0)$ and $C(d/J) < H(L_1)$.
Under V5 supervision ($\mathbf{z}_{\leq 1} \rightarrow L_0$, $\mathbf{z} \rightarrow L_1$):}
\begin{equation}
    I(\mathbf{z}_{\leq 1}; L_0) > I(\mathbf{z}_{\leq 1}; L_1 | L_0) \quad \text{(coarse-prioritized prefix)}
\end{equation}
\emph{Under MRL ($\mathbf{z}_{\leq 1} \rightarrow L_1$, $\mathbf{z} \rightarrow L_1$): no specialization, $I(\mathbf{z}_{\leq 1}; L_0) \approx I(\mathbf{z}; L_0)$.}

The proof follows from the capacity bottleneck: V5's prefix loss depends only on $L_0$, so the optimal prefix maximizes $I(\mathbf{z}_{\leq 1}; L_0)$. Since $C(d/J) < H(L_1)$ but $C(d/J) \geq H(L_0)$, the prefix allocates capacity preferentially to the coarse task.
MRL's prefix loss targets $L_1$, distributing capacity across both $L_0$ and $L_1|L_0$ components without specialization.

\paragraph{Connection to successive refinement.}
Hierarchical sources are naturally \emph{successively refinable}~\citep{rimoldi1994successive}: the optimal multi-resolution code first encodes $Y_0$ at rate $R_1 \geq H(Y_0)$, then encodes the residual $Y_1|Y_0$ at rate $R_2 \geq H(Y_1|Y_0)$.
V5 training approximates this: block 1 encodes $Y_0$, blocks 2--$J$ encode the refinement.
MRL instead performs single-resolution coding at each rate, losing the nested structure.
This explains why V5 achieves accuracy parity at full resolution while gaining steerability at short prefixes: it matches the rate-distortion bound for the multi-resolution problem.

\paragraph{Theorem 2 (Goldilocks capacity-demand matching, informal).}
\emph{With fixed $H(Y_1)$ and varying $K_0$, steerability $\steer(K_0)$ peaks at $H^*(L_0) \approx C(d/J)$:}
\begin{itemize}
    \item \emph{When $H(L_0) < C(d/J)$}: spare prefix capacity leaks $L_1|L_0$ information, reducing $\steer$.
    \item \emph{When $H(L_0) > C(d/J)$}: by Fano's inequality, prefix errors degrade coarse classification, reducing $\steer$.
    \item \emph{Taylor expansion around $H^*$}: $\steer \approx \steer^* - \alpha(H(L_0) - H^*)^2$, matching the empirical quadratic fit ($R^2 = 0.964$).
\end{itemize}

\paragraph{Testable prediction.}
Doubling the prefix dimension from 64 to 128 should shift the Goldilocks peak rightward (to higher $K_0$), as $C(d/J)$ increases. This is verifiable via a capacity sweep ablation.

%=======================================================================
\section{Related Work}
\label{sec:related}
%=======================================================================

\paragraph{Multi-resolution embeddings.}
Matryoshka Representation Learning (MRL)~\citep{kusupati2022matryoshka} trains embeddings that support prefix truncation, but all prefix lengths are supervised on the same task, producing no semantic specialization.
SMEC~\citep{li2025smrl} rethinks MRL training for retrieval embedding compression; Matryoshka Multimodal Models~\citep{cai2024m3} apply the nesting principle to visual tokens in vision-language models.
None of these approaches introduce hierarchy-awareness: truncation changes \emph{fidelity}, not \emph{semantic level}.
Our work shows that aligning prefix supervision with hierarchy is the missing ingredient that converts fidelity truncation into semantic zoom.

\paragraph{Dimensional redundancy.}
\citet{dufter2025random} show that randomly removing 50\% of embedding dimensions causes $<$10\% performance degradation, revealing massive redundancy in standard embeddings.
We exploit this redundancy differently: rather than discarding dimensions for compression, we \emph{structure} them to carry semantically distinct information at each prefix length.
Random removal preserves flat performance but destroys hierarchical steerability---precisely the property we engineer.
\citet{luan2025limits} prove that the expressiveness of single-vector embeddings for top-$k$ retrieval is bounded by dimensionality, motivating multi-resolution access patterns that our method enables.

\paragraph{Hierarchical embeddings.}
Hyperbolic embeddings~\citep{nickel2017poincare} represent hierarchies through non-Euclidean geometry.
HEAL~\citep{zhang2025heal} (ICLR 2025) aligns LLM embeddings with domain hierarchies via hierarchical contrastive losses and matrix factorization.
Unlike HEAL, which requires external hierarchical labels and preprocessing, our method derives hierarchy from the embedding's own prefix structure, making it intrinsic and architecture-agnostic.
HEAL does not offer steerability via dimensional truncation.

\paragraph{Sparse and compressed embeddings.}
CSR~\citep{chen2025csr} (ICML 2025) and CSRv2~\citep{chen2026csrv2} (ICLR 2026) learn sparse codes as alternatives to MRL, achieving superior efficiency through selective activation.
These address adaptive dimensionality via sparsity; our approach addresses it via hierarchical prefix structure.
The two are orthogonal and potentially complementary: fractal embeddings could be sparsified for further efficiency.

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

We have shown that a simple modification to embedding training---aligning prefix supervision with semantic hierarchy---creates embeddings where dimensional truncation corresponds to semantic zoom.
Through controlled ablations on two datasets, synthetic hierarchy experiments, cross-model replication on three families, and a random-effects meta-analysis ($d = 1.23$, $p = 0.003$), we support the \textbf{Fractal Embedding Principle}: steerability is driven by alignment, scales with prefix task demand, and is absent in standard MRL.
We connect this to the classical theory of successive refinement, showing that V5 training approximates the optimal multi-resolution code for hierarchical semantic sources.

The practical implications extend beyond classification.
In retrieval systems, fractal embeddings enable \emph{adaptive search}: start with a 64-dimensional prefix for fast coarse filtering, then extend to 256 dimensions only for queries that require fine-grained resolution---achieving the quality of full-resolution search at a fraction of the compute.
In any domain with hierarchical semantics---product taxonomies, medical coding (ICD-10), intent classification, scientific categorization---a single fractal embedding replaces separate coarse and fine models.

Our synthetic hierarchy experiment reveals that the prefix acts as a \emph{routing bottleneck}: steerability peaks when coarse task complexity matches the prefix's representational capacity.
This Goldilocks effect provides principled design guidance: measure $\hlz$ of your hierarchy and size prefix dimensions to match.
The connection to successive refinement theory suggests this is not merely an empirical regularity but a fundamental property of information allocation in hierarchical representations.

\bibliography{references}
\bibliographystyle{plainnat}

%=======================================================================
% APPENDIX
%=======================================================================
\appendix

\section{Entropy Allocation Analysis}
\label{app:entropy}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{../results/figures/paper/fig7_entropy_allocation.pdf}
    \caption{Disentangling the scaling law. \textbf{Left:} Steerability vs.\ $\hlz$ (prefix task demand) --- the true driver, confirmed by the synthetic experiment. \textbf{Right:} Steerability vs.\ $\hlo$ --- a confounded proxy in observational data. Synthetic data (green diamonds) breaks the confound: $\hlo$ anti-correlates with steerability when $\hlz$ is varied independently.}
    \label{fig:entropy_alloc}
\end{figure}

\section{Full Synthetic Hierarchy Results}
\label{app:synthetic}

See Table~\ref{tab:synthetic} for complete results across all 8 coarse partition sizes.
The experiment uses CLINC-150 text with randomly reassigned hierarchies, holding total class count fixed at 150 while varying $K_0$ from 2 to 75.

\section{Training Convergence}
\label{app:convergence}

All models converge within 5 epochs of head-only training.
Stage 2 (backbone fine-tuning) was tested but provided no improvement over head-only training, consistent with the finding that the frozen backbone already provides sufficient representational capacity for hierarchy-aligned supervision.

\section{Reproducibility}
\label{app:reproducibility}

\paragraph{Code and data.}
All code, trained models, and result JSONs are publicly available at \url{https://github.com/dl1683/ai-moonshots}.
Every experiment uses publicly available datasets (CLINC-150, TREC, Yahoo Answers, 20 Newsgroups, GoEmotions, arXiv) loaded through the \texttt{datasets} library, with deterministic train/test splits seeded per experiment.

\paragraph{Hyperparameters.}
All experiments use identical hyperparameters: 5 epochs head-only training, batch size 16, learning rate $10^{-4}$, AdamW optimizer with cosine decay, FP16 mixed precision, gradient clipping at 1.0.
Prefix sampling probabilities $[0.4, 0.3, 0.2, 0.1]$ and block dropout keep rates $[0.95, 0.9, 0.8, 0.7]$ are fixed across all datasets and models.
No hyperparameter tuning was performed per dataset.

\paragraph{Compute.}
All experiments were run on a single NVIDIA RTX 5090 Laptop GPU (24GB VRAM).
Each V5 or MRL training run takes approximately 2 minutes for BGE-small (33M parameters) and 8 minutes for Qwen3-0.6B.
The full experimental suite (6 datasets $\times$ 5 seeds $\times$ 2 methods $\times$ 3 model families $+$ ablations $+$ synthetic experiments) requires approximately 12 GPU-hours.

\section{Metric Robustness}
\label{app:robustness}

To verify that our conclusions do not depend on the specific formulation of $\steer$ (Eq.~\ref{eq:steerability}), we evaluate three alternative steerability metrics:
\begin{itemize}
    \item $\steer_\text{AUC}$: Average of $(L_0@j_1 - L_0@j_k) + (L_1@j_k - L_1@j_1)$ over $k = 2, 3, 4$, integrating across all prefix lengths rather than using only the endpoints.
    \item $\steer_\text{mono}$: Fraction of adjacent prefix pairs where $L_0$ accuracy decreases and $L_1$ accuracy increases with prefix length (perfect ordering $= 1.0$, random $= 0.5$).
    \item $\steer_\text{gap}$: Specialization gap $= (L_0@j_1 - L_1@j_1) - (L_0@j_4 - L_1@j_4)$, measuring how much more coarse-specialized $j_1$ is relative to $j_4$.
\end{itemize}

Table~\ref{tab:robustness} reports the V5\,--\,MRL gap for each metric on each dataset.
Three of four metrics show V5 $>$ MRL on all six datasets (sign test $p = 0.016$); the fourth ($\steer_\text{mono}$) agrees on 4/6.
All pairwise rank correlations across datasets exceed $\rho = 0.93$ ($p < 0.008$), confirming that the four metrics recover the same dataset ordering of steerability.

\begin{table}[h]
\caption{Metric robustness: V5\,--\,MRL gap under four steerability formulations. Positive values indicate V5 advantage. Three of four metrics agree V5 $>$ MRL on all 6 datasets.}
\label{tab:robustness}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dataset & $\hlo$ & $\Delta\steer_\text{orig}$ & $\Delta\steer_\text{AUC}$ & $\Delta\steer_\text{mono}$ & $\Delta\steer_\text{gap}$ \\
\midrule
Yahoo & 1.23 & $+0.010$ & $+0.013$ & $-0.10$ & $+0.010$ \\
GoEmotions & 1.88 & $+0.014$ & $+0.014$ & $-0.07$ & $+0.014$ \\
Newsgroups & 1.88 & $+0.035$ & $+0.037$ & $+0.03$ & $+0.035$ \\
TREC & 2.21 & $+0.045$ & $+0.039$ & $+0.27$ & $+0.045$ \\
arXiv & 2.62 & $+0.028$ & $+0.020$ & $+0.17$ & $+0.028$ \\
CLINC & 3.90 & $+0.143$ & $+0.124$ & $+0.27$ & $+0.143$ \\
\midrule
V5 $>$ MRL & & 6/6 & 6/6 & 4/6 & 6/6 \\
Sign test $p$ & & 0.016 & 0.016 & 0.34 & 0.016 \\
\bottomrule
\end{tabular}
\end{table}

\section{Per-Seed Steerability Values}
\label{app:perseed}

Table~\ref{tab:perseed} reports individual seed steerability values for all six datasets, enabling full transparency about the variability underlying the summary statistics in Table~\ref{tab:steerability}.

\begin{table}[h]
\caption{Per-seed steerability values (V5 and MRL) across all six datasets. Seeds: 42, 123, 456, 789, 1024.}
\label{tab:perseed}
\centering
\scriptsize
\begin{tabular}{lrrrrr|rrrrr}
\toprule
& \multicolumn{5}{c|}{V5 $\steer$ by seed} & \multicolumn{5}{c}{MRL $\steer$ by seed} \\
Dataset & 42 & 123 & 456 & 789 & 1024 & 42 & 123 & 456 & 789 & 1024 \\
\midrule
Yahoo & +.016 & +.020 & $-.004$ & $-.002$ & +.044 & $-.010$ & +.016 & +.006 & +.016 & $-.002$ \\
GoEmo & $-.002$ & +.024 & +.026 & +.006 & +.044 & +.022 & +.022 & +.006 & $-.012$ & $-.010$ \\
News & +.040 & $-.012$ & +.038 & +.044 & +.066 & $-.002$ & +.022 & +.008 & $-.018$ & $-.010$ \\
TREC & +.018 & +.062 & +.054 & +.042 & +.046 & +.000 & +.024 & $-.014$ & $-.002$ & $-.012$ \\
arXiv & +.038 & +.018 & +.012 & +.018 & +.048 & $-.014$ & $-.010$ & +.000 & +.000 & +.020 \\
CLINC & +.104 & +.178 & +.150 & +.168 & +.150 & +.012 & +.028 & +.006 & $-.016$ & +.004 \\
\bottomrule
\end{tabular}
\end{table}

\section{Scaling Trend Robustness}
\label{app:scaling_robust}

With only six datasets, the scaling trend ($\rho = 0.83$, Section~\ref{sec:scaling}) deserves careful sensitivity analysis.
Table~\ref{tab:loo} reports leave-one-out (LOO) Spearman correlations.

\begin{table}[h]
\caption{Leave-one-out sensitivity analysis for the scaling trend. Dropping CLINC weakens the correlation most ($\rho = 0.62$), reflecting its high leverage as the deepest hierarchy. Dropping arXiv strengthens the trend ($\rho = 0.98$), consistent with arXiv's fuzzy boundaries. Pearson $r$ remains $\geq 0.53$ in all cases.}
\label{tab:loo}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Dropped & $k$ & Spearman $\rho$ & $p$ & Pearson $r$ & $p$ \\
\midrule
None (full) & 6 & 0.83 & 0.042 & 0.90 & 0.014 \\
\midrule
Yahoo & 5 & 0.62 & 0.269 & 0.92 & 0.026 \\
GoEmotions & 5 & 0.70 & 0.188 & 0.90 & 0.038 \\
Newsgroups & 5 & 0.90 & 0.037 & 0.90 & 0.035 \\
TREC & 5 & 0.82 & 0.089 & 0.90 & 0.036 \\
arXiv & 5 & 0.98 & 0.005 & 0.98 & 0.004 \\
CLINC & 5 & 0.62 & 0.269 & 0.53 & 0.354 \\
\bottomrule
\end{tabular}
\end{table}

CLINC has the highest Cook's distance ($D = 6.3$), reflecting its dual role as the deepest hierarchy ($\hlo = 3.90$) and the strongest effect ($\steer = 0.150$).
This leverage is expected: scaling trend claims inherently require observations spanning a wide range of the predictor variable, and CLINC provides the upper anchor.
A bootstrap analysis (10{,}000 resamples) yields 95.9\% of Spearman $\rho$ values $> 0$, confirming that the positive monotonic trend is robust to sampling variation even with $k = 6$.
The meta-analysis prediction interval for a new dataset's Cohen's $d$ is $[-1.13, 3.59]$, reflecting the substantial heterogeneity ($I^2 = 59\%$) that the scaling trend itself predicts: datasets differ systematically in effect size based on hierarchy depth.

\section{Broader Impact}
\label{app:impact}

Fractal embeddings add a semantic control knob to existing embedding models without modifying the backbone.
The primary application is more efficient retrieval systems: coarse-first filtering reduces compute by 4$\times$ (64d vs.\ 256d) without sacrificing fine-grained accuracy when needed.
We do not foresee negative societal impacts beyond those inherent to embedding-based retrieval systems generally.
The method is agnostic to the content domain and does not introduce new biases beyond those present in the frozen backbone.

\end{document}
